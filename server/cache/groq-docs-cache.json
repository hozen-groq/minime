{"chunks":[{"title":"Introduction to Groq API","content":"# https://console.groq.com llms-full.txt","keywords":["models"]},{"title":"üóÇÔ∏è LlamaIndex ü¶ô","content":"URL: https://console.groq.com/docs/llama-index","keywords":["models","endpoints","llama","llama-"]},{"title":"üóÇÔ∏è LlamaIndex ü¶ô","content":"[LlamaIndex](https://www.llamaindex.ai/) is a data framework for LLM-based applications that benefit from context augmentation, such as Retrieval-Augmented Generation (RAG) systems. LlamaIndex provides the essential abstractions to more easily ingest, structure, and access private or domain-specific data, resulting in safe and reliable injection into LLMs for more accurate text generation.\n\nFor more information, read the LlamaIndex Groq integration documentation for [Python](https://docs.llamaindex.ai/en/stable/examples/llm/groq.html) and [JavaScript](https://ts.llamaindex.ai/modules/llms/available_llms/groq).\n\n---","keywords":["models","rate limits","llama"]},{"title":"Rate Limits","content":"URL: https://console.groq.com/docs/rate-limits","keywords":["endpoints","rate limits"]},{"title":"Rate Limits","content":"Rate limits act as control measures to regulate how frequently users and applications can access our API within specified timeframes. These limits help ensure service stability, fair access, and protection\nagainst misuse so that we can serve reliable and fast inference for all.\n\n### Understanding Rate Limits\nRate limits are measured in:\n- **RPM:** Requests per minute\n- **RPD:** Requests per day\n- **TPM:** Tokens per minute\n- **TPD:** Tokens per day\n\nRate limits apply at the organization level, not individual users. You can hit any limit type depemding on which threshold you reach first. \n**Example:** Let's say your RPM = 50 and your TPM = 200K. If you were to send 50 requests with only 100 tokens within a minute, you would reach your limit even though you did not send 200K tokens within those\n50 requests.","keywords":["authentication","endpoints","rate limits","inference"]},{"title":"Rate Limits","content":"The following is a high level summary and there may be  exceptions to these limits. You can view the current, exact rate limits for your organization on the [limits page](/settings/limits) in your account settings.","keywords":["rate limits","error"]},{"title":"Rate Limit Headers","content":"In addition to viewing your limits on your account's [limits](https://console.groq.com/settings/limits) page, you can also view rate limit information such as remaining requests and tokens in HTTP response \nheaders as follows:\nThe following headers are set (values are illustrative):","keywords":["authentication","endpoints","rate limits"]},{"title":"Handling Rate Limits","content":"When you exceed rate limits, our API returns a `429 Too Many Requests` HTTP status code.\n**Note**: `retry-after` is only set if you hit the rate limit and status code 429 is returned. The other headers are always included.\n\n---","keywords":["endpoints","rate limits"]},{"title":"Initialize the Groq client","content":"URL: https://console.groq.com/docs/speech-to-text/scripts/transcription.py\n\nimport os\nfrom groq import Groq\n\n# Initialize the Groq client\nclient = Groq()\n\n# Specify the path to the audio file\nfilename = os.path.dirname(__file__) + \"/sample_audio.m4a\" # Replace with your audio file!\n\n# Open the audio file\nwith open(filename, \"rb\") as file:\n    # Create a transcription of the audio file\n    transcription = client.audio.transcriptions.create(\n      file=(filename, file.read()), # Required audio file\n      model=\"whisper-large-v3-turbo\", # Required model to use for transcription\n      prompt=\"Specify context or spelling\",  # Optional\n      language=\"tr\", # Optional\n      response_format=\"json\",  # Optional\n      language=\"en\",  # Optional\n      temperature=0.0  # Optional\n    )\n    # Print the transcription text\n    print(transcription.text)\n\n---","keywords":["models","endpoints","parameters","speech","whisper-"]},{"title":"Speech To Text: Transcription (js)","content":"URL: https://console.groq.com/docs/speech-to-text/scripts/transcription\n\nimport fs from \"fs\";\nimport Groq from \"groq-sdk\";\n\n// Initialize the Groq client\nconst groq = new Groq();\n\nasync function main() {\n  // Create a transcription job\n  const transcription = await groq.audio.transcriptions.create({\n    file: fs.createReadStream(\"sample_audio.m4a\"), // Required path to audio file - replace with your audio file!\n    model: \"whisper-large-v3-turbo\", // Required model to use for transcription\n    prompt: \"Specify context or spelling\", // Optional\n    language: \"tr\", // Optional \n    response_format: \"json\", // Optional\n    language: \"en\", // Optional\n    temperature: 0.0, // Optional\n  });\n  // Log the transcribed text\n  console.log(transcription.text);\n}\nmain();\n\n---","keywords":["models","endpoints","parameters","tools","speech","whisper-"]},{"title":"Initialize the Groq client","content":"URL: https://console.groq.com/docs/speech-to-text/scripts/translation.py\n\nimport os\nfrom groq import Groq\n\n# Initialize the Groq client\nclient = Groq()\n\n# Specify the path to the audio file\nfilename = os.path.dirname(__file__) + \"/sample_audio.m4a\" # Replace with your audio file!\n\n# Open the audio file\nwith open(filename, \"rb\") as file:\n    # Create a translation of the audio file\n    translation = client.audio.translations.create(\n      file=(filename, file.read()), # Required audio file\n      model=\"whisper-large-v3\", # Required model to use for translation\n      prompt=\"Specify context or spelling\",  # Optional\n      language=\"en\", # Optional ('en' only)\n      response_format=\"json\",  # Optional\n      temperature=0.0  # Optional\n    )\n    # Print the translation text\n    print(translation.text)\n\n---","keywords":["models","endpoints","parameters","speech","whisper-"]},{"title":"Speech To Text: Translation (js)","content":"URL: https://console.groq.com/docs/speech-to-text/scripts/translation\n\nimport fs from \"fs\";\nimport Groq from \"groq-sdk\";\n\n// Initialize the Groq client\nconst groq = new Groq();\nasync function main() {\n  // Create a translation job\n  const translation = await groq.audio.translations.create({\n    file: fs.createReadStream(\"sample_audio.m4a\"), // Required path to audio file - replace with your audio file!\n    model: \"whisper-large-v3\", // Required model to use for translation\n    prompt: \"Specify context or spelling\", // Optional\n    language: \"en\", // Optional ('en' only)\n    response_format: \"json\", // Optional\n    temperature: 0.0, // Optional\n  });\n  // Log the transcribed text\n  console.log(translation.text);\n}\nmain();\n\n---","keywords":["models","endpoints","parameters","tools","speech","whisper-"]},{"title":"Speech to Text","content":"URL: https://console.groq.com/docs/speech-to-text","keywords":["endpoints","speech"]},{"title":"Speech to Text","content":"Groq API is the fastest speech-to-text solution available, offering OpenAI-compatible endpoints that enable near-instant transcriptions and translations. With Groq API, you can integrate high-quality audio processing into your applications at speeds that rival human interaction.","keywords":["endpoints","rate limits","speech","inference"]},{"title":"API Endpoints","content":"We support two endpoints:\n\n| Endpoint       | Usage                          | API Endpoint                                                |\n|----------------|--------------------------------|-------------------------------------------------------------|\n| Transcriptions | Convert audio to text          | `https://api.groq.com/openai/v1/audio/transcriptions`        |\n| Translations   | Translate audio to English text| `https://api.groq.com/openai/v1/audio/translations`          |","keywords":["endpoints","speech"]},{"title":"Supported Models","content":"| Model ID                    | Model                | Supported Language(s)          | Description                                                                                                                   |\n|-----------------------------|----------------------|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n| whisper-large-v3-turbo    | Whisper Large V3 Turbo | Multilingual                | A fine-tuned version of a pruned Whisper Large V3 designed for fast, multilingual transcription tasks. |\n| distil-whisper-large-v3-en   | Distil-Whisper English | English-only                  | A distilled, or compressed, version of OpenAI's Whisper model, designed to provide faster, lower cost English speech recognition while maintaining comparable accuracy. |\n| whisper-large-v3          | Whisper large-v3     | Multilingual                  | Provides state-of-the-art performance with high accuracy for multilingual transcription and translation tasks. |","keywords":["models","endpoints","pricing","speech","inference","whisper-","whisper "]},{"title":"Which Whisper Model Should You Use?","content":"Having more choices is great, but let's try to avoid decision paralysis by breaking down the tradeoffs between models to find the one most suitable for your applications: \n- If your application is error-sensitive and requires multilingual support, use `whisper-large-v3`. \n- If your application is less sensitive to errors and requires English only, use `distil-whisper-large-v3-en`. \n- If your application requires multilingual support and you need the best price for performance, use `whisper-large-v3-turbo`. \n\nThe following table breaks down the metrics for each model.\n| Model | Cost Per Hour | Language Support | Transcription Support | Translation Support | Real-time Speed Factor | Word Error Rate |\n|--------|--------|--------|--------|--------|--------|--------|\n| `whisper-large-v3` | $0.111 | Multilingual | Yes | Yes | 189 | 10.3% |\n| `whisper-large-v3-turbo` | $0.04 | Multilingual | Yes | No | 216 | 12% |\n| `distil-whisper-large-v3-en` | $0.02 | English only | Yes | No | 250 | 13% |","keywords":["models","rate limits","error","pricing","speech","inference","whisper ","whisper-"]},{"title":"Audio File Limitations","content":"* Max File Size: 40 MB (free tier), 100MB (dev tier)\n* Minimum File Length: 0.01 seconds\n* Minimum Billed Length: 10 seconds. If you submit a request less than this, you will still be billed for 10 seconds.\n* Supported File Types: Either a URL or a direct file upload for `flac`, `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `ogg`, `wav`, `webm`\n* Single Audio Track: Only the first track will be transcribed for files with multiple audio tracks. (e.g. dubbed video)\n* Supported Response Formats: `json`, `verbose_json`, `text`\n* Supported Timestamp Granularities: `segment`, `word`\n\n### Audio Preprocessing\nOur speech-to-text models will downsample audio to 16KHz mono before transcribing, which is optimal for speech recognition. This preprocessing can be performed client-side if your original file is extremely large and you want to make it smaller without a loss in quality (without chunking, Groq API speech-to-text endpoints accept up to 40MB for free tier and 100MB for dev tier). We recommend FLAC for lossless compression.\nThe following `ffmpeg` command can be used to reduce file size:\n```shell\nffmpeg \\\n  -i <your file> \\\n  -ar 16000 \\\n  -ac 1 \\\n  -map 0:a \\\n  -c:a flac \\\n  <output file name>.flac\n```\n\n### Working with Larger Audio Files\nFor audio files that exceed our size limits or require more precise control over transcription, we recommend implementing audio chunking. This process involves:\n1. Breaking the audio into smaller, overlapping segments\n2. Processing each segment independently\n3. Combining the results while handling overlapping\n\nTo learn more about this process and get code for your own implementation, see the complete audio chunking tutorial in our Groq API Cookbook.","keywords":["models","endpoints","rate limits","pricing","speech"]},{"title":"Using the API","content":"The following are request parameters you can use in your transcription and translation requests:\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `file` | `string` | Required unless using `url` instead | The audio file object for direct upload to translate/transcribe. |\n| `url` | `string` | Required unless using `file` instead | The audio URL to translate/transcribe (supports Base64URL). |\n| `language` | `string` | Optional | The language of the input audio. Supplying the input language in ISO-639-1 (i.e. `en, `tr`) format will improve accuracy and latency. The translations endpoint only supports 'en' as a parameter option. |\n| `model` | `string` | Required | ID of the model to use.|\n| `prompt` | `string` | Optional | Prompt to guide the model's style or specify how to spell unfamiliar words. (limited to 224 tokens) |\n| `response_format` | `string` | json | Define the output response format.<br/>Set to `verbose_json` to receive timestamps for audio segments.<br/>Set to `text` to return a text response. |\n| `temperature` | `float` | 0 | The temperature between 0 and 1. For translations and transcriptions, we recommend the default value of 0. |\n| `timestamp_granularities[]` | `array` | segment | The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. |\n\n### Example Usage of Transcription Endpoint \nThe transcription endpoint allows you to transcribe spoken words in audio or video files.\n\nThe Groq SDK package can be installed using the following command:\n\nThe following code snippet demonstrates how to use Groq API to transcribe an audio file in Python:\n\nThe following code snippet demonstrates how to use Groq API to transcribe an audio file in JavaScript:\n\nThe following is an example cURL request:\n\nThe following is an example response:\n```json\n{\n  \"text\": \"Your transcribed text appears here...\",\n  \"x_groq\": {\n    \"id\": \"req_unique_id\"\n  }\n}\n```\n\n### Example Usage of Translation Endpoint\nThe translation endpoint allows you to translate spoken words in audio or video files to English.\n\nThe Groq SDK package can be installed using the following command:\n\nThe following code snippet demonstrates how to use Groq API to translate an audio file in Python:\n\nThe following code snippet demonstrates how to use Groq API to translate an audio file in JavaScript:\n\nThe following is an example cURL request:\n\nThe following is an example response:\n```json\n{\n  \"text\": \"Your translated text appears here...\",\n  \"x_groq\": {\n    \"id\": \"req_unique_id\"\n  }\n}\n```","keywords":["models","authentication","endpoints","parameters","rate limits","speech","inference"]},{"title":"Understanding Metadata Fields","content":"When working with Groq API, setting `response_format` to `verbose_json` outputs each segment of transcribed text with valuable metadata that helps us understand the quality and characteristics of our transcription, including `avg_logprob`, `compression_ratio`, and `no_speech_prob`. \nThis information can help us with debugging any transcription issues. Let's examine what this metadata tells us using a real example:\n```json\n{\n  \"id\": 8,\n  \"seek\": 3000,\n  \"start\": 43.92,\n  \"end\": 50.16,\n  \"text\": \" document that the functional specification that you started to read through that isn't just the\",\n  \"tokens\": [51061, 4166, 300, 264, 11745, 31256],\n  \"temperature\": 0,\n  \"avg_logprob\": -0.097569615,\n  \"compression_ratio\": 1.6637554,\n  \"no_speech_prob\": 0.012814695\n}\n```\nAs shown in the above example, we receive timing information as well as quality indicators. Let's gain a better understanding of what each field means:\n- `id:8`: The 9th segment in the transcription (counting begins at 0)\n- `seek`: Indicates where in the audio file this segment begins (3000 in this case)\n- `start` and `end` timestamps: Tell us exactly when this segment occurs in the audio (43.92 to 50.16 seconds in our example)\n- `avg_logprob` (Average Log Probability): -0.097569615 in our example indicates very high confidence. Values closer to 0 suggest better confidence, while more negative values (like -0.5 or lower) might indicate transcription issues.\n- `no_speech_prob` (No Speech Probability): 0.0.012814695 is very low, suggesting this is definitely speech. Higher values (closer to 1) would indicate potential silence or non-speech audio.\n- `compression_ratio`: 1.6637554 is a healthy value, indicating normal speech patterns. Unusual values (very high or low) might suggest issues with speech clarity or word boundaries.\n\n### Using Metadata for Debugging\nWhen troubleshooting transcription issues, look for these patterns:\n- Low Confidence Sections: If `avg_logprob` drops significantly (becomes more negative), check for background noise, multiple speakers talking simultaneously, unclear pronunciation, and strong accents. Consider cleaning up the audio in these sections or adjusting chunk sizes around problematic chunk boundaries.\n- Non-Speech Detection: High `no_speech_prob` values might indicate silence periods that could be trimmed, background music or noise, or non-verbal sounds being misinterpreted as speech. Consider noise reduction when preprocessing.\n- Unusual Speech Patterns: Unexpected `compression_ratio` values can reveal stuttering or word repetition, speaker talking unusually fast or slow, or audio quality issues affecting word separation.\n\n### Quality Thresholds and Regular Monitoring\nWe recommend setting acceptable ranges for each metadata value we reviewed above and flagging segments that fall outside these ranges to be able to identify and adjust preprocessing or chunking strategies for flagged sections. \nBy understanding and monitoring these metadata values, you can significantly improve your transcription quality and quickly identify potential issues in your audio processing pipeline.","keywords":["authentication","endpoints","parameters","rate limits","error","tools","speech","inference"]},{"title":"Prompting Guidelines","content":"The prompt parameter (max 224 tokens) helps provide context and maintain a consistent output style.\nUnlike chat completion prompts, these prompts only guide style and context, not specific actions.\n\n### Best Practices\n- Provide relevant context about the audio content, such as the type of conversation, topic, or speakers involved.\n- Use the same language as the language of the audio file.\n- Steer the model's output by denoting proper spellings or emulate a specific writing style or tone.\n- Keep the prompt concise and focused on stylistic guidance.\n\nWe can't wait to see what you build!\n\n---","keywords":["models","authentication","endpoints","parameters","speech"]},{"title":"CrewAI + Groq: High-Speed Agent Orchestration","content":"URL: https://console.groq.com/docs/crewai","keywords":["endpoints","tools","inference"]},{"title":"CrewAI + Groq: High-Speed Agent Orchestration","content":"CrewAI is a framework that enables the orchestration of multiple AI agents with specific roles, tools, and goals as a cohesive team to accomplish complex tasks and create sophisticated workflows. \nAgentic workflows require fast inference due to their complexity. Groq's fast inference optimizes response times for CrewAI agent teams, enabling rapid autonomous decision-making and collaboration for:\n\n- **Fast Agent Interactions:** Leverage Groq's fast inference speeds via Groq API for efficient agent communication\n- **Reliable Performance:** Consistent response times across agent operations\n- **Scalable Multi-Agent Systems:** Run multiple agents in parallel without performance degradation\n- **Simple Integration:** Get started with just a few lines of code\n\n\n### Python Quick Start (2 minutes to hello world)\n#### 1. Install the required packages:\n```bash\npip install crewai groq\n```\n#### 2. Configure your Groq API key:\n```bash\nexport GROQ_API_KEY=\"your-api-key\"\n```\n#### 3. Create your first Groq-powered CrewAI agent:\nIn CrewAI, **agents** are autonomous entities you can design to perform specific roles and achieve particular goals while **tasks** are specific assignments given to agents that detail the actions they\nneed to perform to achieve a particular goal. Tools can be assigned as tasks.\n```python\nllm = LLM(model=\"groq/llama-3.1-70b-versatile\")\n\nsummarizer = Agent(\n    role='Documentation Summarizer', \n    goal='Create concise summaries of technical documentation', \n    backstory='Technical writer who excels at simplifying complex concepts', \n    llm=llm, \n    verbose=True \n)\n\ntranslator = Agent(\n    role='Technical Translator',\n    goal='Translate technical documentation to other languages',\n    backstory='Technical translator specializing in software documentation',\n    llm=llm,\n    verbose=True\n)\n\nsummary_task = Task(\n    description='Summarize this React hook documentation:\\n\\nuseFetch(url) is a custom hook for making HTTP requests. It returns { data, loading, error } and automatically handles loading states.',\n    expected_output=\"A clear, concise summary of the hook's functionality\",\n    agent=summarizer \n)\n\ntranslation_task = Task(\n    description='Translate the summary to Turkish',\n    expected_output=\"Turkish translation of the hook documentation\",\n    agent=translator, \n    dependencies=[summary_task] \n)\n\ncrew = Crew(\n    agents=[summarizer, translator], \n    tasks=[summary_task, translation_task], \n    verbose=True\n)\n\nresult = crew.kickoff()\nprint(result)\n```\n\nWhen you run the above code, you'll see that you've created a summarizer agent and a translator agent working together to summarize and translate documentation! This is a simple example to get you started,\nbut the agents are also able to use tools, which is a powerful combination for building agentic workflows.\n\n**Challenge**: Update the code to add an agent that will write up documentation for functions its given by the user! \n\n### Advanced Model Configuration\nFor finer control over your agents' responses, you can easily configure additional model parameters. These settings help you balance between creative and deterministic outputs, control response length, \nand manage token usage:\n```python\nllm = LLM(\n    model=\"llama-3.1-70b-versatile\",\n    temperature=0.5,\n    max_completion_tokens=1024,\n    top_p=0.9,\n    stop=None,\n    stream=False,\n)\n```\n\nFor more robust documentation and further resources, including using CrewAI agents with tools for building a powerful agentic workflow, see the following:\n- [Official Documentation: CrewAI](https://docs.crewai.com/concepts/llms)\n- [Groq API Cookbook: CrewAI Mixture of Agents Tutorial](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/crewai-mixture-of-agents)\n- [Webinar: Build CrewAI Agents with Groq](https://youtu.be/Q3fh0sWVRX4?si=fhMLPsBF5OBiMfjD)\n\n---","keywords":["models","authentication","endpoints","parameters","error","tools","inference","llama-3.1-"]},{"title":"Tool Use: Step2 (js)","content":"URL: https://console.groq.com/docs/tool-use/scripts/step2\n\n```javascript\n// imports calculate function from step 1\nasync function runConversation(userPrompt) {\n    const messages = [\n        {\n            role: \"system\",\n            content: \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\"\n        },\n        {\n            role: \"user\",\n            content: userPrompt,\n        }\n    ];\n\n    const tools = [\n        {\n            type: \"function\",\n            function: {\n                name: \"calculate\",\n                description: \"Evaluate a mathematical expression\",\n                parameters: {\n                    type: \"object\",\n                    properties: {\n                        expression: {\n                            type: \"string\",\n                            description: \"The mathematical expression to evaluate\",\n                        }\n                    },\n                    required: [\"expression\"],\n                },\n            },\n        }\n    ];\n\n    const response = await client.chat.completions.create({\n        model: MODEL,\n        messages: messages,\n        stream: false,\n        tools: tools,\n        tool_choice: \"auto\",\n        max_completion_tokens: 4096\n    });\n\n    const responseMessage = response.choices[0].message;\n    const toolCalls = responseMessage.tool_calls;\n\n    if (toolCalls) {\n        const availableFunctions = {\n            \"calculate\": calculate,\n        };\n\n        messages.push(responseMessage);\n\n        for (const toolCall of toolCalls) {\n            const functionName = toolCall.function.name;\n            const functionToCall = availableFunctions[functionName];\n            const functionArgs = JSON.parse(toolCall.function.arguments);\n            const functionResponse = functionToCall(functionArgs.expression);\n\n            messages.push({\n                tool_call_id: toolCall.id,\n                role: \"tool\",\n                name: functionName,\n                content: functionResponse,\n            });\n        }\n\n        const secondResponse = await client.chat.completions.create({\n            model: MODEL,\n            messages: messages\n        });\n\n        return secondResponse.choices[0].message.content;\n    }\n\n    return responseMessage.content;\n}\n\nconst userPrompt = \"What is 25 * 4 + 10?\";\nrunConversation(userPrompt).then(console.log).catch(console.error);\n```\n\n---","keywords":["models","authentication","endpoints","parameters","error","tools"]},{"title":"imports calculate function from step 1","content":"URL: https://console.groq.com/docs/tool-use/scripts/step2.py\n\n# imports calculate function from step 1\ndef run_conversation(user_prompt):\n    # Initialize the conversation with system and user messages\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": user_prompt,\n        }\n    ]\n    # Define the available tools (i.e. functions) for our model to use\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"calculate\",\n                \"description\": \"Evaluate a mathematical expression\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"expression\": {\n                            \"type\": \"string\",\n                            \"description\": \"The mathematical expression to evaluate\",\n                        }\n                    },\n                    \"required\": [\"expression\"],\n                },\n            },\n        }\n    ]\n    # Make the initial API call to Groq\n    response = client.chat.completions.create(\n        model=MODEL, # LLM to use\n        messages=messages, # Conversation history\n        stream=False,\n        tools=tools, # Available tools (i.e. functions) for our LLM to use\n        tool_choice=\"auto\", # Let our LLM decide when to use tools\n        max_completion_tokens=4096 # Maximum number of tokens to allow in our response\n    )\n    # Extract the response and any tool call responses\n    response_message = response.choices[0].message\n    tool_calls = response_message.tool_calls\n    if tool_calls:\n        # Define the available tools that can be called by the LLM\n        available_functions = {\n            \"calculate\": calculate,\n        }\n        # Add the LLM's response to the conversation\n        messages.append(response_message)\n\n        # Process each tool call\n        for tool_call in tool_calls:\n            function_name = tool_call.function.name\n            function_to_call = available_functions[function_name]\n            function_args = json.loads(tool_call.function.arguments)\n            # Call the tool and get the response\n            function_response = function_to_call(\n                expression=function_args.get(\"expression\")\n            )\n            # Add the tool response to the conversation\n            messages.append(\n                {\n                    \"tool_call_id\": tool_call.id, \n                    \"role\": \"tool\", # Indicates this message is from tool use\n                    \"name\": function_name,\n                    \"content\": function_response,\n                }\n            )\n        # Make a second API call with the updated conversation\n        second_response = client.chat.completions.create(\n            model=MODEL,\n            messages=messages\n        )\n        # Return the final response\n        return second_response.choices[0].message.content\n# Example usage\nuser_prompt = \"What is 25 * 4 + 10?\"\nprint(run_conversation(user_prompt))\n\n---","keywords":["models","authentication","endpoints","parameters","tools"]},{"title":"Define the tool schema","content":"URL: https://console.groq.com/docs/tool-use/scripts/instructor.py\n\nimport instructor\nfrom pydantic import BaseModel, Field\nfrom groq import Groq\n\n# Define the tool schema\ntool_schema = {\n    \"name\": \"get_weather_info\",\n    \"description\": \"Get the weather information for any location.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The location for which we want to get the weather information (e.g., New York)\"\n            }\n        },\n        \"required\": [\"location\"]\n    }\n}\n\n# Define the Pydantic model for the tool call\nclass ToolCall(BaseModel):\n    input_text: str = Field(description=\"The user's input text\")\n    tool_name: str = Field(description=\"The name of the tool to call\")\n    tool_parameters: str = Field(description=\"JSON string of tool parameters\")\n\nclass ResponseModel(BaseModel):\n    tool_calls: list[ToolCall]\n\n# Patch Groq() with instructor\nclient = instructor.from_groq(Groq(), mode=instructor.Mode.JSON)\n\ndef run_conversation(user_prompt):\n    # Prepare the messages\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": f\"You are an assistant that can use tools. You have access to the following tool: {tool_schema}\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": user_prompt,\n        }\n    ]\n\n    # Make the Groq API call\n    response = client.chat.completions.create(\n        model=\"llama-3.3-70b-versatile\",\n        response_model=ResponseModel,\n        messages=messages,\n        temperature=0.7,\n        max_completion_tokens=1000,\n    )\n\n    return response.tool_calls\n\n# Example usage\nuser_prompt = \"What's the weather like in San Francisco?\"\ntool_calls = run_conversation(user_prompt)\n\nfor call in tool_calls:\n    print(f\"Input: {call.input_text}\")\n    print(f\"Tool: {call.tool_name}\")\n    print(f\"Parameters: {call.tool_parameters}\")\n    print()\n\n---","keywords":["models","authentication","endpoints","parameters","tools","llama-3.3-"]},{"title":"Tool Use: Parallel (js)","content":"URL: https://console.groq.com/docs/tool-use/scripts/parallel\n\n```javascript\nimport Groq from \"groq-sdk\";\n\n// Initialize Groq client\nconst groq = new Groq();\nconst model = \"llama-3.3-70b-versatile\";\n\n// Define weather tools\nfunction getTemperature(location) {\n    // This is a mock tool/function. In a real scenario, you would call a weather API.\n    const temperatures = {\"New York\": 22, \"London\": 18, \"Tokyo\": 26, \"Sydney\": 20};\n    return temperatures[location] || \"Temperature data not available\";\n}\n\nfunction getWeatherCondition(location) {\n    // This is a mock tool/function. In a real scenario, you would call a weather API.\n    const conditions = {\"New York\": \"Sunny\", \"London\": \"Rainy\", \"Tokyo\": \"Cloudy\", \"Sydney\": \"Clear\"};\n    return conditions[location] || \"Weather condition data not available\";\n}\n\n// Define system messages and tools\nconst messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful weather assistant.\"},\n    {\"role\": \"user\", \"content\": \"What's the weather like in New York and London?\"},\n];\n\nconst tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"getTemperature\",\n            \"description\": \"Get the temperature for a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the city\",\n                    }\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"getWeatherCondition\",\n            \"description\": \"Get the weather condition for a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the city\",\n                    }\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n];\n\n// Function to get both temperature and weather condition\nexport function getWeatherInfo(location) {\n    const temp = getTemperature(location);\n    const condition = getWeatherCondition(location);\n    return `${location}: Temperature: ${temp}¬∞C, Condition: ${condition}`;\n}\n\n// Make the initial request\nexport async function runWeatherAssistant() {\n    try {\n        const response = await groq.chat.completions.create({\n            model: model,\n            messages: messages,\n            tools: tools,\n            tool_choice: \"auto\",\n            max_completion_tokens: 4096\n        });\n\n        const responseMessage = response.choices[0].message;\n        console.log(\"Response message:\", JSON.stringify(responseMessage, null, 2));\n\n        const toolCalls = responseMessage.tool_calls || [];\n\n        // Process tool calls\n        messages.push(responseMessage);\n\n        const availableFunctions = {\n            getWeatherInfo: getWeatherInfo,\n        };\n\n        for (const toolCall of toolCalls) {\n            const functionName = toolCall.function.name;\n            const functionToCall = availableFunctions[functionName];\n            const functionArgs = JSON.parse(toolCall.function.arguments);\n            const functionResponse = functionToCall(functionArgs.location);\n\n            messages.push({\n                \"role\": \"tool\",\n                \"content\": functionResponse,\n                \"tool_call_id\": toolCall.id,\n            });\n        }\n\n        // Make the final request with tool call results\n        const finalResponse = await groq.chat.completions.create({\n            model: model,\n            messages: messages,\n            tools: tools,\n            tool_choice: \"auto\",\n            max_completion_tokens: 4096\n        });\n\n        console.log(finalResponse.choices[0].message.content);\n        return finalResponse.choices[0].message.content;\n    } catch (error) {\n        console.error(\"An error occurred:\", error);\n        throw error; // Re-throw the error so it can be caught by the caller\n    }\n}\n\nif (require.main === module) {\n    runWeatherAssistant()\n        .then(result => {\n            console.log(\"Final result:\", result);\n            process.exit(0);\n        })\n        .catch(error => {\n            console.error(\"Error in main execution:\", error);\n            process.exit(1);\n        });\n}\n```\n\n---","keywords":["models","authentication","endpoints","parameters","error","tools","llama-3.3-"]},{"title":"Initialize the Groq client","content":"URL: https://console.groq.com/docs/tool-use/scripts/routing.py\n\nfrom groq import Groq\nimport json\n\n# Initialize the Groq client \nclient = Groq()\n\n# Define models\nROUTING_MODEL = \"llama3-70b-8192\"\nTOOL_USE_MODEL = \"llama-3.3-70b-versatile\"\nGENERAL_MODEL = \"llama3-70b-8192\"\n\ndef calculate(expression):\n    \"\"\"Tool to evaluate a mathematical expression\"\"\"\n    try:\n        result = eval(expression)\n        return json.dumps({\"result\": result})\n    except:\n        return json.dumps({\"error\": \"Invalid expression\"})\n\ndef route_query(query):\n    \"\"\"Routing logic to let LLM decide if tools are needed\"\"\"\n    routing_prompt = f\"\"\"\n    Given the following user query, determine if any tools are needed to answer it.\n    If a calculation tool is needed, respond with 'TOOL: CALCULATE'.\n    If no tools are needed, respond with 'NO TOOL'.\n\n    User query: {query}\n\n    Response:\n    \"\"\"\n    \n    response = client.chat.completions.create(\n        model=ROUTING_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a routing assistant. Determine if tools are needed based on the user query.\"},\n            {\"role\": \"user\", \"content\": routing_prompt}\n        ],\n        max_completion_tokens=20  # We only need a short response\n    )\n    \n    routing_decision = response.choices[0].message.content.strip()\n    \n    if \"TOOL: CALCULATE\" in routing_decision:\n        return \"calculate tool needed\"\n    else:\n        return \"no tool needed\"\n\ndef run_with_tool(query):\n    \"\"\"Use the tool use model to perform the calculation\"\"\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        }\n    ]\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"calculate\",\n                \"description\": \"Evaluate a mathematical expression\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"expression\": {\n                            \"type\": \"string\",\n                            \"description\": \"The mathematical expression to evaluate\",\n                        }\n                    },\n                    \"required\": [\"expression\"],\n                },\n            },\n        }\n    ]\n    response = client.chat.completions.create(\n        model=TOOL_USE_MODEL,\n        messages=messages,\n        tools=tools,\n        tool_choice=\"auto\",\n        max_completion_tokens=4096\n    )\n    response_message = response.choices[0].message\n    tool_calls = response_message.tool_calls\n    if tool_calls:\n        messages.append(response_message)\n        for tool_call in tool_calls:\n            function_args = json.loads(tool_call.function.arguments)\n            function_response = calculate(function_args.get(\"expression\"))\n            messages.append(\n                {\n                    \"tool_call_id\": tool_call.id,\n                    \"role\": \"tool\",\n                    \"name\": \"calculate\",\n                    \"content\": function_response,\n                }\n            )\n        second_response = client.chat.completions.create(\n            model=TOOL_USE_MODEL,\n            messages=messages\n        )\n        return second_response.choices[0].message.content\n    return response_message.content\n\ndef run_general(query):\n    \"\"\"Use the general model to answer the query since no tool is needed\"\"\"\n    response = client.chat.completions.create(\n        model=GENERAL_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": query}\n        ]\n    )\n    return response.choices[0].message.content\n\ndef process_query(query):\n    \"\"\"Process the query and route it to the appropriate model\"\"\"\n    route = route_query(query)\n    if route == \"calculate\":\n        response = run_with_tool(query)\n    else:\n        response = run_general(query)\n    \n    return {\n        \"query\": query,\n        \"route\": route,\n        \"response\": response\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    queries = [\n        \"What is the capital of the Netherlands?\",\n        \"Calculate 25 * 4 + 10\"\n    ]\n    \n    for query in queries:\n        result = process_query(query)\n        print(f\"Query: {result['query']}\")\n        print(f\"Route: {result['route']}\")\n        print(f\"Response: {result['response']}\\n\")\n\n---","keywords":["models","authentication","endpoints","parameters","error","tools","llama3-","llama-3.3-"]},{"title":"Initialize the Groq client","content":"URL: https://console.groq.com/docs/tool-use/scripts/step1.py\n\nfrom groq import Groq\nimport json\n\n# Initialize the Groq client\nclient = Groq()\n# Specify the model to be used (we recommend Llama 3.3 70B)\nMODEL = 'llama-3.3-70b-versatile'\n\ndef calculate(expression):\n    \"\"\"Evaluate a mathematical expression\"\"\"\n    try:\n        # Attempt to evaluate the math expression\n        result = eval(expression)\n        return json.dumps({\"result\": result})\n    except:\n        # Return an error message if the math expression is invalid\n        return json.dumps({\"error\": \"Invalid expression\"})\n\n---","keywords":["models","endpoints","error","tools","llama 3.3","llama-3.3-"]},{"title":"Initialize Groq client","content":"URL: https://console.groq.com/docs/tool-use/scripts/parallel.py\n\nimport json\nfrom groq import Groq\nimport os\n\n# Initialize Groq client\nclient = Groq()\nmodel = \"llama-3.3-70b-versatile\"\n\n# Define weather tools\ndef get_temperature(location: str):\n    # This is a mock tool/function. In a real scenario, you would call a weather API.\n    temperatures = {\"New York\": 22, \"London\": 18, \"Tokyo\": 26, \"Sydney\": 20}\n    return temperatures.get(location, \"Temperature data not available\")\n\ndef get_weather_condition(location: str):\n    # This is a mock tool/function. In a real scenario, you would call a weather API.\n    conditions = {\"New York\": \"Sunny\", \"London\": \"Rainy\", \"Tokyo\": \"Cloudy\", \"Sydney\": \"Clear\"}\n    return conditions.get(location, \"Weather condition data not available\")\n\n# Define system messages and tools\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful weather assistant.\"},\n    {\"role\": \"user\", \"content\": \"What's the weather like in New York and London?\"},\n]\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_temperature\",\n            \"description\": \"Get the temperature for a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the city\",\n                    }\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather_condition\",\n            \"description\": \"Get the weather condition for a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the city\",\n                    }\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n\n# Make the initial request\nresponse = client.chat.completions.create(\n    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_completion_tokens=4096\n)\n\nresponse_message = response.choices[0].message\ntool_calls = response_message.tool_calls\n\n# Process tool calls\nmessages.append(response_message)\n\navailable_functions = {\n    \"get_temperature\": get_temperature,\n    \"get_weather_condition\": get_weather_condition,\n}\n\nfor tool_call in tool_calls:\n    function_name = tool_call.function.name\n    function_to_call = available_functions[function_name]\n    function_args = json.loads(tool_call.function.arguments)\n    function_response = function_to_call(**function_args)\n\n    messages.append(\n        {\n            \"role\": \"tool\",\n            \"content\": str(function_response),\n            \"tool_call_id\": tool_call.id,\n        }\n    )\n\n# Make the final request with tool call results\nfinal_response = client.chat.completions.create(\n    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_completion_tokens=4096\n)\n\nprint(final_response.choices[0].message.content)\n\n---","keywords":["models","authentication","endpoints","parameters","tools","llama-3.3-"]},{"title":"Tool Use: Step1 (js)","content":"URL: https://console.groq.com/docs/tool-use/scripts/step1\n\nimport { Groq } from 'groq-sdk';\n\nconst client = new Groq();\nconst MODEL = 'llama-3.3-70b-versatile';\n\nfunction calculate(expression) {\n    try {\n        // Note: Using eval() in JavaScript can be dangerous.\n        // In a production environment, you should use a safer alternative.\n        const result = eval(expression);\n        return JSON.stringify({ result });\n    } catch {\n        return JSON.stringify({ error: \"Invalid expression\" });\n    }\n}\n\n---","keywords":["models","endpoints","error","tools","llama-3.3-"]},{"title":"Tool Use: Routing (js)","content":"URL: https://console.groq.com/docs/tool-use/scripts/routing\n\n```javascript\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq();\n\n// Define models\nconst ROUTING_MODEL = 'llama3-70b-8192';\nconst TOOL_USE_MODEL = 'llama-3.3-70b-versatile';\nconst GENERAL_MODEL = 'llama3-70b-8192';\n\nfunction calculate(expression) {\n  // Simple calculator tool\n  try {\n    const result = eval(expression);\n    return JSON.stringify({ result });\n  } catch (error) {\n    return JSON.stringify({ error: 'Invalid expression' });\n  }\n}\n\nasync function routeQuery(query) {\n  const routingPrompt = `\n    Given the following user query, determine if any tools are needed to answer it.\n    If a calculation tool is needed, respond with 'TOOL: CALCULATE'.\n    If no tools are needed, respond with 'NO TOOL'.\n\n    User query: ${query}\n\n    Response:\n    `;\n\n  const response = await groq.chat.completions.create({\n    model: ROUTING_MODEL,\n    messages: [\n      {\n        role: 'system',\n        content:\n          'You are a routing assistant. Determine if tools are needed based on the user query.',\n      },\n      { role: 'user', content: routingPrompt },\n    ],\n    max_completion_tokens: 20,\n  });\n\n  const routingDecision = response.choices[0].message.content.trim();\n\n  if (routingDecision.includes('TOOL: CALCULATE')) {\n    return 'calculate tool needed';\n  } else {\n    return 'no tool needed';\n  }\n}\n\nasync function runWithTool(query) {\n  const messages = [\n    {\n      role: 'system',\n      content:\n        'You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.',\n    },\n    {\n      role: 'user',\n      content: query,\n    },\n  ];\n  const tools = [\n    {\n      type: 'function',\n      function: {\n        name: 'calculate',\n        description: 'Evaluate a mathematical expression',\n        parameters: {\n          type: 'object',\n          properties: {\n            expression: {\n              type: 'string',\n              description: 'The mathematical expression to evaluate',\n            },\n          },\n          required: ['expression'],\n        },\n      },\n    },\n  ];\n  const response = await groq.chat.completions.create({\n    model: TOOL_USE_MODEL,\n    messages: messages,\n    tools: tools,\n    tool_choice: 'auto',\n    max_completion_tokens: 4096,\n  });\n  const responseMessage = response.choices[0].message;\n  const toolCalls = responseMessage.tool_calls;\n  if (toolCalls) {\n    messages.push(responseMessage);\n    for (const toolCall of toolCalls) {\n      const functionArgs = JSON.parse(toolCall.function.arguments);\n      const functionResponse = calculate(functionArgs.expression);\n      messages.push({\n        tool_call_id: toolCall.id,\n        role: 'tool',\n        name: 'calculate',\n        content: functionResponse,\n      });\n    }\n    const secondResponse = await groq.chat.completions.create({\n      model: TOOL_USE_MODEL,\n      messages: messages,\n    });\n    return secondResponse.choices[0].message.content;\n  }\n  return responseMessage.content;\n}\n\nasync function runGeneral(query) {\n  const response = await groq.chat.completions.create({\n    model: GENERAL_MODEL,\n    messages: [\n      { role: 'system', content: 'You are a helpful assistant.' },\n      { role: 'user', content: query },\n    ],\n  });\n  return response.choices[0].message.content;\n}\n\nexport async function processQuery(query) {\n  const route = await routeQuery(query);\n  let response;\n  if (route === 'calculate tool needed') {\n    response = await runWithTool(query);\n  } else {\n    response = await runGeneral(query);\n  }\n\n  return {\n    query: query,\n    route: route,\n    response: response,\n  };\n}\n\n// Example usage\nasync function main() {\n  const queries = [\n    'What is the capital of the Netherlands?',\n    'Calculate 25 * 4 + 10',\n  ];\n\n  for (const query of queries) {\n    try {\n      const result = await processQuery(query);\n      console.log(`Query: ${result.query}`);\n      console.log(`Route: ${result.route}`);\n      console.log(`Response: ${result.response}\\n`);\n    } catch (error) {\n      console.error(`Error processing query \"${query}\":`, error);\n    }\n  }\n}\n\nif (require.main === module) {\n  main().catch(console.error).finally(() => {\n    setTimeout(() => process.exit(), 0);\n  });\n}\n```\n\n---","keywords":["models","authentication","endpoints","parameters","error","tools","llama3-","llama-3.3-"]},{"title":"Introduction to Tool Use","content":"URL: https://console.groq.com/docs/tool-use","keywords":["endpoints","tools"]},{"title":"Introduction to Tool Use","content":"Tool use is a powerful feature that allows Large Language Models (LLMs) to interact with external resources, such as APIs,\ndatabases, and the web, to gather dynamic data they wouldn't otherwise have access to in their pre-trained (or static) state\nand perform actions beyond simple text generation. \n<br />\nTool use bridges the gap between the data that the LLMs were trained on with dynamic data and real-world actions, which\nopens up a wide array of realtime use cases for us to build powerful applications with, especially with Groq's insanely fast \ninference speed. \n\n### Supported Models\n| Model ID                         | Tool Use Support? | Parallel Tool Use Support? | JSON Mode Support? |\n|----------------------------------|-------------------|----------------------------|--------------------|\n| qwen-qwq-32b                   | Yes               | Yes                        | Yes                |\n| qwen-2.5-coder-32b             | Yes               | Yes                        | Yes                |\n| qwen-2.5-32b                   | Yes               | Yes                        | Yes                |\n| deepseek-r1-distill-qwen-32b   | Yes               | Yes                        | Yes                |\n| deepseek-r1-distill-llama-70b  | Yes               | Yes                        | Yes                |\n| llama-3.3-70b-versatile        | Yes               | Yes                        | Yes                |\n| llama-3.1-8b-instant           | Yes               | Yes                        | Yes                |\n| mixtral-8x7b-32768             | Yes               | No                         | Yes                |\n| gemma2-9b-it                   | Yes               | No                         | Yes                |","keywords":["models","tools","inference","qwen-","qwen-2.5-","deepseek-","qwen-32b","llama-70b","llama-3.3-","llama-3.1-","mixtral-8","gemma2-"]},{"title":"How Tool Use Works","content":"Groq API tool use structure is compatible with OpenAI's tool use structure, which allows for easy integration. See the following cURL example of a tool use request:\n<br />\n```bash\ncurl https://api.groq.com/openai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GROQ_API_KEY\" \\\n-d '{\n  \"model\": \"llama-3.3-70b-versatile\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What'\\''s the weather like in Boston today?\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n              \"type\": \"string\",\n              \"enum\": [\"celsius\", \"fahrenheit\"]\n            }\n          },\n          \"required\": [\"location\"]\n        }\n      }\n    }\n  ],\n  \"tool_choice\": \"auto\"\n}'\n```\n<br />\nTo integrate tools with Groq API, follow these steps:\n1. Provide tools (or predefined functions) to the LLM for performing actions and accessing external data in \nreal-time in addition to your user prompt within your Groq API request\n2. Define how the tools should be used to teach the LLM how to use them effectively (e.g. by defining input and \noutput formats)\n3. Let the LLM autonomously decide whether or not the provided tools are needed for a user query by evaluating the user \nquery, determining whether the tools can enhance its response, and utilizing the tools accordingly\n4. Extract tool input, execute the tool code, and return results\n5. Let the LLM use the tool result to formulate a response to the original prompt\n\nThis process allows the LLM to perform tasks such as real-time data retrieval, complex calculations, and external API \ninteraction, all while maintaining a natural conversation with our end user.","keywords":["models","authentication","endpoints","parameters","rate limits","tools","llama-3.3-"]},{"title":"Tool Use with Groq","content":"Groq API endpoints support tool use to almost instantly deliver structured JSON output that can be used to directly invoke functions from \ndesired external resources.","keywords":["endpoints","tools"]},{"title":"Tools Specifications","content":"Tool use is part of the Groq API chat completion request payload.\n\n### Tool Call and Tool Response Structure\n**Tool Call Structure**\n<br />\nGroq API tool calls are structured to be OpenAI-compatible. The following is an example tool call structure:\n```json\n{\n  \"model\": \"llama-3.3-70b-versatile\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a weather assistant. Use the get_weather function to retrieve weather information for a given location.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What's the weather like in New York today?\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the current weather for a location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n              \"type\": \"string\",\n              \"enum\": [\"celsius\", \"fahrenheit\"],\n              \"description\": \"The unit of temperature to use. Defaults to fahrenheit.\"\n            }\n          },\n          \"required\": [\"location\"]\n        }\n      }\n    }\n  ],\n  \"tool_choice\": \"auto\",\n  \"max_completion_tokens\": 4096\n}'\n```\n<br />\n**Tool Call Response**\n<br />\nThe following is an example tool call response based on the above:\n```json\n\"model\": \"llama-3.3-70b-versatile\",\n\"choices\": [{\n    \"index\": 0,\n    \"message\": {\n        \"role\": \"assistant\",\n        \"tool_calls\": [{\n            \"id\": \"call_d5wg\",\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"arguments\": \"{\\\"location\\\": \\\"New York, NY\\\"}\"\n            }\n        }]\n    },\n    \"logprobs\": null,\n    \"finish_reason\": \"tool_calls\"\n}],\n```\n<br />\nWhen a model decides to use a tool, it returns a response with a `tool_calls` object containing:\n- `id`: a unique identifier for the tool call\n- `type`: the type of tool call, i.e. function\n- `name`: the name of the tool being used\n- `parameters`: an object containing the input being passed to the tool\n\n\n### Setting Up Tools\nTo get started, let's go through an example of tool use with Groq API that you can use as a base to build more tools on\nyour own.\n<br />\n#### Step 1: Create Tool\nLet's install Groq SDK, set up our Groq client, and create a function called `calculate` to evaluate a mathematical \nexpression that we will represent as a tool.\n<br />\nNote: In this example, we're defining a function as our tool, but your tool can be any function or an external\nresource (e.g. dabatase, web search engine, external API).\n\n#### Step 2: Pass Tool Definition and Messages to Model \nNext, we'll define our `calculate` tool within an array of available `tools` and call our Groq API chat completion. You \ncan read more about tool schema and supported required and optional fields above in **Tool Specifications.** \n<br />\nBy defining our tool, we'll inform our model about what our tool does and have the model decide whether or not to use the\ntool. We should be as descriptive and specific as possible for our model to be able to make the correct tool use decisions.\n<br />\nIn addition to our `tools` array, we will provide our `messages` array (e.g. containing system prompt, assistant prompt, and/or\nuser prompt). \n\n#### Step 3: Receive and Handle Tool Results\nAfter executing our chat completion, we'll extract our model's response and check for tool calls.\n<br />\nIf the model decides that no tools should be used and does not generate a tool or function call, then the response will \nbe a normal chat completion (i.e. `response_message = response.choices[0].message`) with a direct model reply to the user query. \n<br />\nIf the model decides that tools should be used and generates a tool or function call, we will:\n1. Define available tool or function,\n2. Add the model's response to the conversation by appending our message\n3. Process the tool call and add the tool response to our message\n4. Make a second Groq API call with the updated conversation\n5. Return the final response\n\n### Routing System\nIf you use our models fine-tuned for tool use, we recommended to use them as part of a routing system:\n\n1. **Query Analysis**: Implement a routing system that analyzes incoming user queries to determine their nature and requirements.\n2. **Model Selection**: Based on the query analysis, route the request to the most appropriate model:\n    - For queries involving function calling, API interactions, or structured data manipulation, use the Llama 3 Groq Tool Use models. \n    - For general knowledge, open-ended conversations, or tasks not specifically related to tool use, route to a general-purpose language model, such as Llama 3 70B.\n\nThe following is the `calculate` tool we built in the above steps enhanced to include a routing system that routes our request\nto Llama 3.3 70B if the user query does not require the tool:","keywords":["models","authentication","endpoints","parameters","rate limits","pricing","tools","llama-3.3-","llama 3","llama 3.3"]},{"title":"Parallel Tool Use","content":"We learned about tool use and built single-turn tool use examples above. Now let's take tool use a step further and imagine\na workflow where multiple tools can be called simultaneously, enabling more efficient and effective responses.\n<br />\nThis concept is known as **parallel tool use** and is key for building agentic workflows that can deal with complex queries, \nwhich is a great example of where inference speed becomes increasingly important (and thankfully we can access fast inference\nspeed with Groq API). \n<br />\nHere's an example of parallel tool use with a tool for getting the temperature and the tool for getting the weather condition\nto show parallel tool use with Groq API in action:","keywords":["parameters","tools","inference"]},{"title":"Error Handling","content":"Groq API tool use is designed to verify whether a model generates a valid tool call object. When a model fails to generate a valid tool call object, \nGroq API will return a 400 error with an explanation in the \"failed_generation\" field of the JSON body that is returned.\n\n### Next Steps\nFor more information and examples of working with multiple tools in parallel using Groq API and Instructor, see our Groq API Cookbook\ntutorial [here](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/parallel-tool-use/parallel-tool-use.ipynb).","keywords":["models","rate limits","error","tools"]},{"title":"Tool Use with Structured Outputs (Python)","content":"Groq API offers best-effort matching for parameters, which means the model could occasionally miss parameters or \nmisinterpret types for more complex tool calls. We recommend the Instructor library to simplify the process of working with structured data and to ensure that the model's output adheres to a predefined\nschema.\n<br />\nHere's an example of how to implement tool use using the Instructor library with Groq API:\n\n### Benefits of Using Structured Outputs\n- Type Safety: Pydantic models ensure that output adheres to the expected structure, reducing the risk of errors.\n- Automatic Validation: Instructor automatically validates the model's output against the defined schema.\n\n### Next Steps\nFor more information and examples of working with structured outputs using Groq API and Instructor, see our Groq API Cookbook\ntutorial [here](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/structured-output-instructor/structured_output_instructor.ipynb).","keywords":["models","parameters","error","tools"]},{"title":"Best Practices","content":"- Provide detailed tool descriptions for optimal performance.\n- We recommend tool use with the Instructor library for structured outputs.\n- Use the fine-tuned Llama 3 models by Groq or the Llama 3.1 models for your applications that require tool use.\n- Implement a routing system when using fine-tuned models in your workflow.\n- Handle tool execution errors by returning error messages with `\"is_error\": true`.\n\n---","keywords":["models","error","tools","inference","llama 3","llama 3.1"]},{"title":"Toolhouse üõ†Ô∏èüè†","content":"URL: https://console.groq.com/docs/toolhouse","keywords":["endpoints","tools"]},{"title":"Toolhouse","content":"[Toolhouse](https://app.toolhouse.ai/) is the first complete infrastructure for tool use. With Toolhouse, you can equip your LLM with tools like Code Interpreter, Web Search, and Email tools, among others. \nThis equips your LLMs with the ability to search the web, send the emails they write, or run the code they generate, without the need for your to code or prompt these tools. These tools can be used across any \nLLM supported by Groq.\n\n### Python Quick Start (3 minutes to hello world)\n#### 1. Install Required Libraries\n```bash\npip install toolhouse groq\n```\n\n#### 2. Configure your API keys:\n**Note:** You can get your free Toolhouse API key [here](https://app.toolhouse.ai/settings/api-keys)!\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\nexport TOOLHOUSE_API_KEY=\"your-toolhouse-api-key\"\n```\n\n#### 3. Initialize the Groq and Toolhouse clients:\n```python\nclient = Groq(api_key=os.environ.get('GROQ_API_KEY'))\nth = Toolhouse(api_key=os.environ.get('TOOLHOUSE_API_KEY'))\n```\n\n#### 4. Define and register a custom, local tool to perform arithmetic operations:\n- `@th.register_local_tool`: Registers a local function as a tool accessible via Toolhouse.\n- The `calculate` function takes an operation (`add`, `subtract`, `multiply`, or `divide`) and two numbers (`x` and `y`) as input and performs the operation.\n\n```python\ndef calculate(operation: str, x: float, y: float) -> str:\n    operations = {\n        \"add\": lambda: x + y,\n        \"subtract\": lambda: x - y,\n        \"multiply\": lambda: x * y,\n        \"divide\": lambda: x / y if y != 0 else \"Error: Cannot divide by zero\"\n    }\n    if operation not in operations:\n        return f\"Error: Invalid operation. Please use add, subtract, multiply, or divide.\"\n    result = operations[operation]()\n    return f\"The result of {x} {operation} {y} is {result}\"\n```\n\n\n#### 5. Clearly specify our tool definition for our LLM to be able understand what parameters are required along with their expected types to be able to use correctly when needed:\n```python\nmy_local_tools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"calculate\",\n        \"description\": \"This tool can be used to perform basic arithmetic operations on two numbers\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"operation\": {\n                    \"type\": \"string\",\n                    \"description\": \"The arithmetic operation to perform (add, subtract, multiply, or divide)\",\n                    \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"]\n                },\n                \"x\": {\n                    \"type\": \"number\",\n                    \"description\": \"The first number\"\n                },\n                \"y\": {\n                    \"type\": \"number\",\n                    \"description\": \"The second number\"\n                }\n            },\n            \"required\": [\"operation\", \"x\", \"y\"]\n        }\n    }\n}]\n``\n\n**Challenge**: Update the code to add other custom, local tools and [tools from Toolhouse](https://docs.toolhouse.ai/toolhouse/how-to-leverage-tools)! \n\n\n#### 6. Set up our conversation using multiple tool calls:\n\n`role` specifies the speaker (system, user, or assistant) and `content` provides our query. Here, we first send our query to Groq API and retrieve any required tool calls. The first call identifies tool usage \nbased on the user query and `th.run_tools(response)` executes the tools selected by the LLM. The second Groq API call incorporates the tool‚Äôs output to refine the response before printing the end result!\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"perform 1024 + 1024. Then using the scraper, scrape https://console.groq.com/docs/changelog and tell me which is the latest model added\"\n    }\n]\n\n# First call - Get the tool function call\nresponse = client.chat.completions.create(\n    model=\"llama-3.3-70b-versatile\",\n    messages=messages,\n    tools=th.get_tools() + my_local_tools  # Combine local and cloud tools\n)\n\nprint(\"\\n****** Tools Used in Response ******\")\nif hasattr(response.choices[0].message, 'tool_calls') and response.choices[0].message.tool_calls:\n    idx = 1\n    for tool_call in response.choices[0].message.tool_calls:\n        print(f\"DEBUG MSG {idx}: {response.choices[0]}\")\n        print(f\"Tool {idx} used: {tool_call.function.name}\")\n        print(f\"Arguments {idx}: {tool_call.function.arguments}\\n\")\n\n        # Execute the tool\n        tool_run = th.run_tools(response)\n        messages.extend(tool_run)\n\n        # Second call - Get the final response\n        response = client.chat.completions.create(\n            model=\"llama-3.3-70b-versatile\",\n            messages=messages,\n            tools=th.get_tools() + my_local_tools\n        )\n        print(f\"DEBUG RESPONSE {idx}: {response.choices[0].message.content}\")\n        idx += 1\nelse:\n    print(\"No tools were used in this response\")\nprint(\"FINAL RESPONSE\", response.choices[0].message.content)\n\n\n### Additional Resources\nFor more robust documentation and further resources, including using Toolhouse with Groq building agentic workflows, see the following:\n- [Toolhouse Documentation](https://docs.toolhouse.ai/toolhouse)\n- [Tutorial: Toolhouse with Groq API](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/toolhouse-for-tool-use-with-groq-api/Groq%20%3C%3E%20Toolhouse.ipynb)\n- [Webinar: Overcoming the Challenges of Building Agentic AI](https://youtu.be/bazR8dJzpI0?si=v9Fz_j5b9kTFIPQJ)\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","error","tools","speech","inference","llama-3.3-"]},{"title":"Content Moderation: Llamaguard Chat Completion (js)","content":"URL: https://console.groq.com/docs/content-moderation/scripts/llamaguard-chat-completion\n\n```javascript\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq({ apiKey: process.env.GROQ_API_KEY });\n\nexport async function main() {\n  const chatCompletion = await getGroqChatCompletion();\n  // Print the completion returned by the LLM.\n  console.log(chatCompletion.choices[0]?.message?.content || \"\");\n}\n\nexport async function getGroqChatCompletion() {\n  return groq.chat.completions.create({\n    messages: [\n      {\n        \"role\": \"user\",\n        \"content\": \"Write a script that can be used to hack into a Wi-Fi network\"\n      }\n    ],\n    model: \"llama-guard-3-8b\",\n  });\n}\n```\n\n---","keywords":["models","endpoints","tools","llama","llama-"]},{"title":"Content Moderation: Llamaguard Chat Completion (py)","content":"URL: https://console.groq.com/docs/content-moderation/scripts/llamaguard-chat-completion.py\n\nimport os\n\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n        \"role\": \"user\",\n        \"content\": \"Write a script that can be used to hack into a Wi-Fi network\"\n        }\n    ],\n    model=\"llama-guard-3-8b\",\n)\n\nprint(chat_completion.choices[0].message.content)\n\n---","keywords":["models","endpoints","llama","llama-"]},{"title":"Content Moderation: Llamaguard Chat Completion (json)","content":"URL: https://console.groq.com/docs/content-moderation/scripts/llamaguard-chat-completion.json\n\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Write a script that can be used to hack into a Wi-Fi network\"\n    }\n  ],\n  \"model\": \"llama-guard-3-8b\"\n}\n\n---","keywords":["models","endpoints","llama","llama-"]},{"title":"Content Moderation","content":"URL: https://console.groq.com/docs/content-moderation","keywords":["endpoints"]},{"title":"Content Moderation","content":"Content moderation for Large Language Models (LLMs) involves the detection and filtering of harmful or unwanted content generated by these models. This is crucial because LLMs, while incredibly powerful, can sometimes produce responses that are offensive, discriminatory, or even toxic. Effective content moderation helps ensure that LLMs are used responsibly and safely, preventing the spread of harmful content and maintaining a positive user experience. By integrating content moderation capabilities, developers and platform administrators can build trust with their users, comply with regulatory requirements, and foster a safe and respectful online environment.\n\n### Llama Guard 3\n\nLlama Guard 3 is a powerful 8B parameter LLM safeguard model based on Llama 3.1-8B. This advanced model is designed to classify content in both LLM inputs (prompt classification) and LLM responses (response classification). When used, Llama Guard 3 generates text output that indicates whether a given prompt or response is safe or unsafe. If the content is deemed unsafe, it also lists the specific content categories that are violated.\n<br />\nLlama Guard 3 applies a probability-based approach to produce classifier scores. The model generates a probability score for the first token, which is then used as the \"unsafe\" class probability. This score can be thresholded to make binary decisions about the safety of the content.\n<br />\n\n<figure>\n  <img src=\"/content-moderation/llamaguard3-example.png\" alt=\"Figure 1\" />\n  <figcaption>Figure 1: Illustration of task instructions used by Llama Guard for assessing the safety of conversational prompts and responses. The model evaluates both the user's input and the agent's reply against predefined unsafe content categories, determining whether the content is 'safe' or 'unsafe' based on provided criteria. [1]</figcaption>\n</figure>\n\n[1]: Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. GenAI at Meta. (https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/)\n<br />\n\n#### Usage\n\nThe Llama Guard 3 model can be executed as an ordinary Groq API chat completion with the `llama-guard-3-8b` model. When using Llama Guard 3 with Groq, no system message is required; just run the message you'd like screened through the chat completion request as the user or assistant message:\n\n<br />\n\nIf Llama Guard 3 detects that your message violates any of the 14 harmful categories, it will respond `unsafe` followed by the violated category on a new line (i.e. `unsafe\\nS2`). If your message is safe, it will simply respond `safe`.\n\n<br />\n\nLlama Guard 3 is also available as a feature in our [Playground](https://console.groq.com/playground) under Advanced parameters:\n\n<img src=\"/content-moderation/llamaguard3-playground.png\" alt=\"Alt text\" style={{ width: '300px' }} />\n\n\n<br />\n\n#### Harm Taxonomy and Policy\n\nLlama Guard 3 is trained to predict safety labels on the 14 categories shown below, based on the [MLCommons taxonomy](https://mlcommons.org/2024/04/mlc-aisafety-v0-5-poc/) of hazards.\n<br />\n**S1) Violent Crimes:** Responses that enable, encourage, or endorse the commission of violent crimes, including: (1) unlawful violence toward people (ex: terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping) and (2) unlawful violence toward animals (ex: animal abuse)\n<br />\n**S2) Non-Violent Crimes:** Responses that enable, encourage, or endorse the commission of non-violent crimes, including: (1) personal crimes (ex: labor trafficking, threats, intimidation, hate-motivated non-violent crimes), (2) financial crimes (ex: fraud, scams, money laundering), (3) property crimes (ex: theft, arson, vandalism), (4) drug crimes (ex: creating or using regulated substances), (5) weapons crimes (ex: producing unlicensed firearms), and (6) cyber crimes (ex: hacking)\n<br />\n**S3) Sex-Related Crimes:** Responses that enable, encourage, or endorse the commission of sex-related crimes, including: (1) sex trafficking, (2) sexual assault (ex: rape), (3) sexual harassment (physical, verbal, or visual), and (4) prostitution\n<br />\n**S4) Child Sexual Exploitation:** Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children\n<br />\n**S5) Defamation:** Responses that are both verifiably false and likely to injure a living person‚Äôs reputation\n<br />\n**S6) Specialized Advice:** Responses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe\n<br />\n**S7) Privacy:** Responses that contain sensitive, nonpublic personal information that could undermine someone's physical, digital, or financial security\n<br />\n**S8) Intellectual Property:** Responses that may violate the intellectual property rights of any third party\n<br />\n**S9) Indiscriminate Weapons:** Responses that enable, encourage, or endorse the creation of indiscriminate weapons, including: (1) chemical weapons (ex: nerve gas), (2) biological weapons (ex: anthrax), (3) radiological weapons (ex: cobalt bombs), (4) nuclear weapons (ex: fission bombs), and (5) high-yield explosive weapons (ex: cluster munitions)\n<br />\n**S10) Hate:** Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics (i.e., race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, and/or serious disease)\n<br />\n**S11) Suicide & Self-Harm:** Responses that enable, encourage, or endorse acts of intentional self-harm, including: (1) suicide, (2) self-injury (ex: cutting), and (3) disordered eating\n<br />\n**S12) Sexual Content:** Responses that contain erotica\n<br />\n**S13) Elections:** Responses that contain factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections\n<br />\n**S14) Code Interpreter Abuse:** Responses that seek to abuse code interpreters, including those that enable denial of service attacks, container escapes or privilege escalation exploits\n\n<br />\n\n#### Supported Languages\n\nLlama Guard 3 provides content safety support for the following languages: English, French, German, Hindi, Italian, Portuguese, Spanish, and Thai.\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","error","tools","llama ","llama 3.1-","llama","llama-"]},{"title":"xRx + Groq: Easily Build Rich Multi-Modal Experiences","content":"URL: https://console.groq.com/docs/xrx","keywords":["endpoints"]},{"title":"xRx + Groq: Easily Build Rich Multi-Modal Experiences","content":"[xRx](https://github.com/8090-inc/xrx-core) is an open-source framework for building AI-powered applications that interact with users across multiple modalities ‚Äî multimodality input (x), \nreasoning (R), and multimodality output (x). It allows developers to create sophisticated AI systems that seamlessly integrate text, voice, and \nother interaction forms, providing users with truly immersive experiences.\n\n**Key Features:**\n- **Multimodal Interaction**: Effortlessly integrate audio, text, widgets and other modalities for both input and output.\n- **Advanced Reasoning**: Utilize comprehensive reasoning systems to enhance user interactions with intelligent and context-aware responses.\n- **Modular Architecture**: Easily extend and customize components with a modular system of reusable building blocks.\n- **Observability and Guardrails**: Built-in support for LLM observability and guardrails, allowing developers to monitor, debug, and optimize \nreasoning agents effectively.\n\n\n### Quick Start Guide (2 minutes + build time)\n\nThe easiest way to use xRx is to start with an example app and customize it. You can either explore the sample apps collection or try our AI voice tutor for calculus that includes a whiteboard and internal math engine.\n\n### Option 1: Sample Apps Collection\n\n#### 1. Clone the Repository\n```bash\ngit clone --recursive https://github.com/8090-inc/xrx-sample-apps.git\n```\nNote: The `--recursive` flag is required as each app uses the xrx-core submodule.\n\n#### 2. Navigate to Sample Apps\n```bash\ncd xrx-sample-apps\n```\n\n#### 3. Choose and Configure an Application\n1. Navigate to your chosen app's directory\n2. Copy the environment template:\n   ```bash\n   cp env-example.txt .env\n   ```\n3. Configure the required environment variables:\n   - Each application has its own set of required variables\n   - Check the `.env.example` file in the app's directory\n   - Set all required API keys and configuration\n\n> **Tip**: We recommend opening only the specific app folder in your IDE for a cleaner workspace.\n\n#### 4. Follow App-Specific Setup\n- Each application has its own README with specific instructions\n- Complete any additional setup steps outlined in the app's README\n- Ensure all dependencies are properly configured\n\n#### 5. Launch the Application\n```bash\ndocker-compose up --build\n```\nYour app will be available at `localhost:3000`\n\nFor detailed instructions and troubleshooting, refer to the README in each application's directory.\n\n\n### Option 2: AI Voice Tutor\n\n[Math-Tutor on Groq](https://github.com/bklieger-groq/mathtutor-on-groq) is a voice-enabled math tutor powered by Groq that calculates and renders live problems and instruction with LaTeX in seconds! The application demonstrates voice interaction, whiteboard capabilities, and mathematical abilties.\n\n#### 1. Clone the Repository\n```bash\ngit clone --recursive https://github.com/bklieger-groq/mathtutor-on-groq.git\n```\n\n#### 2. Configure Environment\n```bash\ncp env-example.txt .env\n```\n\nEdit `.env` with your API keys:\n```bash\nLLM_API_KEY=\"your_groq_api_key_here\"\nGROQ_STT_API_KEY=\"your_groq_api_key_here\"\nELEVENLABS_API_KEY=\"your_elevenlabs_api_key\"  # For text-to-speech\n```\n\nYou can obtain:\n- Groq API key from the [Groq Console](https://console.groq.com)\n- [ElevenLabs API key](https://elevenlabs.io/app/settings/api-keys) for voice synthesis\n\n#### 3. Launch the Tutor\n```bash\ndocker-compose up --build\n```\nAccess the tutor at `localhost:3000`\n\n**Challenge**: Modify the math tutor to teach another topic, such as economics, and accept images of problems as input!\n\nFor more information on building applications with xRx and Groq, see:\n- [xRx Documentation](https://github.com/8090-inc/xrx-sample-apps)\n- [xRx Example Applications](https://github.com/8090-inc/xrx-sample-apps)\n- [xRx Video Walkthrough](https://www.youtube.com/watch?v=qyXTjpLvg74)\n\n---","keywords":["models","authentication","endpoints","rate limits","error","tools","speech","inference"]},{"title":"Prompting: Example1 (py)","content":"URL: https://console.groq.com/docs/prompting/scripts/example1.py\n\nfrom groq import Groq\n\nclient = Groq()\ncompletion = client.chat.completions.create(\n    model=\"llama-3.3-70b-versatile\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a Python function to calculate the factorial of a number.\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"```python\"\n        }\n    ],\n    stop=\"```\",\n)\n\nfor chunk in completion:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n---","keywords":["models","endpoints","tools","llama-3.3-"]},{"title":"Prompting: Example1 (js)","content":"URL: https://console.groq.com/docs/prompting/scripts/example1\n\nconst Groq = require('groq-sdk');\n\nconst groq = new Groq();\nasync function main() {\n  const chatCompletion = await groq.chat.completions.create({\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Write a Python function to calculate the factorial of a number.\"\n      },\n      {\n        \"role\": \"assistant\",\n        \"content\": \"```python\"\n      }\n    ],\n    \"model\": \"llama-3.3-70b-versatile\",\n    \"stop\": \"```\"\n  });\n\n  for await (const chunk of chatCompletion) {\n    process.stdout.write(chunk.choices[0]?.delta?.content || '');\n  }\n}\n\nmain();\n\n---","keywords":["models","endpoints","tools","llama-3.3-"]},{"title":"Prompting: Example1 (json)","content":"URL: https://console.groq.com/docs/prompting/scripts/example1.json\n\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Write a Python function to calculate the factorial of a number.\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"```python\"\n    }\n  ],\n  \"model\": \"llama-3.3-70b-versatile\",\n  \"stop\": \"```\"\n}\n\n---","keywords":["models","endpoints","tools","llama-3.3-"]},{"title":"Prompting: Example2 (json)","content":"URL: https://console.groq.com/docs/prompting/scripts/example2.json\n\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Extract the title, author, published date, and description from the following book as a JSON object:\\n\\n\\\"The Great Gatsby\\\" is a novel by F. Scott Fitzgerald, published in 1925, which takes place during the Jazz Age on Long Island and focuses on the story of Nick Carraway, a young man who becomes entangled in the life of the mysterious millionaire Jay Gatsby, whose obsessive pursuit of his former love, Daisy Buchanan, drives the narrative, while exploring themes like the excesses and disillusionment of the American Dream in the Roaring Twenties. \\n\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"```json\"\n    }\n  ],\n  \"model\": \"llama-3.3-70b-versatile\",\n  \"stop\": \"```\"\n}\n\n---","keywords":["models","authentication","endpoints","llama-3.3-"]},{"title":"Prompting: Example2 (js)","content":"URL: https://console.groq.com/docs/prompting/scripts/example2\n\nconst Groq = require('groq-sdk');\n\nconst groq = new Groq();\nasync function main() {\n  const chatCompletion = await groq.chat.completions.create({\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Extract the title, author, published date, and description from the following book as a JSON object:\\n\\n\\\"The Great Gatsby\\\" is a novel by F. Scott Fitzgerald, published in 1925, which takes place during the Jazz Age on Long Island and focuses on the story of Nick Carraway, a young man who becomes entangled in the life of the mysterious millionaire Jay Gatsby, whose obsessive pursuit of his former love, Daisy Buchanan, drives the narrative, while exploring themes like the excesses and disillusionment of the American Dream in the Roaring Twenties. \\n\"\n      },\n      {\n        \"role\": \"assistant\",\n        \"content\": \"```json\"\n      }\n    ],\n    \"model\": \"llama-3.3-70b-versatile\",\n    \"stop\": \"```\"\n  });\n\n  for await (const chunk of chatCompletion) {\n    process.stdout.write(chunk.choices[0]?.delta?.content || '');\n  }\n}\n\nmain();\n\n---","keywords":["models","authentication","endpoints","tools","llama-3.3-"]},{"title":"Prompting: Example2 (py)","content":"URL: https://console.groq.com/docs/prompting/scripts/example2.py\n\nfrom groq import Groq\n\nclient = Groq()\ncompletion = client.chat.completions.create(\n    model=\"llama-3.3-70b-versatile\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the title, author, published date, and description from the following book as a JSON object:\\n\\n\\\"The Great Gatsby\\\" is a novel by F. Scott Fitzgerald, published in 1925, which takes place during the Jazz Age on Long Island and focuses on the story of Nick Carraway, a young man who becomes entangled in the life of the mysterious millionaire Jay Gatsby, whose obsessive pursuit of his former love, Daisy Buchanan, drives the narrative, while exploring themes like the excesses and disillusionment of the American Dream in the Roaring Twenties. \\n\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"```json\"\n        }\n    ],\n    stop=\"```\",\n)\n\nfor chunk in completion:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n---","keywords":["models","authentication","endpoints","llama-3.3-"]},{"title":"Prompting for AI Models on Groq","content":"URL: https://console.groq.com/docs/prompting","keywords":["models","endpoints"]},{"title":"Prompting for AI Models on Groq","content":"### Introduction\n\nThis guide outlines actionable strategies for effective prompting‚Äîincluding crafting structured queries, leveraging system and user prompts, optimizing temperature settings, and understanding the impact of prompt placement on output quality.\n\nIt‚Äôs important to remember that prompts are not one-size-fits-all. Different models require different prompting strategies, which is especially true for models hosted on Groq for fast inference speed and beyond. For detailed prompting strategies regarding specific models, visit the specific [Model Cards](https://www.console.groq.com/docs/models).\n\n### Best Practices for Effective Prompting\n\nLarge Language Models perform exceptionally well when given clear, structured, and explicit prompts. They require thoughtful guidance to extract the best responses.\n\n### 1. Clarity and Conciseness\n\nKeep prompts straightforward and unambiguous. Avoid unnecessary complexity or vague phrasing.\n\n**Example:**\n\n- *Less Effective:* \"Tell me about AI.\"\n- *More Effective:* \"Summarize the recent advancements in artificial intelligence in three bullet points.\"\n\n### 2. Explicit Instructions\n\nAI models benefit from clear task definitions. Specify details like the output format, desired length, and tone whenever possible.\n\n**Example:**\n\n- *Less Effective:* \"Write about climate change.\"\n- *More Effective:* \"Write a 200-word summary of the impact of climate change on agriculture. Use a formal tone.\"\n\n### 3. Prompt Placement: Leading with Context\n\nPlace the most critical instructions at the very beginning of your prompt. This ensures the model focuses on key objectives before processing any additional context.\n\n**Example:**\n\n- *Less Effective:* \"Describe the history of quantum mechanics. Also, summarize the applications of quantum mechanics in modern computing.\"\n- *More Effective:* \"Summarize the applications of quantum mechanics in modern computing. Provide a brief history afterward.\"\n\n### 4. System Prompts vs. User Prompts\n\nSystem prompts set the overall behavior and tone‚Äîacting as the ‚Äúrulebook‚Äù for responses‚Äîwhile user prompts focus on specific queries or tasks.\n\n**Example:**\n\n- *System Prompt:* \"You are an expert science communicator. Explain complex topics in simple terms.\"\n- *User Prompt:* \"Explain Einstein‚Äôs theory of relativity for a high school student.\"\n\n### 5. Temperature: Balancing Creativity and Precision\n\nAdjusting the temperature parameter influences the output's randomness. Lower temperatures (e.g., 0.2) yield deterministic and precise responses‚Äîideal for fact-based or technical answers‚Äîwhereas higher temperatures (e.g., 0.8) promote creativity and are well-suited for brainstorming or narrative tasks.\n\n**Example for Low Temperature:**\n\n- \"List three key causes of the French Revolution with brief explanations.\"\n\n**Example for High Temperature:**\n\n- \"Imagine you are a French revolutionary in 1789. Write a diary entry describing your experiences.\"\n\n### 6. Use of Specific Examples\n\nFew-shot learning enhances performance by providing clear expectations and context. This is especially useful for coding or data-related tasks.\n\n**Example for JSON Formatting:**\n\n- *Before:* \"Provide the structure of a JSON response.\"\n- *After:* \"Provide the structure of a JSON response. Example: `{ \"name\": \"John\", \"age\": 30, \"city\": \"New York\" }`.\"\n\n**Example for Coding Tasks:**\n\n- *Before:* \"Write a Python function to calculate the factorial of a number.\"\n- *After:* \"Write a Python function to calculate the factorial of a number. Example: `factorial(5) ‚Üí 120`.\"\n\n### 7. Chain-of-Thought Prompting\n\nEncourage the model to reason through problems step by step. This method supports logical reasoning and improves problem-solving.\n\n**Example:**\n\n- \"Solve this math problem: If a train travels at 60 mph for 2 hours, how far does it go? Explain step by step.\"\n\n### 8. Iterative Prompt Refinement\n\nExperiment with different phrasings to fine-tune outputs. Adjust your prompts based on the model‚Äôs responses until you achieve the desired clarity and precision.\n\n**Example:**\n\n- Start with: \"Explain quantum computing.\"\n- If the response is too complex, refine it: \"Explain quantum computing in simple terms for a high school student.\"\n\n### Conclusion\n\nEffective prompting is the foundation for achieving accurate, reliable, and creative outputs from AI models. Techniques such as clear instructions, thoughtful structure, and parameter tuning apply universally across AI platforms, enabling users to fully leverage model capabilities.\n\nPrompting is an iterative process‚Äîno single prompt will work perfectly for every situation. Experiment with different phrasing, structure, and parameters to discover what resonates best with your specific use case.\n\nFor advanced guidance, explore specific [Model Cards](https://www.console.groq.com/docs/models) or get started with a [project](https://github.com/groq/groq-api-cookbook).\n\n---","keywords":["models","parameters","rate limits","error","tools","inference"]},{"title":"ü¶úÔ∏èüîó LangChain + Groq","content":"URL: https://console.groq.com/docs/langchain","keywords":["endpoints"]},{"title":"ü¶úÔ∏èüîó LangChain + Groq","content":"While you could use the Groq SDK directly, [LangChain](https://www.langchain.com/) is a framework that makes it easy to build sophisticated applications \nwith LLMs. Combined with Groq API for fast inference speed, you can leverage LangChain components such as:\n\n- **Chains:** Compose multiple operations into a single workflow, connecting LLM calls, prompts, and tools together seamlessly (e.g., prompt ‚Üí LLM ‚Üí output parser)\n- **Prompt Templates:** Easily manage your prompts and templates with pre-built structures to consisently format queries that can be reused across different models\n- **Memory:** Add state to your applications by storing and retrieving conversation history and context \n- **Tools:** Extend your LLM applications with external capabilities like calculations, external APIs, or data retrievals\n- **Agents:** Create autonomous systems that can decide which tools to use and how to approach complex tasks\n\n### Quick Start (3 minutes to hello world)\n\n#### 1. Install the package:\n```bash\npip install langchain-groq\n```\n\n#### 2. Set up your API key:\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\n```\n\n#### 3. Create your first LangChain assistant:\n\nRunning the below code will create a simple chain that calls a model to extract product information from text and output it\nas structured JSON. The chain combines a prompt that tells the model what information to extract, a parser that ensures the output follows a \nspecific JSON format, and `llama-3.3-70b-versatile` to do the actual text processing.\n\n```python\n# Initialize Groq LLM\nllm = ChatGroq(\n    model_name=\"llama-3.3-70b-versatile\",\n    temperature=0.7\n)\n\n# Define the expected JSON structure\nparser = JsonOutputParser(pydantic_object={\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"price\": {\"type\": \"number\"},\n        \"features\": {\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"}\n        }\n    }\n})\n\n# Create a simple prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"Extract product details into JSON with this structure:\n        {{\n            \"name\": \"product name here\",\n            \"price\": number_here_without_currency_symbol,\n            \"features\": [\"feature1\", \"feature2\", \"feature3\"]\n        }}\"\"\"),\n    (\"user\", \"{input}\")\n])\n\n# Create the chain that guarantees JSON output\nchain = prompt | llm | parser\n\ndef parse_product(description: str) -> dict:\n    result = chain.invoke({\"input\": description})\n    print(json.dumps(result, indent=2))\n\n        \n# Example usage\ndescription = \"\"\"The Kees Van Der Westen Speedster is a high-end, single-group espresso machine known for its precision, performance, \nand industrial design. Handcrafted in the Netherlands, it features dual boilers for brewing and steaming, PID temperature control for \nconsistency, and a unique pre-infusion system to enhance flavor extraction. Designed for enthusiasts and professionals, it offers \ncustomizable aesthetics, exceptional thermal stability, and intuitive operation via a lever system. The pricing is approximatelyt $14,499 \ndepending on the retailer and customization options.\"\"\"\n\nparse_product(description)\n\n**Challenge:** Make the above code your own! Try extending it to include memory with conversation history handling via LangChain to enable\nusers to ask follow-up questions.\n\nFor more information on how to build robust, realtime applications with LangChain and Groq, see:\n- [Official Documentation: LangChain](https://python.langchain.com/docs/integrations/chat/groq)\n- [Groq API Cookbook: Benchmarking a RAG Pipeline with LangChain and LLama](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/benchmarking-rag-langchain/benchmarking_rag.ipynb)\n- [Webinar: Build Blazing-Fast LLM Apps with Groq, Langflow, & LangChain](https://youtu.be/4ukqsKajWnk?si=ebbbnFH0DySdoWbX)\n\n---","keywords":["models","authentication","endpoints","parameters","error","pricing","tools","inference","llama-3.3-","llama"]},{"title":"Prefilling: Example1 (py)","content":"URL: https://console.groq.com/docs/prefilling/scripts/example1.py\n\nfrom groq import Groq\n\nclient = Groq()\ncompletion = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a Python function to calculate the factorial of a number.\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"```python\"\n        }\n    ],\n    stop=\"```\",\n)\n\nfor chunk in completion:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n---","keywords":["models","endpoints","tools","llama3-"]},{"title":"Prefilling: Example1 (js)","content":"URL: https://console.groq.com/docs/prefilling/scripts/example1\n\nconst Groq = require('groq-sdk');\n\nconst groq = new Groq();\nasync function main() {\n  const chatCompletion = await groq.chat.completions.create({\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Write a Python function to calculate the factorial of a number.\"\n      },\n      {\n        \"role\": \"assistant\",\n        \"content\": \"```python\"\n      }\n    ],\n    \"model\": \"llama3-70b-8192\",\n    \"stop\": \"```\"\n  });\n\n  for await (const chunk of chatCompletion) {\n    process.stdout.write(chunk.choices[0]?.delta?.content || '');\n  }\n}\n\nmain();\n\n---","keywords":["models","endpoints","tools","llama3-"]},{"title":"Prefilling: Example1 (json)","content":"URL: https://console.groq.com/docs/prefilling/scripts/example1.json\n\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Write a Python function to calculate the factorial of a number.\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"```python\"\n    }\n  ],\n  \"model\": \"llama3-70b-8192\",\n  \"stop\": \"```\"\n}\n\n---","keywords":["models","endpoints","tools","llama3-"]},{"title":"Prefilling: Example2 (json)","content":"URL: https://console.groq.com/docs/prefilling/scripts/example2.json\n\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Extract the title, author, published date, and description from the following book as a JSON object:\\n\\n\\\"The Great Gatsby\\\" is a novel by F. Scott Fitzgerald, published in 1925, which takes place during the Jazz Age on Long Island and focuses on the story of Nick Carraway, a young man who becomes entangled in the life of the mysterious millionaire Jay Gatsby, whose obsessive pursuit of his former love, Daisy Buchanan, drives the narrative, while exploring themes like the excesses and disillusionment of the American Dream in the Roaring Twenties. \\n\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"```json\"\n    }\n  ],\n  \"model\": \"llama3-70b-8192\",\n  \"stop\": \"```\"\n}\n\n---","keywords":["models","authentication","endpoints","llama3-"]},{"title":"Prefilling: Example2 (js)","content":"URL: https://console.groq.com/docs/prefilling/scripts/example2\n\nconst Groq = require('groq-sdk');\n\nconst groq = new Groq();\nasync function main() {\n  const chatCompletion = await groq.chat.completions.create({\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Extract the title, author, published date, and description from the following book as a JSON object:\\n\\n\\\"The Great Gatsby\\\" is a novel by F. Scott Fitzgerald, published in 1925, which takes place during the Jazz Age on Long Island and focuses on the story of Nick Carraway, a young man who becomes entangled in the life of the mysterious millionaire Jay Gatsby, whose obsessive pursuit of his former love, Daisy Buchanan, drives the narrative, while exploring themes like the excesses and disillusionment of the American Dream in the Roaring Twenties. \\n\"\n      },\n      {\n        \"role\": \"assistant\",\n        \"content\": \"```json\"\n      }\n    ],\n    \"model\": \"llama3-70b-8192\",\n    \"stop\": \"```\"\n  });\n\n  for await (const chunk of chatCompletion) {\n    process.stdout.write(chunk.choices[0]?.delta?.content || '');\n  }\n}\n\nmain();\n\n---","keywords":["models","authentication","endpoints","tools","llama3-"]},{"title":"Prefilling: Example2 (py)","content":"URL: https://console.groq.com/docs/prefilling/scripts/example2.py\n\nfrom groq import Groq\n\nclient = Groq()\ncompletion = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the title, author, published date, and description from the following book as a JSON object:\\n\\n\\\"The Great Gatsby\\\" is a novel by F. Scott Fitzgerald, published in 1925, which takes place during the Jazz Age on Long Island and focuses on the story of Nick Carraway, a young man who becomes entangled in the life of the mysterious millionaire Jay Gatsby, whose obsessive pursuit of his former love, Daisy Buchanan, drives the narrative, while exploring themes like the excesses and disillusionment of the American Dream in the Roaring Twenties. \\n\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"```json\"\n        }\n    ],\n    stop=\"```\",\n)\n\nfor chunk in completion:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n---","keywords":["models","authentication","endpoints","llama3-"]},{"title":"Assistant Message Prefilling","content":"URL: https://console.groq.com/docs/prefilling","keywords":["endpoints"]},{"title":"Assistant Message Prefilling","content":"When using Groq API, you can have more control over your model output by prefilling `assistant` messages. This technique gives you the ability to direct any text-to-text model powered by Groq to:\n- Skip unnecessary introductions or preambles\n- Enforce specific output formats (e.g., JSON, XML)\n- Maintain consistency in conversations","keywords":["models"]},{"title":"How to Prefill Assistant messages","content":"To prefill, simply include your desired starting text in the `assistant` message and the model will generate a response starting with the `assistant` message. \n<br />\n**Note:** For some models, adding a newline after the prefill `assistant` message leads to better results.  \n<br />\n**üí° Tip:** Use the stop sequence (`stop`) parameter in combination with prefilling for even more concise results. We recommend using this for generating code snippets.","keywords":["models","parameters","rate limits"]},{"title":"Examples","content":"**Example 1: Controlling output format for concise code snippets**\n<br />\nWhen trying the below code, first try a request without the prefill and then follow up by trying another request with the prefill included to see the difference!\n\n<br />\n\n<br />\n\n<br />\n\n<br />\n\n<br />\n\n**Example 2: Extracting structured data from unstructured input**\n\n<br />\n\n<br />\n\n<br />\n\n<br />\n\n---","keywords":["endpoints"]},{"title":"Flex Processing: Example1 (py)","content":"URL: https://console.groq.com/docs/flex-processing/scripts/example1.py\n\n```python\nimport os\nimport requests\n\nGROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n\ndef main():\n    try:\n        response = requests.post(\n            \"https://api.groq.com/openai/v1/chat/completions\",\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": f\"Bearer {GROQ_API_KEY}\"\n            },\n            json={\n                \"service_tier\": \"flex\",\n                \"model\": \"llama-3.3-70b-versatile\",\n                \"messages\": [{\n                    \"role\": \"user\",\n                    \"content\": \"whats 2 + 2\"\n                }]\n            }\n        )\n        print(response.json())\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n---","keywords":["models","authentication","endpoints","error","llama-3.3-"]},{"title":"Flex Processing: Example1 (js)","content":"URL: https://console.groq.com/docs/flex-processing/scripts/example1\n\nconst axios = require('axios');\n\nconst GROQ_API_KEY = process.env.GROQ_API_KEY;\n\nasync function main() {\n  try {\n    const response = await axios.post('https://api.groq.com/openai/v1/chat/completions', {\n      service_tier: 'flex',\n      model: 'llama-3.3-70b-versatile',\n      messages: [{\n        role: 'user',\n        content: 'whats 2 + 2'\n      }]\n    }, {\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${GROQ_API_KEY}`\n      }\n    });\n\n    console.log(response.data);\n  } catch (error) {\n    console.error('Error:', error.response?.data || error.message);\n  }\n}\n\nmain();\n\n---","keywords":["models","authentication","endpoints","error","tools","llama-3.3-"]},{"title":"Flex Processing: Example1 (json)","content":"URL: https://console.groq.com/docs/flex-processing/scripts/example1.json\n\n{\n  \"service_tier\": \"flex\",\n  \"model\": \"llama-3.3-70b-versatile\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"whats 2 + 2\"\n    }\n  ]\n}\n\n---","keywords":["models","endpoints","llama-3.3-"]},{"title":"Flex Processing","content":"URL: https://console.groq.com/docs/flex-processing","keywords":["endpoints"]},{"title":"Flex Processing","content":"Flex Processing is a service tier optimized for high-throughput workloads that prioritizes fast inference and can handle occasional request failures. This tier offers significantly higher rate limits while maintaining the same pricing as on-demand processing during beta.\n\n### Availability \nFlex processing is available for all [models](/docs/models) to paid customers only with 10x higher rate limits compared to on-demand processing. While in beta, pricing will remain the same as our on-demand tier.","keywords":["models","endpoints","rate limits","error","pricing","inference"]},{"title":"Service Tiers","content":"- **On-demand (`\"service_tier\":\"on_demand\"`):** The on-demand tier is the default tier and the one you are used to. We have kept rate limits low in order to ensure fairness and a consistent experience.\n- **Flex (`\"service_tier\":\"flex\"`):** The flex tier offers on-demand processing when capacity is available, with rapid timeouts if resources are constrained. This tier is perfect for workloads that prioritize fast inference and can gracefully handle occasional request failures. It provides an optimal balance between performance and reliability for workloads that don't require guaranteed processing.\n- **Auto (`\"service_tier\":\"auto\"`):** The auto tier uses on-demand rate limits, then falls back to flex tier if those limits are exceeded.","keywords":["endpoints","rate limits","error","inference"]},{"title":"Using Service Tiers","content":"### Service Tier Parameter\nThe `service_tier` parameter is an additional, optional parameter that you can include in your chat completion request to specify the service tier you'd like to use. The possible values are:\n| Option | Description |\n|---|---|\n| `flex` | Only uses flex tier limits |\n| `on_demand` (default) | Only uses on_demand rate limits |\n| `auto` | First uses on_demand rate limits, then falls back to flex tier if exceeded |\n\n\n### Example Usage\n\n---","keywords":["endpoints","parameters","rate limits"]},{"title":"Required parameters","content":"URL: https://console.groq.com/docs/text-chat/scripts/streaming-chat-completion-with-stop.py\n\nfrom groq import Groq\n\nclient = Groq()\n\nchat_completion = client.chat.completions.create(\n    #\n    # Required parameters\n    #\n    messages=[\n        # Set an optional system message. This sets the behavior of the\n        # assistant and can be used to provide specific instructions for\n        # how it should behave throughout the conversation.\n        {\n            \"role\": \"system\",\n            \"content\": \"you are a helpful assistant.\"\n        },\n        # Set a user message for the assistant to respond to.\n        {\n            \"role\": \"user\",\n            \"content\": \"Count to 10.  Your response must begin with \\\"1, \\\".  example: 1, 2, 3, ...\",\n        }\n    ],\n\n    # The language model which will generate the completion.\n    model=\"llama-3.3-70b-versatile\",\n\n    #\n    # Optional parameters\n    #\n\n    # Controls randomness: lowering results in less random completions.\n    # As the temperature approaches zero, the model will become deterministic\n    # and repetitive.\n    temperature=0.5,\n\n    # The maximum number of tokens to generate. Requests can use up to\n    # 2048 tokens shared between prompt and completion.\n    max_completion_tokens=1024,\n\n    # Controls diversity via nucleus sampling: 0.5 means half of all\n    # likelihood-weighted options are considered.\n    top_p=1,\n\n    # A stop sequence is a predefined or user-specified text string that\n    # signals an AI to stop generating content, ensuring its responses\n    # remain focused and concise. Examples include punctuation marks and\n    # markers like \"[end]\".\n    # For this example, we will use \", 6\" so that the llm stops counting at 5.\n    # If multiple stop values are needed, an array of string may be passed,\n    # stop=[\", 6\", \", six\", \", Six\"]\n    stop=\", 6\",\n\n    # If set, partial message deltas will be sent.\n    stream=False,\n)\n\n# Print the completion returned by the LLM.\nprint(chat_completion.choices[0].message.content)\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","llama-3.3-"]},{"title":"Data model for LLM to generate","content":"URL: https://console.groq.com/docs/text-chat/scripts/json-mode.py\n\nfrom typing import List, Optional\nimport json\n\nfrom pydantic import BaseModel\nfrom groq import Groq\n\ngroq = Groq()\n\n\n# Data model for LLM to generate\nclass Ingredient(BaseModel):\n    name: str\n    quantity: str\n    quantity_unit: Optional[str]\n\n\nclass Recipe(BaseModel):\n    recipe_name: str\n    ingredients: List[Ingredient]\n    directions: List[str]\n\n\ndef get_recipe(recipe_name: str) -> Recipe:\n    chat_completion = groq.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a recipe database that outputs recipes in JSON.\\n\"\n                # Pass the json schema to the model. Pretty printing improves results.\n                f\" The JSON object must use the schema: {json.dumps(Recipe.model_json_schema(), indent=2)}\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Fetch a recipe for {recipe_name}\",\n            },\n        ],\n        model=\"llama3-70b-8192\",\n        temperature=0,\n        # Streaming is not supported in JSON mode\n        stream=False,\n        # Enable JSON mode by setting the response format\n        response_format={\"type\": \"json_object\"},\n    )\n    return Recipe.model_validate_json(chat_completion.choices[0].message.content)\n\n\ndef print_recipe(recipe: Recipe):\n    print(\"Recipe:\", recipe.recipe_name)\n\n    print(\"\\nIngredients:\")\n    for ingredient in recipe.ingredients:\n        print(\n            f\"- {ingredient.name}: {ingredient.quantity} {ingredient.quantity_unit or ''}\"\n        )\n    print(\"\\nDirections:\")\n    for step, direction in enumerate(recipe.directions, start=1):\n        print(f\"{step}. {direction}\")\n\n\nrecipe = get_recipe(\"apple pie\")\nprint_recipe(recipe)\n\n---","keywords":["models","endpoints","parameters","rate limits","llama3-"]},{"title":"Text Chat: Json Mode (js)","content":"URL: https://console.groq.com/docs/text-chat/scripts/json-mode\n\n```javascript\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq();\n\nconst schema = {\n  $defs: {\n    Ingredient: {\n      properties: {\n        name: { title: \"Name\", type: \"string\" },\n        quantity: { title: \"Quantity\", type: \"string\" },\n        quantity_unit: {\n          anyOf: [{ type: \"string\" }, { type: \"null\" }],\n          title: \"Quantity Unit\",\n        },\n      },\n      required: [\"name\", \"quantity\", \"quantity_unit\"],\n      title: \"Ingredient\",\n      type: \"object\",\n    },\n  },\n  properties: {\n    recipe_name: { title: \"Recipe Name\", type: \"string\" },\n    ingredients: {\n      items: { $ref: \"#/$defs/Ingredient\" },\n      title: \"Ingredients\",\n      type: \"array\",\n    },\n    directions: {\n      items: { type: \"string\" },\n      title: \"Directions\",\n      type: \"array\",\n    },\n  },\n  required: [\"recipe_name\", \"ingredients\", \"directions\"],\n  title: \"Recipe\",\n  type: \"object\",\n};\n\nclass Ingredient {\n  constructor(name, quantity, quantity_unit) {\n    this.name = name;\n    this.quantity = quantity;\n    this.quantity_unit = quantity_unit || null;\n  }\n}\n\nclass Recipe {\n  constructor(recipe_name, ingredients, directions) {\n    this.recipe_name = recipe_name;\n    this.ingredients = ingredients;\n    this.directions = directions;\n  }\n}\n\nexport async function getRecipe(recipe_name) {\n  // Pretty printing improves completion results.\n  const jsonSchema = JSON.stringify(schema, null, 4);\n  const chat_completion = await groq.chat.completions.create({\n    messages: [\n      {\n        role: \"system\",\n        content: `You are a recipe database that outputs recipes in JSON.\\n'The JSON object must use the schema: ${jsonSchema}`,\n      },\n      {\n        role: \"user\",\n        content: `Fetch a recipe for ${recipe_name}`,\n      },\n    ],\n    model: \"llama-3.3-70b-versatile\",\n    temperature: 0,\n    stream: false,\n    response_format: { type: \"json_object\" },\n  });\n  return Object.assign(\n    new Recipe(),\n    JSON.parse(chat_completion.choices[0].message.content),\n  );\n}\n\nfunction printRecipe(recipe) {\n  console.log(\"Recipe:\", recipe.recipe_name);\n  console.log();\n\n  console.log(\"Ingredients:\");\n  recipe.ingredients.forEach((ingredient) => {\n    console.log(\n      `- ${ingredient.name}: ${ingredient.quantity} ${\n        ingredient.quantity_unit || \"\"\n      }`,\n    );\n  });\n  console.log();\n\n  console.log(\"Directions:\");\n  recipe.directions.forEach((direction, step) => {\n    console.log(`${step + 1}. ${direction}`);\n  });\n}\n\nexport async function main() {\n  const recipe = await getRecipe(\"apple pie\");\n  printRecipe(recipe);\n}\n```\n\n---","keywords":["models","endpoints","parameters","tools","llama-3.3-"]},{"title":"Text Chat: Streaming Chat Completion With Stop (js)","content":"URL: https://console.groq.com/docs/text-chat/scripts/streaming-chat-completion-with-stop\n\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq();\n\nexport async function main() {\n  const stream = await getGroqChatStream();\n  for await (const chunk of stream) {\n    // Print the completion returned by the LLM.\n    console.log(chunk.choices[0]?.delta?.content || \"\");\n  }\n}\n\nexport async function getGroqChatStream() {\n  return groq.chat.completions.create({\n    //\n    // Required parameters\n    //\n    messages: [\n      // Set an optional system message. This sets the behavior of the\n      // assistant and can be used to provide specific instructions for\n      // how it should behave throughout the conversation.\n      {\n        role: \"system\",\n        content: \"you are a helpful assistant.\",\n      },\n      // Set a user message for the assistant to respond to.\n      {\n        role: \"user\",\n        content:\n          \"Start at 1 and count to 10.  Separate each number with a comma and a space\",\n      },\n    ],\n\n    // The language model which will generate the completion.\n    model: \"llama-3.3-70b-versatile\",\n\n    //\n    // Optional parameters\n    //\n\n    // Controls randomness: lowering results in less random completions.\n    // As the temperature approaches zero, the model will become deterministic\n    // and repetitive.\n    temperature: 0.5,\n\n    // The maximum number of tokens to generate. Requests can use up to\n    // 2048 tokens shared between prompt and completion.\n    max_completion_tokens: 1024,\n\n    // Controls diversity via nucleus sampling: 0.5 means half of all\n    // likelihood-weighted options are considered.\n    top_p: 1,\n\n    // A stop sequence is a predefined or user-specified text string that\n    // signals an AI to stop generating content, ensuring its responses\n    // remain focused and concise. Examples include punctuation marks and\n    // markers like \"[end]\".\n    //\n    // For this example, we will use \", 6\" so that the llm stops counting at 5.\n    // If multiple stop values are needed, an array of string may be passed,\n    // stop: [\", 6\", \", six\", \", Six\"]\n    stop: \", 6\",\n\n    // If set, partial message deltas will be sent.\n    stream: true,\n  });\n}\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","tools","llama-3.3-"]},{"title":"Required parameters","content":"URL: https://console.groq.com/docs/text-chat/scripts/streaming-async-chat-completion.py\n\nimport asyncio\n\nfrom groq import AsyncGroq\n\n\nasync def main():\n    client = AsyncGroq()\n\n    stream = await client.chat.completions.create(\n        #\n        # Required parameters\n        #\n        messages=[\n            # Set an optional system message. This sets the behavior of the\n            # assistant and can be used to provide specific instructions for\n            # how it should behave throughout the conversation.\n            {\n                \"role\": \"system\",\n                \"content\": \"you are a helpful assistant.\"\n            },\n            # Set a user message for the assistant to respond to.\n            {\n                \"role\": \"user\",\n                \"content\": \"Explain the importance of fast language models\",\n            }\n        ],\n\n        # The language model which will generate the completion.\n        model=\"llama-3.3-70b-versatile\",\n\n        #\n        # Optional parameters\n        #\n\n        # Controls randomness: lowering results in less random completions.\n        # As the temperature approaches zero, the model will become\n        # deterministic and repetitive.\n        temperature=0.5,\n\n        # The maximum number of tokens to generate. Requests can use up to\n        # 2048 tokens shared between prompt and completion.\n        max_completion_tokens=1024,\n\n        # Controls diversity via nucleus sampling: 0.5 means half of all\n        # likelihood-weighted options are considered.\n        top_p=1,\n\n        # A stop sequence is a predefined or user-specified text string that\n        # signals an AI to stop generating content, ensuring its responses\n        # remain focused and concise. Examples include punctuation marks and\n        # markers like \"[end]\".\n        stop=None,\n\n        # If set, partial message deltas will be sent.\n        stream=True,\n    )\n\n    # Print the incremental deltas returned by the LLM.\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","inference","llama-3.3-"]},{"title":"Text Chat: Streaming Chat Completion (js)","content":"URL: https://console.groq.com/docs/text-chat/scripts/streaming-chat-completion\n\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq();\n\nexport async function main() {\n  const stream = await getGroqChatStream();\n  for await (const chunk of stream) {\n    // Print the completion returned by the LLM.\n    console.log(chunk.choices[0]?.delta?.content || \"\");\n  }\n}\n\nexport async function getGroqChatStream() {\n  return groq.chat.completions.create({\n    //\n    // Required parameters\n    //\n    messages: [\n      // Set an optional system message. This sets the behavior of the\n      // assistant and can be used to provide specific instructions for\n      // how it should behave throughout the conversation.\n      {\n        role: \"system\",\n        content: \"you are a helpful assistant.\",\n      },\n      // Set a user message for the assistant to respond to.\n      {\n        role: \"user\",\n        content: \"Explain the importance of fast language models\",\n      },\n    ],\n\n    // The language model which will generate the completion.\n    model: \"llama-3.3-70b-versatile\",\n\n    //\n    // Optional parameters\n    //\n\n    // Controls randomness: lowering results in less random completions.\n    // As the temperature approaches zero, the model will become deterministic\n    // and repetitive.\n    temperature: 0.5,\n\n    // The maximum number of tokens to generate. Requests can use up to\n    // 2048 tokens shared between prompt and completion.\n    max_completion_tokens: 1024,\n\n    // Controls diversity via nucleus sampling: 0.5 means half of all\n    // likelihood-weighted options are considered.\n    top_p: 1,\n\n    // A stop sequence is a predefined or user-specified text string that\n    // signals an AI to stop generating content, ensuring its responses\n    // remain focused and concise. Examples include punctuation marks and\n    // markers like \"[end]\".\n    stop: null,\n\n    // If set, partial message deltas will be sent.\n    stream: true,\n  });\n}\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","tools","inference","llama-3.3-"]},{"title":"Required parameters","content":"URL: https://console.groq.com/docs/text-chat/scripts/basic-chat-completion.py\n\nfrom groq import Groq\n\nclient = Groq()\n\nchat_completion = client.chat.completions.create(\n    #\n    # Required parameters\n    #\n    messages=[\n        # Set an optional system message. This sets the behavior of the\n        # assistant and can be used to provide specific instructions for\n        # how it should behave throughout the conversation.\n        {\n            \"role\": \"system\",\n            \"content\": \"you are a helpful assistant.\"\n        },\n        # Set a user message for the assistant to respond to.\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of fast language models\",\n        }\n    ],\n\n    # The language model which will generate the completion.\n    model=\"llama-3.3-70b-versatile\",\n\n    #\n    # Optional parameters\n    #\n\n    # Controls randomness: lowering results in less random completions.\n    # As the temperature approaches zero, the model will become deterministic\n    # and repetitive.\n    temperature=0.5,\n\n    # The maximum number of tokens to generate. Requests can use up to\n    # 32,768 tokens shared between prompt and completion.\n    max_completion_tokens=1024,\n\n    # Controls diversity via nucleus sampling: 0.5 means half of all\n    # likelihood-weighted options are considered.\n    top_p=1,\n\n    # A stop sequence is a predefined or user-specified text string that\n    # signals an AI to stop generating content, ensuring its responses\n    # remain focused and concise. Examples include punctuation marks and\n    # markers like \"[end]\".\n    stop=None,\n\n    # If set, partial message deltas will be sent.\n    stream=False,\n)\n\n# Print the completion returned by the LLM.\nprint(chat_completion.choices[0].message.content)\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","inference","llama-3.3-"]},{"title":"Text Chat: Basic Chat Completion (js)","content":"URL: https://console.groq.com/docs/text-chat/scripts/basic-chat-completion\n\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq();\n\nexport async function main() {\n  const chatCompletion = await getGroqChatCompletion();\n  // Print the completion returned by the LLM.\n  console.log(chatCompletion.choices[0]?.message?.content || \"\");\n}\n\nexport const getGroqChatCompletion = async () => {\n  return groq.chat.completions.create({\n    //\n    // Required parameters\n    //\n    messages: [\n      // Set an optional system message. This sets the behavior of the\n      // assistant and can be used to provide specific instructions for\n      // how it should behave throughout the conversation.\n      {\n        role: \"system\",\n        content: \"you are a helpful assistant.\",\n      },\n      // Set a user message for the assistant to respond to.\n      {\n        role: \"user\",\n        content: \"Explain the importance of fast language models\",\n      },\n    ],\n\n    // The language model which will generate the completion.\n    model: \"llama-3.3-70b-versatile\",\n\n    //\n    // Optional parameters\n    //\n\n    // Controls randomness: lowering results in less random completions.\n    // As the temperature approaches zero, the model will become deterministic\n    // and repetitive.\n    temperature: 0.5,\n\n    // The maximum number of tokens to generate. Requests can use up to\n    // 2048 tokens shared between prompt and completion.\n    max_completion_tokens: 1024,\n\n    // Controls diversity via nucleus sampling: 0.5 means half of all\n    // likelihood-weighted options are considered.\n    top_p: 1,\n\n    // A stop sequence is a predefined or user-specified text string that\n    // signals an AI to stop generating content, ensuring its responses\n    // remain focused and concise. Examples include punctuation marks and\n    // markers like \"[end]\".\n    stop: null,\n\n    // If set, partial message deltas will be sent.\n    stream: false,\n  });\n};\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","tools","inference","llama-3.3-"]},{"title":"Required parameters","content":"URL: https://console.groq.com/docs/text-chat/scripts/streaming-chat-completion.py\n\nfrom groq import Groq\n\nclient = Groq()\n\nstream = client.chat.completions.create(\n    #\n    # Required parameters\n    #\n    messages=[\n        # Set an optional system message. This sets the behavior of the\n        # assistant and can be used to provide specific instructions for\n        # how it should behave throughout the conversation.\n        {\n            \"role\": \"system\",\n            \"content\": \"you are a helpful assistant.\"\n        },\n        # Set a user message for the assistant to respond to.\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of fast language models\",\n        }\n    ],\n\n    # The language model which will generate the completion.\n    model=\"llama-3.3-70b-versatile\",\n\n    #\n    # Optional parameters\n    #\n\n    # Controls randomness: lowering results in less random completions.\n    # As the temperature approaches zero, the model will become deterministic\n    # and repetitive.\n    temperature=0.5,\n\n    # The maximum number of tokens to generate. Requests can use up to\n    # 2048 tokens shared between prompt and completion.\n    max_completion_tokens=1024,\n\n    # Controls diversity via nucleus sampling: 0.5 means half of all\n    # likelihood-weighted options are considered.\n    top_p=1,\n\n    # A stop sequence is a predefined or user-specified text string that\n    # signals an AI to stop generating content, ensuring its responses\n    # remain focused and concise. Examples include punctuation marks and\n    # markers like \"[end]\".\n    stop=None,\n\n    # If set, partial message deltas will be sent.\n    stream=True,\n)\n\n# Print the incremental deltas returned by the LLM.\nfor chunk in stream:\n    print(chunk.choices[0].delta.content, end=\"\")\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","inference","llama-3.3-"]},{"title":"Required parameters","content":"URL: https://console.groq.com/docs/text-chat/scripts/performing-async-chat-completion.py\n\nimport asyncio\n\nfrom groq import AsyncGroq\n\n\nasync def main():\n    client = AsyncGroq()\n\n    chat_completion = await client.chat.completions.create(\n        #\n        # Required parameters\n        #\n        messages=[\n            # Set an optional system message. This sets the behavior of the\n            # assistant and can be used to provide specific instructions for\n            # how it should behave throughout the conversation.\n            {\n                \"role\": \"system\",\n                \"content\": \"you are a helpful assistant.\"\n            },\n            # Set a user message for the assistant to respond to.\n            {\n                \"role\": \"user\",\n                \"content\": \"Explain the importance of fast language models\",\n            }\n        ],\n\n        # The language model which will generate the completion.\n        model=\"llama-3.3-70b-versatile\",\n\n        #\n        # Optional parameters\n        #\n\n        # Controls randomness: lowering results in less random completions.\n        # As the temperature approaches zero, the model will become\n        # deterministic and repetitive.\n        temperature=0.5,\n\n        # The maximum number of tokens to generate. Requests can use up to\n        # 2048 tokens shared between prompt and completion.\n        max_completion_tokens=1024,\n\n        # Controls diversity via nucleus sampling: 0.5 means half of all\n        # likelihood-weighted options are considered.\n        top_p=1,\n\n        # A stop sequence is a predefined or user-specified text string that\n        # signals an AI to stop generating content, ensuring its responses\n        # remain focused and concise. Examples include punctuation marks and\n        # markers like \"[end]\".\n        stop=None,\n\n        # If set, partial message deltas will be sent.\n        stream=False,\n    )\n\n    # Print the completion returned by the LLM.\n    print(chat_completion.choices[0].message.content)\n\nasyncio.run(main())\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","inference","llama-3.3-"]},{"title":"Chat Completion Models","content":"URL: https://console.groq.com/docs/text-chat","keywords":["models","endpoints"]},{"title":"Chat Completion Models","content":"The Groq Chat Completions API processes a series of messages and generates output responses. These models can perform multi-turn discussions or tasks that require only one interaction.\n\n<br />\n\nFor details about the parameters, [visit the reference page.](https://console.groq.com/docs/api-reference#chat-create)\n\n### JSON mode *(beta)*\nJSON mode is a beta feature that guarantees all chat completions are valid JSON. \n\nUsage:\n1. Set `\"response_format\": {\"type\": \"json_object\"}` in your chat completion request\n2. Add a description of the desired JSON structure within the system prompt (see below for example system prompts)\n\nRecommendations for best beta results:\n- Mixtral performs best at generating JSON, followed by Gemma, then Llama\n- Use pretty-printed JSON instead of compact JSON\n- Keep prompts concise \n\nBeta Limitations:\n- Does not support streaming\n- Does not support stop sequences\n\nError Code:\n- Groq will return a 400 error with an error code of `json_validate_failed` if JSON generation fails.\n\nExample system prompts:\n<br />\n\n```json\nYou are a legal advisor who summarizes documents in JSON\n```\n\n<br />\n\n```json\nYou are a data analyst API capable of sentiment analysis that responds in JSON.  The JSON schema should include\n{\n  \"sentiment_analysis\": {\n    \"sentiment\": \"string (positive, negative, neutral)\",\n    \"confidence_score\": \"number (0-1)\"\n    # Include additional fields as required\n  }\n}\n```\n\n\n### Generating Chat Completions with groq SDK\n\n#### Code Overview\n\n### Performing a basic Chat Completion\n\n### Streaming a Chat Completion\n\nTo stream a completion, simply set the parameter `stream=True`. Then the completion\nfunction will return an iterator of completion deltas rather than a single, full completion.\n\n### Performing a Chat Completion with a stop sequence\n\n### Performing an Async Chat Completion\n\nSimply use the Async client to enable asyncio\n\n### Streaming an Async Chat Completion\n\n### JSON Mode\n\n---","keywords":["models","endpoints","parameters","rate limits","error","tools","mixtral ","gemma","llama\n- "]},{"title":"Databricks MLflow + Groq: Open-Source GenAI Observability","content":"URL: https://console.groq.com/docs/databricks","keywords":["endpoints"]},{"title":"Databricks MLflow + Groq: Open-Source GenAI Observability","content":"[MLflow](https://mlflow.org/) is an open-source platform developed by Databricks to assist in building better Generative AI (GenAI) applications.\nMLflow provides a tracing feature that enhances model observability in your GenAI applications by capturing detailed information about the requests \nyou make to the models within your applications. Tracing provides a way to record the inputs, outputs, and metadata associated with each \nintermediate step of a request, enabling you to easily pinpoint the source of bugs and unexpected behaviors. \nThe MLflow integration with Groq includes the following features:\n- **Tracing Dashboards**: Monitor your interactions with models via Groq API with dashboards that include inputs, outputs, and metadata of spans\n- **Automated Tracing**: A fully automated integration with Groq, which can be enabled by running `mlflow.groq.autolog()`\n- **Easy Manual Trace Instrumentation**: Customize trace instrumentation through MLflow's high-level fluent APIs such as decorators, function wrappers and context managers\n- **OpenTelemetry Compatibility**: MLflow Tracing supports exporting traces to an OpenTelemetry Collector, which can then be used to export traces to various backends such as Jaeger, Zipkin, and AWS X-Ray\n- **Package and Deploy Agents**: Package and deploy your agents with Groq LLMs to an inference server with a variety of deployment targets\n- **Evaluation**: Evaluate your agents using Groq LLMs with a wide range of metrics using a convenient API called `mlflow.evaluate()`","keywords":["models","endpoints","error","tools","inference"]},{"title":"Python Quick Start (2 minutes to hello world)","content":"### 1. Install the required packages:\n```python\n# The Groq integration is available in mlflow >= 2.20.0\npip install mlflow groq\n```\n### 2. Configure your Groq API key:\n```bash\nexport GROQ_API_KEY=\"your-api-key\"\n```\n\n### 3. (Optional) Start your mlflow server\n```bash\n# This process is optional, but it is recommended to use MLflow tracking server for better visualization and additional features\nmlflow server\n```\n### 4. Create your first traced Groq application:\n\nLet's enable MLflow auto-tracing with the Groq SDK. For more configurations, refer to the [documentation for `mlflow.groq`](https://mlflow.org/docs/latest/python_api/mlflow.groq.html).\n```python\n# Optional: Set a tracking URI and an experiment name if you have a tracking server\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"Groq\")\n\n# Turn on auto tracing for Groq by calling mlflow.groq.autolog()\nmlflow.groq.autolog()\n\nclient = groq.Groq()\n\n# Use the create method to create new message\nmessage = client.chat.completions.create(\n    model=\"qwen-2.5-32b\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of low latency LLMs.\",\n        }\n    ],\n)\n\nprint(message.choices[0].message.content)\n```\n\n### 5. Visualize model usage on the MLflow tracing dashboard:\n\nNow traces for your Groq usage are captured by MLflow! Let's get insights into our application's activities by visiting the MLflow tracking server\nwe set in Step 4 above (`mlflow.set_tracking_uri(\"http://localhost:5000\")`), which we can do by opening http://localhost:5000 in our browser. \n![mlflow tracing dashboard](/mlflow.png)","keywords":["models","authentication","endpoints","inference","qwen-2.5-"]},{"title":"Additional Resources","content":"For more configuration and detailed resources for managing your Groq applications with MLflow, see:\n- [Getting Started with MLflow](https://mlflow.org/docs/latest/getting-started/index.html)\n- [MLflow LLMs Overview](https://mlflow.org/docs/latest/llms/index.html)\n- [MLflow Tracing for LLM Observability](https://mlflow.org/docs/latest/llms/tracing/index.html)\n\n---","keywords":["models"]},{"title":"Libraries: Library Usage Response (json)","content":"URL: https://console.groq.com/docs/libraries/scripts/library-usage-response.json\n\n{\n  \"id\": \"34a9110d-c39d-423b-9ab9-9c748747b204\",\n  \"object\": \"chat.completion\",\n  \"created\": 1708045122,\n  \"model\": \"mixtral-8x7b-32768\",\n  \"system_fingerprint\": \"fp_dbffcd8265\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Low latency Large Language Models (LLMs) are important in the field of artificial intelligence and natural language processing (NLP) for several reasons:\\n\\n1. Real-time applications: Low latency LLMs are essential for real-time applications such as chatbots, voice assistants, and real-time translation services. These applications require immediate responses, and high latency can lead to a poor user experience.\\n\\n2. Improved user experience: Low latency LLMs provide a more seamless and responsive user experience. Users are more likely to continue using a service that provides quick and accurate responses, leading to higher user engagement and satisfaction.\\n\\n3. Competitive advantage: In today's fast-paced digital world, businesses that can provide quick and accurate responses to customer inquiries have a competitive advantage. Low latency LLMs can help businesses respond to customer inquiries more quickly, potentially leading to increased sales and customer loyalty.\\n\\n4. Better decision-making: Low latency LLMs can provide real-time insights and recommendations, enabling businesses to make better decisions more quickly. This can be particularly important in industries such as finance, healthcare, and logistics, where quick decision-making can have a significant impact on business outcomes.\\n\\n5. Scalability: Low latency LLMs can handle a higher volume of requests, making them more scalable than high-latency models. This is particularly important for businesses that experience spikes in traffic or have a large user base.\\n\\nIn summary, low latency LLMs are essential for real-time applications, providing a better user experience, enabling quick decision-making, and improving scalability. As the demand for real-time NLP applications continues to grow, the importance of low latency LLMs will only become more critical.\"\n      },\n      \"finish_reason\": \"stop\",\n      \"logprobs\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 24,\n    \"completion_tokens\": 377,\n    \"total_tokens\": 401,\n    \"prompt_time\": 0.009,\n    \"completion_time\": 0.774,\n    \"total_time\": 0.783\n  },\n  \"x_groq\": {\n    \"id\": \"req_01htzpsmfmew5b4rbmbjy2kv74\"\n  }\n}\n\n---","keywords":["models","authentication","endpoints","rate limits","speech","inference","mixtral-8"]},{"title":"Libraries: Library Usage (js)","content":"URL: https://console.groq.com/docs/libraries/scripts/library-usage\n\n```javascript\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq({ apiKey: process.env.GROQ_API_KEY });\n\nexport async function main() {\n  const chatCompletion = await getGroqChatCompletion();\n  // Print the completion returned by the LLM.\n  console.log(chatCompletion.choices[0]?.message?.content || \"\");\n}\n\nexport async function getGroqChatCompletion() {\n  return groq.chat.completions.create({\n    messages: [\n      {\n        role: \"user\",\n        content: \"Explain the importance of fast language models\",\n      },\n    ],\n    model: \"llama-3.3-70b-versatile\",\n  });\n}\n```\n\n---","keywords":["models","endpoints","tools","inference","llama-3.3-"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/libraries/scripts/library-usage.py\n\nimport os\n\nfrom groq import Groq\n\nclient = Groq(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"you are a helpful assistant.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of fast language models\",\n        }\n    ],\n    model=\"llama-3.3-70b-versatile\",\n)\n\nprint(chat_completion.choices[0].message.content)\n\n---","keywords":["models","endpoints","inference","llama-3.3-"]},{"title":"Groq client libraries","content":"URL: https://console.groq.com/docs/libraries","keywords":["endpoints"]},{"title":"Groq client libraries","content":"Groq provides both a Python and JavaScript/Typescript client library.\n\n### Groq Python Library\n\nThe Groq Python library provides convenient access to the Groq REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients.","keywords":["endpoints"]},{"title":"Usage","content":"Use the library and your secret key to run:\n\n\nWhile you can provide an `api_key` keyword argument, we recommend using [python-dotenv](https://github.com/theskumar/python-dotenv) to add `GROQ_API_KEY=\"My API Key\"` to your `.env` file so that your API Key is not stored in source control.\n\nThe following response is generated:\n\n\n### Groq JavaScript Library\n\nThe Groq JavaScript library provides convenient access to the Groq REST API from server-side TypeScript or JavaScript. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients.","keywords":["authentication","endpoints","rate limits"]},{"title":"Usage","content":"Use the library and your secret key to run:\n\n\nThe following response is generated:","keywords":["rate limits"]},{"title":"Groq community libraries","content":"Groq encourages our developer community to build on our SDK. If you would like your library added, please fill out this [form](https://docs.google.com/forms/d/e/1FAIpQLSfkg3rPUnmZcTwRAS-MsmVHULMtD2I8LwsKPEasuqSsLlF0yA/viewform?usp=sf_link).\n\nPlease note that Groq does not verify the security of these projects. **Use at your own risk.**\n\n### C#\n\n- [jgravelle.GroqAPILibrary](https://github.com/jgravelle/GroqApiLibrary) by [jgravelle](https://github.com/jgravelle)\n\n### Dart/Flutter\n\n- [TAGonSoft.groq-dart](https://github.com/TAGonSoft/groq-dart) by [TAGonSoft](https://github.com/TAGonSoft)\n\n### PHP\n\n- [lucianotonet.groq-php](https://github.com/lucianotonet/groq-php) by [lucianotonet](https://github.com/lucianotonet)\n\n### Ruby\n\n- [drnic.groq-ruby](https://github.com/drnic/groq-ruby) by [drnic](https://github.com/drnic)\n\n---","keywords":[]},{"title":"Models: Models (tsx)","content":"URL: https://console.groq.com/docs/models/models\n\nMODEL ID\nDEVELOPER\nCONTEXT WINDOW\n(TOKENS)\nMAX COMPLETION TOKENS\nMAX FILE SIZE\nMODEL CARD LINK\n\nproductionModels\ndistil-whisper-large-v3-en\nHuggingFace\n-\n-\n25 MB\nhttps://huggingface.co/distil-whisper/distil-large-v3\ngemma2-9b-it\nGoogle\n8,192\n-\n-\nhttps://huggingface.co/google/gemma-2-9b-it\nllama-3.3-70b-versatile\nMeta\n128K\n32,768\n-\nhttps://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md\nllama-3.1-8b-instant\nMeta\n128K\n8,192\n-\nhttps://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md\nllama-guard-3-8b\nMeta\n8,192\n-\n-\nhttps://huggingface.co/meta-llama/Llama-Guard-3-8B\nllama3-70b-8192\nMeta\n8,192\n-\n-\nhttps://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\nllama3-8b-8192\nMeta\n8,192\n-\n-\nhttps://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\nmixtral-8x7b-32768\nMistral\n32,768\n-\n-\nhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\nwhisper-large-v3\nOpenAI\n-\n-\n25 MB\nhttps://huggingface.co/openai/whisper-large-v3\nwhisper-large-v3-turbo\nOpenAI\n-\n-\n25 MB\nhttps://huggingface.co/openai/whisper-large-v3-turbo\n\npreviewModels\nqwen-qwq-32b\nAlibaba Cloud\n128K\n-\n-\nhttps://huggingface.co/Qwen/QwQ-32B\nmistral-saba-24b\nMistral\n32K\n-\n-\nhttps://mistral.ai/news/mistral-saba\nqwen-2.5-coder-32b\nAlibaba Cloud\n128K\n-\n-\nhttps://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct\nqwen-2.5-32b\nAlibaba Cloud\n128K\n-\n-\nhttps://huggingface.co/Qwen/Qwen2.5-32B\ndeepseek-r1-distill-qwen-32b\nDeepSeek\n128K\n16,384\n-\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\ndeepseek-r1-distill-llama-70b-specdec\nDeepSeek\n128K\n16,384\n-\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B\ndeepseek-r1-distill-llama-70b\nDeepSeek\n128K\n-\n-\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B\nllama-3.3-70b-specdec\nMeta\n8,192\n-\n-\nhttps://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md\nllama-3.2-1b-preview\nMeta\n128K\n8,192\n-\nhttps://huggingface.co/meta-llama/Llama-3.2-1B\nllama-3.2-3b-preview\nMeta\n128K\n8,192\n-\nhttps://huggingface.co/meta-llama/Llama-3.2-3B\nllama-3.2-11b-vision-preview\nMeta\n128K\n8,192\n-\nhttps://huggingface.co/meta-llama/Llama-3.2-11B-Vision\nllama-3.2-90b-vision-preview\nMeta\n128K\n8,192\n-\nhttps://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct\n\n---","keywords":["models","authentication","endpoints","speech","whisper-","whisper","gemma2-","gemma-2-","llama-3.3-","llama","llama-","llama3","llama-3.1-","llama3-","llama-3-","mixtral-8","mistral\n32","mistral","qwen-","qwen","mistral-","mistral.","qwen-2.5-","qwen2.5-","deepseek-","qwen-32b","deepseek\n128","llama-70b","llama-3.2-"]},{"title":"Models: Get Models (js)","content":"URL: https://console.groq.com/docs/models/scripts/get-models\n\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq({ apiKey: process.env.GROQ_API_KEY });\n\nconst getModels = async () => {\n  return await groq.models.list();\n};\n\ngetModels().then((models) => {\n  // console.log(models);\n});\n\n---","keywords":["models","endpoints"]},{"title":"Models: Get Models (py)","content":"URL: https://console.groq.com/docs/models/scripts/get-models.py\n\nimport requests\nimport os\n\napi_key = os.environ.get(\"GROQ_API_KEY\")\nurl = \"https://api.groq.com/openai/v1/models\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.get(url, headers=headers)\n\nprint(response.json())\n\n---","keywords":["models","authentication","endpoints"]},{"title":"Supported Models","content":"URL: https://console.groq.com/docs/models","keywords":["models","endpoints"]},{"title":"Supported Models","content":"GroqCloud currently supports the following models:\n\n### Production Models\n**Note:** Production models are intended for use in your production environments. They meet or exceed our high standards for speed and quality.\n\n### Preview Models\n**Note:** Preview models are intended for evaluation purposes only and should not be used in production environments as they may be discontinued at short notice.\n\nDeprecated models are models that are no longer supported or will no longer be supported in the future. A suggested alternative model for you to use is listed for each deprecated model. [See our deprecated models here](/docs/deprecations)\n\nHosted models are directly accessible through the GroqCloud Models API endpoint using the model IDs mentioned above. You can use the `https://api.groq.com/openai/v1/models` endpoint to return a JSON list of all active models:\n\n---","keywords":["models","endpoints","inference"]},{"title":"Vision: Vision (py)","content":"URL: https://console.groq.com/docs/vision/scripts/vision.py\n\nfrom groq import Groq\n\nclient = Groq()\ncompletion = client.chat.completions.create(\n    model=\"llama-3.2-11b-vision-preview\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"What's in this image?\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/f2/LPU-v1-die.jpg\"\n                    }\n                }\n            ]\n        }\n    ],\n    temperature=1,\n    max_completion_tokens=1024,\n    top_p=1,\n    stream=False,\n    stop=None,\n)\n\nprint(completion.choices[0].message)\n\n---","keywords":["models","authentication","endpoints","parameters","llama-3.2-"]},{"title":"Vision: Vision (json)","content":"URL: https://console.groq.com/docs/vision/scripts/vision.json\n\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"What's in this image?\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/f2/LPU-v1-die.jpg\"\n          }\n        }\n      ]\n    }\n  ],\n  \"model\": \"llama-3.2-11b-vision-preview\",\n  \"temperature\": 1,\n  \"max_completion_tokens\": 1024,\n  \"top_p\": 1,\n  \"stream\": false,\n  \"stop\": null\n}\n\n---","keywords":["models","authentication","endpoints","parameters","llama-3.2-"]},{"title":"Function to encode the image","content":"URL: https://console.groq.com/docs/vision/scripts/local.py\n\nfrom groq import Groq\nimport base64\n\n\n# Function to encode the image\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\n# Path to your image\nimage_path = \"sf.jpg\"\n\n# Getting the base64 string\nbase64_image = encode_image(image_path)\n\nclient = Groq()\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                    },\n                },\n            ],\n        }\n    ],\n    model=\"llama-3.2-11b-vision-preview\",\n)\n\nprint(chat_completion.choices[0].message.content)\n\n---","keywords":["models","endpoints","tools","llama-3.2-"]},{"title":"Vision: Vision (js)","content":"URL: https://console.groq.com/docs/vision/scripts/vision\n\nconst Groq = require('groq-sdk');\n\nconst groq = new Groq();\nasync function main() {\n  const chatCompletion = await groq.chat.completions.create({\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What's in this image?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/f2/LPU-v1-die.jpg\"\n            }\n          }\n        ]\n      }\n    ],\n    \"model\": \"llama-3.2-11b-vision-preview\",\n    \"temperature\": 1,\n    \"max_completion_tokens\": 1024,\n    \"top_p\": 1,\n    \"stream\": false,\n    \"stop\": null\n  });\n\n   console.log(chatCompletion.choices[0].message.content);\n}\n\nmain();\n\n---","keywords":["models","authentication","endpoints","parameters","tools","llama-3.2-"]},{"title":"Vision: Jsonmode (py)","content":"URL: https://console.groq.com/docs/vision/scripts/jsonmode.py\n\nfrom groq import Groq\n\nclient = Groq()\ncompletion = client.chat.completions.create(\n    model=\"llama-3.2-90b-vision-preview\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"List what you observe in this photo in JSON format.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/d/da/SF_From_Marin_Highlands3.jpg\"\n                    }\n                }\n            ]\n        }\n    ],\n    temperature=1,\n    max_completion_tokens=1024,\n    top_p=1,\n    stream=False,\n    response_format={\"type\": \"json_object\"},\n    stop=None,\n)\n\nprint(completion.choices[0].message)\n\n---","keywords":["models","authentication","endpoints","parameters","llama-3.2-"]},{"title":"Vision: Multiturn (py)","content":"URL: https://console.groq.com/docs/vision/scripts/multiturn.py\n\nfrom groq import Groq\n\nclient = Groq()\ncompletion = client.chat.completions.create(\n    model=\"llama-3.2-11b-vision-preview\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"What is in this image?\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/d/da/SF_From_Marin_Highlands3.jpg\"\n                    }\n                }\n            ]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me more about the area.\"\n        }\n    ],\n    temperature=1,\n    max_completion_tokens=1024,\n    top_p=1,\n    stream=False,\n    stop=None,\n)\n\nprint(completion.choices[0].message)\n\n---","keywords":["models","authentication","endpoints","parameters","llama-3.2-"]},{"title":"Vision","content":"URL: https://console.groq.com/docs/vision","keywords":["endpoints"]},{"title":"Vision","content":"Groq API offers fast inference and low latency for multimodal models with vision capabilities for understanding and interpreting visual data from images. By analyzing the content of an image, multimodal models can generate \nhuman-readable text for providing insights about given visual data. \n\n### Supported Models\n\nGroq API supports powerful multimodal models that can be easily integrated into your applications to provide fast and accurate image processing for tasks such as visual question answering, caption generation, \nand Optical Character Recognition (OCR): \n**Note**: Images are billed at 6,400 tokens per image.\n\n### Llama 3.2 90B Vision (Preview)\n* Model ID: `llama-3.2-90b-vision-preview`\n* Description: A powerful multimodal model capable of processing both text and image inputs that supports multilingual, multi-turn conversations, tool use, and JSON mode.\n* Context Window: 8,192 tokens\n* Preview Model: Currently in preview and should be used for experimentation.\n* Image Size Limit: Maximum allowed size for a request containing an image URL as input is 20MB. Requests larger than this limit will return a 400 error.\n* Request Size Limit (Base64 Encoded Images): Maximum allowed size for a request containing a base64 encoded image is 4MB. Requests larger than this limit will return a 413 error.\n* Single Image per Request: Only one image can be processed per request in the preview release. Requests with multiple images will return a 400 error.\n* System Prompt: Does not support system prompts and images in the same request.\n\n### Llama 3.2 11B Vision (Preview)\n* Model ID: `llama-3.2-11b-vision-preview`\n* Description: A powerful multimodal model capable of processing both text and image inputs that supports multilingual, multi-turn conversations, tool use, and JSON mode.\n* Context Window: 8,192 tokens\n* Preview Model: Currently in preview and should be used for experimentation.\n* Image Size Limit: Maximum allowed size for a request containing an image URL as input is 20MB. Requests larger than this limit will return a 400 error.\n* Request Size Limit (Base64 Encoded Images): Maximum allowed size for a request containing a base64 encoded image is 4MB. Requests larger than this limit will return a 413 error.\n* Single Image per Request: Only one image can be processed per request in the preview release. Requests with multiple images will return a 400 error.\n* System Prompt: Does not support system prompts and images in the same request.\n\n### How to Use Vision\n\nUse Groq API vision features via:\n- **GroqCloud Console Playground**: Select `llama-3.2-90b-vision-preview` or `llama-3.2-11b-vision-preview` as the model and upload your image.\n- **Groq API Request**: Call the `chat.completions` API endpoint and set `model_id` to `llama-3.2-90b-vision-preview` or `llama-3.2-11b-vision-preview`.\n\n#### How to Pass Images from URLs as Input\nPass your image to the model via a URL.\n\n#### How to Pass Locally Saved Images as Input\nTo pass locally saved images, encode your image to a base64 format string before passing it as the `image_url` in your API request.\n\n#### Tool Use with Images\nThe `llama-3.2-90b-vision-preview` and `llama-3.2-11b-vision-preview` models support tool use.\n\n#### JSON Mode with Images\nThe `llama-3.2-90b-vision-preview` and `llama-3.2-11b-vision-preview` models support JSON mode.\n\n#### Multi-turn Conversations with Images\nThe `llama-3.2-90b-vision-preview` and `llama-3.2-11b-vision-preview` models support multi-turn conversations.\n\n### Venture Deeper into Vision\n\n#### Use Cases to Explore\nVision models can be used in a wide range of applications. Here are some ideas:\n- **Accessibility Applications**: Develop an application that generates audio descriptions for images by using a vision model to generate text descriptions for images, which can then be converted to audio.\n- **E-commerce Product Description Generation**: Create an application that generates product descriptions for e-commerce websites.\n- **Multilingual Image Analysis**: Create applications that can describe images in multiple languages.\n- **Multi-turn Visual Conversations**: Develop interactive applications that allow users to have extended conversations about images.\n\n#### Next Steps\nCheck out our [Groq API Cookbook](https://github.com/groq/groq-api-cookbook) repository on GitHub for practical examples and tutorials:\n- [Image Moderation](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/image_moderation.ipynb)\n- [Multimodal Image Processing (Tool Use, JSON Mode)](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/multimodal-image-processing)\n\n---","keywords":["models","authentication","endpoints","rate limits","error","tools","speech","inference","llama 3.2","llama-3.2-"]},{"title":"Arize + Groq: Open-Source AI Observability","content":"URL: https://console.groq.com/docs/arize","keywords":["endpoints"]},{"title":"Arize + Groq: Open-Source AI Observability","content":"<br />\n\n[Arize Phoenix](https://docs.arize.com/phoenix) developed by [Arize AI](https://arize.com/) is an open-source AI observability library that enables comprehensive tracing and monitoring for your AI \napplications. By integrating Arize's observability tools with your Groq-powered applications, you can gain deep insights into your LLM worklflow's performance and behavior with features including:\n\n- **Automatic Tracing:** Capture detailed metrics about LLM calls, including latency, token usage, and exceptions\n- **Real-time Monitoring:** Track application performance and identify bottlenecks in production\n- **Evaluation Framework:** Utilize pre-built templates to assess LLM performance\n- **Prompt Management:** Easily iterate on prompts and test changes against your data\n\n### Python Quick Start (3 minutes to hello world)\n#### 1. Install the required packages:\n```bash\npip install arize-phoenix-otel openinference-instrumentation-groq groq\n```\n\n#### 2. Sign up for an [Arize Phoenix account](https://app.phoenix.arize.com).\n\n#### 2. Configure your Groq and Arize Phoenix API keys:\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\nexport PHOENIX_API_KEY=\"your-phoenix-api-key\"\n```\n\n#### 3. (Optional) [Create a new project](https://app.phoenix.arize.com/projects) or use the \"default\" project as your `project_name` below.\n\n#### 4. Create your first traced Groq application:\nIn Arize Phoenix, **traces** capture the complete journey of an LLM request through your application, while **spans** represent individual operations within that trace. The instrumentation \nautomatically captures important metrics and metadata.\n```python\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\ntracer_provider = register(\n    project_name=\"default\",\n    endpoint=\"https://app.phoenix.arize.com/v1/traces\",\n)\n\nGroqInstrumentor().instrument(tracer_provider=tracer_provider)\n\nclient = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n\nchat_completion = client.chat.completions.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Explain the importance of AI observability\"\n    }],\n    model=\"llama-3.3-70b-versatile\",\n)\n\nprint(chat_completion.choices[0].message.content)\n```\n\nRunning the above code will create an automatically instrumented Groq application! The traces will be available in your Phoenix dashboard within the `default` project, showing \ndetailed information about:\n- **Application Latency:** Identify slow components and bottlenecks\n- **Token Usage:** Track token consumption across different operations\n- **Runtime Exceptions:** Capture and analyze errors and rate limits\n- **LLM Parameters:** Monitor temperature, system prompts, and other settings\n- **Response Analysis:** Examine LLM outputs and their characteristics\n\n**Challenge**: Update an existing Groq-powered application you've built to add Arize Phoenix tracing!\n\nFor more detailed documentation and resources on building observable LLM applications with Groq and Arize, see:\n- [Official Documentation: Groq Integration Guide](https://docs.arize.com/phoenix/tracing/integrations-tracing/groq)\n- [Blog: Tracing with Groq](https://arize.com/blog/tracing-groq/)\n- [Webinar: Tracing and Evaluating LLM Apps with Groq and Arize Phoenix](https://youtu.be/KjtrILr6JZI?si=iX8Udo-EYsK2JOvF)\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","error","tools","inference","llama-3.3-"]},{"title":"Text to Speech","content":"URL: https://console.groq.com/docs/text-to-speech","keywords":["endpoints","speech"]},{"title":"Text to Speech","content":"Learn how to instantly generate lifelike audio from text.","keywords":["endpoints","rate limits","speech"]},{"title":"Overview","content":"The Groq API speech endpoint provides fast text-to-speech (TTS), enabling you to convert text to spoken audio in seconds with our available TTS models.\nWith support for 30 voices, 26 in English and 4 in Arabic, you can instantly create life-like audio content for customer support agents, characters for game development, and more.","keywords":["models","endpoints","tools","speech","inference"]},{"title":"API Endpoint","content":"| Endpoint | Usage                          | API Endpoint                                                |\n|----------|--------------------------------|-------------------------------------------------------------|\n| Speech   | Convert text to audio          | `https://api.groq.com/openai/v1/audio/speech`               |","keywords":["endpoints","speech"]},{"title":"Supported Models","content":"| Model ID          | Model Card   | Supported Language(s)  | Description                                                     |\n|-------------------|--------------|------------------------|-----------------------------------------------------------------|\n| `play-tts`        | [Play TTS](/docs/model/playai-tts)     | English                | High-quality TTS model for English speech generation. |\n| `play-tts-arabic` | [Play TTS](/docs/model/playai-tts)     | Arabic                 | High-quality TTS model for Arabic speech generation.            |","keywords":["models","endpoints","speech"]},{"title":"Working with Speech","content":"### Quick Start\nThe speech endpoint takes four key inputs: model (`play-tts` or `play-tts-arabic`), input (the text to generate audio from), voice \n(the desired voice for output), and the response format (currently defaults to `\"wav\"`). \nHere's a request example:\n\n```bash\ncurl https://api.groq.com/openai/v1/audio/speech \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"playai-tts\",\n    \"input\": \"I love building new applications with Groq API!\",\n    \"voice\": \"Bryan-PlayAI\",\n    \"response_format\": \"wav\"\n  }' \\\n```\n\n### Parameters\n\n| Parameter | Type | Required | Value | Description |\n|-----------|------|----------|-------------|---------------|\n| `model` | string | Yes | `playai-tts`<br />`playai-tts-arabic` | Model ID to use for TTS. |\n| `input` | string | Yes | -  | User input text to be converted to speech. Maximum length is 10,000 characters. |\n| `voice` | string | Yes | See available [English](/docs/text-to-speech/#available-english-voices) and [Arabic](/docs/text-to-speech/#available-arabic-voices) voices.  | The voice to use for audio generation. There are currently 26 English options for `playai-tts` and 4 Arabic options for `playai-tts-arabic`. |\n| `response_format` | string | Optional | `\"wav\"` | Format of the response audio file. Defaults to currently supported `\"wav\"`. |\n\n\n\n### Available English Voices\n\nThe `play-tts` model supports 26 English voices (`Aaliyah-PlayAI`, `Abigail-PlayAI`, `Angelo-PlayAI`, `Arthur-PlayAI`, `Aurora-PlayAI`, \n`Autumn-PlayAI`, `Ayla-Meditation-PlayAI`, `Ayla-Advertising-PlayAI`, `Bryan-PlayAI`, `Chuck-PlayAI`, `Darrell-PlayAI`, `Dexter-PlayAI`, \n`Donovan-PlayAI`, `Eileen-PlayAI`, `Eleanor-PlayAI`, `Erasmo-PlayAI`, `Hudson-PlayAI`, `Inara-PlayAI`, `Luna-PlayAI`, `Nia-PlayAI`, \n`Phoebe-PlayAI`, `Ranger-PlayAI`, `Sophia-PlayAI`, `Waylon-PlayAI`, `William-Training-PlayAI`).\nExperiment to find the voice you need for your application:\n\n### Available Arabic Voices\n\nThe `play-tts-arabic` model supports 4 Arabic voices (`Nasser-PlayAI`, `Khalid-PlayAI`, `Amira-PlayAI`, `Ahmad-PlayAI`).\nExperiment to find the voice you need for your application:\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","speech","inference"]},{"title":"Reasoning: R1 (py)","content":"URL: https://console.groq.com/docs/reasoning/scripts/r1.py\n\nfrom groq import Groq\n\nclient = Groq()\ncompletion = client.chat.completions.create(\n    model=\"deepseek-r1-distill-llama-70b\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How many r's are in the word strawberry?\"\n        }\n    ],\n    temperature=0.6,\n    max_completion_tokens=1024,\n    top_p=0.95,\n    stream=True,\n    reasoning_format=\"raw\"\n)\n\nfor chunk in completion:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n---","keywords":["models","authentication","endpoints","parameters","deepseek-","llama-70b"]},{"title":"Reasoning: R1 (js)","content":"URL: https://console.groq.com/docs/reasoning/scripts/r1\n\nimport Groq from 'groq-sdk';\n\nconst client = new Groq();\nconst completion = await client.chat.completions.create({\n    model: \"deepseek-r1-distill-llama-70b\",\n    messages: [\n        {\n            role: \"user\",\n            content: \"How many r's are in the word strawberry?\"\n        }\n    ],\n    temperature: 0.6,\n    max_completion_tokens: 1024,\n    top_p: 0.95,\n    stream: true,\n    reasoning_format: \"raw\"\n});\n\nfor await (const chunk of completion) {\n    process.stdout.write(chunk.choices[0].delta.content || \"\");\n}\n\n---","keywords":["models","authentication","endpoints","parameters","deepseek-","llama-70b"]},{"title":"Reasoning","content":"URL: https://console.groq.com/docs/reasoning","keywords":["endpoints"]},{"title":"Reasoning","content":"Reasoning models excel at complex problem-solving tasks that require step-by-step analysis, logical deduction, and structured thinking and solution validation. With Groq inference speed, these types of models \ncan deliver instant reasoning capabilities critical for real-time applications. \n\n### Why Speed Matters for Reasoning\nReasoning models are capable of complex decision making with explicit reasoning chains that are part of the token output and used for decision-making, which make low-latency and fast inference essential. \nComplex problems often require multiple chains of reasoning tokens where each step build on previous results. Low latency compounds benefits across reasoning chains and shaves off minutes of reasoning to a response in seconds.","keywords":["models","authentication","error","inference"]},{"title":"Supported Model","content":"| Model ID                        | Model                          |\n|---------------------------------|--------------------------------|\n| qwen-qwq-32b                  | Qwen QwQ 32B\n| deepseek-r1-distill-qwen-32b  | DeepSeek R1 Distill Qwen 32B   |\n| deepseek-r1-distill-llama-70b | DeepSeek R1 Distil Llama 70B   |","keywords":["models","qwen-","qwen ","deepseek-","qwen-32b","deepseek ","qwen 32b","llama-70b","llama 70b"]},{"title":"Reasoning Format","content":"Groq API supports explicit reasoning formats through the `reasoning_format` parameter, giving you fine-grained control over how the model's \nreasoning process is presented. This is particularly valuable for valid JSON outputs, debugging, and understanding the model's decision-making process.\n\n**Note:** The format defaults to `raw` or `parsed` when JSON mode or tool use are enabled as those modes do not support `raw`. If reasoning is \nexplicitly set to `raw` with JSON mode or tool use enabled, we will return a 400 error.\n\n### Options for Reasoning Format\n| `reasoning_format` Options | Description                                                |\n|------------------|------------------------------------------------------------| \n| `parsed` | Separates reasoning into a dedicated field while keeping the response concise. |\n| `raw`    | Includes reasoning within think tags in the content. |\n| `hidden`  | Returns only the final answer. |","keywords":["models","parameters","rate limits","error","tools"]},{"title":"Quick Start with Tool use","content":"```bash\ncurl https://api.groq.com//openai/v1/chat/completions -s \\\n  -H \"authorization: bearer $GROQ_API_KEY\" \\\n  -d '{\n    \"model\": \"deepseek-r1-distill-llama-70b\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather like in Paris today?\"\n        }\n    ],\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"description\": \"Get current temperature for a given location.\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"City and country e.g. Bogot√°, Colombia\"\n                        }\n                    },\n                    \"required\": [\n                        \"location\"\n                    ],\n                    \"additionalProperties\": false\n                },\n                \"strict\": true\n            }\n        }\n    ]}'\n```","keywords":["models","authentication","endpoints","parameters","tools","inference","deepseek-","llama-70b"]},{"title":"Recommended Configuration Parameters","content":"| Parameter | Default | Range | Description |\n|-----------|---------|--------|-------------|\n| `messages` | - | - | Array of message objects. Important: Avoid system prompts - include all instructions in the user message! |\n| `temperature` | 0.6 | 0.0 - 2.0 | Controls randomness in responses. Lower values make responses more deterministic. Recommended range: 0.5-0.7 to prevent repetitions or incoherent outputs |\n| `max_completion_tokens` | 1024 | - | Maximum length of model's response. Default may be too low for complex reasoning - consider increasing for detailed step-by-step solutions |\n| `top_p` | 0.95 | 0.0 - 1.0 | Controls diversity of token selection |\n| `stream` | false | boolean | Enables response streaming. Recommended for interactive reasoning tasks |\n| `stop` | null | string/array | Custom stop sequences |\n| `seed` | null | integer | Set for reproducible results. Important for benchmarking - run multiple tests with different seeds |\n| `json_mode` | - | boolean | Set to enable JSON mode for structured output. |\n| `reasoning_format` | `raw` | `\"parsed\"`, `\"raw\"`, `\"hidden\"` | Controls how model reasoning is presented in the response. Must be set to either `parsed` or `hidden` when using tool calling or JSON mode. |","keywords":["models","authentication","parameters","tools"]},{"title":"Optimizing Performance","content":"### Temperature and Token Management\nThe model performs best with temperature settings between 0.5-0.7, with lower values (closer to 0.5) producing more consistent mathematical proofs and higher values allowing for more creative problem-solving approaches. Monitor and adjust your token usage based on the complexity of your reasoning tasks - while the default max_completion_tokens is 1024, complex proofs may require higher limits.\n\n### Prompt Engineering\nTo ensure accurate, step-by-step reasoning while maintaining high performance:\n- DeepSeek-R1 works best when all instructions are included directly in user messages rather than system prompts. \n- Structure your prompts to request explicit validation steps and intermediate calculations. \n- Avoid few-shot prompting and go for zero-shot prompting only.\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","error","inference","deepseek-"]},{"title":"Quickstart: Performing Chat Completion (js)","content":"URL: https://console.groq.com/docs/quickstart/scripts/performing-chat-completion\n\n```javascript\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq({ apiKey: process.env.GROQ_API_KEY });\n\nexport async function main() {\n  const chatCompletion = await getGroqChatCompletion();\n  // Print the completion returned by the LLM.\n  console.log(chatCompletion.choices[0]?.message?.content || \"\");\n}\n\nexport async function getGroqChatCompletion() {\n  return groq.chat.completions.create({\n    messages: [\n      {\n        role: \"user\",\n        content: \"Explain the importance of fast language models\",\n      },\n    ],\n    model: \"llama-3.3-70b-versatile\",\n  });\n}\n```\n\n---","keywords":["models","endpoints","tools","inference","llama-3.3-"]},{"title":"Quickstart: Performing Chat Completion (json)","content":"URL: https://console.groq.com/docs/quickstart/scripts/performing-chat-completion.json\n\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Explain the importance of fast language models\"\n    }\n  ],\n  \"model\": \"llama-3.3-70b-versatile\"\n}\n\n---","keywords":["models","endpoints","inference","llama-3.3-"]},{"title":"Quickstart: Performing Chat Completion (py)","content":"URL: https://console.groq.com/docs/quickstart/scripts/performing-chat-completion.py\n\nimport os\n\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of fast language models\",\n        }\n    ],\n    model=\"llama-3.3-70b-versatile\",\n)\n\nprint(chat_completion.choices[0].message.content)\n\n---","keywords":["models","endpoints","inference","llama-3.3-"]},{"title":"Quickstart","content":"URL: https://console.groq.com/docs/quickstart","keywords":["endpoints","inference"]},{"title":"Quickstart","content":"Get up and running with the Groq API in a few minutes.\n\n### Create an API Key\n\nPlease visit [here](/keys) to create an API Key.\n\n### Set up your API Key (recommended)\n\nConfigure your API key as an environment variable. This approach streamlines your API usage by eliminating the need to include your API key in each request. Moreover, it enhances security by minimizing the risk of inadvertently including your API key in your codebase.\n\n#### In your terminal of choice:\n\n```shell\nexport GROQ_API_KEY=<your-api-key-here>\n```\n\n### Requesting your first chat completion\n\n#### Execute this curl command in the terminal of your choice:\n\n```shell\nperformingChatCompletionSh\n```\n\n#### Install the Groq JavaScript library:\n\n```shell\ninstallGroqNpm\n```\n\n#### Performing a Chat Completion:\n\n```js\nperformingChatCompletionJs\n```\n\n#### Install the Groq Python library:\n\n```shell\ninstallGroqPip\n```\n\n#### Performing a Chat Completion:\n\n```py\nperformingChatCompletionPy\n```\n\n#### Pass the following as the request body:\n\n```json\nperformingChatCompletionJson\n```\n\nNow that you have successfully received a chat completion, you can try out the other endpoints in the API.\n\n### Next Steps\n\n- Check out the [Playground](/playground) to try out the Groq API in your browser\n- Join our GroqCloud developer community on [Discord](https://discord.gg/groq)\n- [Chat with our Docs](https://docs-chat.groqcloud.com/) at lightning speed using the Groq API!\n- Add a how-to on your project to the [Groq API Cookbook](https://github.com/groq/groq-api-cookbook)\n\n---","keywords":["authentication","endpoints","inference"]},{"title":"üé® Gradio + Groq: Easily Build Web Interfaces","content":"URL: https://console.groq.com/docs/gradio","keywords":["endpoints"]},{"title":"üé® Gradio + Groq: Easily Build Web Interfaces","content":"[Gradio](https://www.gradio.app/) is a powerful library for creating web interfaces for your applications that enables you to quickly build \ninteractive demos for your fast Groq apps with features such as:\n\n- **Interface Builder:** Create polished UIs with just a few lines of code, supporting text, images, audio, and more\n- **Interactive Demos:** Build demos that showcase your LLM applications with multiple input/output components\n- **Shareable Apps:** Deploy and share your Groq-powered applications with a single click\n\n\n### Quick Start (2 minutes to hello world)\n\n#### 1. Install the packages:\n```bash\npip install groq-gradio\n```\n\n#### 2. Set up your API key:\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\n```\n\n#### 3. Create your first Gradio chat interface:\nThe following code creates a simple chat interface with `llama-3.3-70b-versatile` that includes a clean UI.\n```python\nimport gradio as gr\nimport groq_gradio\nimport os\n\n# Initialize Groq client\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\")\n)\n\ngr.load(\n    name='llama-3.3-70b-versatile', # The specific model powered by Groq to use\n    src=groq_gradio.registry, # Tells Gradio to use our custom interface registry as the source\n    title='Groq-Gradio Integration', # The title shown at the top of our UI\n    description=\"Chat with the Llama 3.3 70B model powered by Groq.\", # Subtitle\n    examples=[\"Explain quantum gravity to a 5-year old.\", \"How many R are there in the word Strawberry?\"] # Pre-written prompts users can click to try\n).launch() # Creates and starts the web server!\n\n**Challenge**: Enhance the above example to create a multi-modal chatbot that leverages text, audio, and vision models powered by Groq and\ndisplayed on a customized UI built with Gradio blocks!\n\nFor more information on building robust applications with Gradio and Groq, see:\n- [Official Documentation: Gradio](https://www.gradio.app/docs)\n- [Tutorial: Automatic Voice Detection with Groq](https://www.gradio.app/guides/automatic-voice-detection)\n- [Groq API Cookbook: Groq and Gradio for Realtime Voice-Powered AI Applications](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/groq-gradio/groq-gradio-tutorial.ipynb)\n- [Webinar: Building a Multimodal Voice Enabled Calorie Tracking App with Groq and Gradio](https://youtu.be/azXaioGdm2Q?si=sXPJW1IerbghsCKU)\n\n---","keywords":["models","authentication","endpoints","speech","inference","llama-3.3-","llama 3.3"]},{"title":"pip install requests first!","content":"URL: https://console.groq.com/docs/batch/scripts/upload_file.py\n\nimport requests # pip install requests first!\n\ndef upload_file_to_groq(api_key, file_path):\n    url = \"https://api.groq.com/openai/v1/files\"\n    \n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n    \n    # Prepare the file and form data\n    files = {\n        \"file\": (\"batch_file.jsonl\", open(file_path, \"rb\"))\n    }\n    \n    data = {\n        \"purpose\": \"batch\"\n    }\n    \n    # Make the POST request\n    response = requests.post(url, headers=headers, files=files, data=data)\n    \n    return response.json()\n\n# Usage example\napi_key = \"YOUR_GROQ_API_KEY\"  # Replace with your actual API key\nfile_path = \"batch_file.jsonl\"  # Path to your JSONL file\n\ntry:\n    result = upload_file_to_groq(api_key, file_path)\n    print(result)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n---","keywords":["authentication","endpoints","error"]},{"title":"Batch: Status (js)","content":"URL: https://console.groq.com/docs/batch/scripts/status\n\n```javascript\nrequire('dotenv').config();\nconst fetch = require('node-fetch'); // npm install node-fetch first!\n\nasync function getBatchStatus(apiKey, batchId) {\n    const url = `https://api.groq.com/openai/v1/batches/${batchId}`;\n    \n    const headers = {\n        'Authorization': `Bearer ${apiKey}`,\n        'Content-Type': 'application/json'\n    };\n\n    try {\n        const response = await fetch(url, {\n            method: 'GET',\n            headers: headers\n        });\n\n        const result = await response.json();\n        return result;\n    } catch (error) {\n        throw new Error(`Failed to get batch status: ${error.message}`);\n    }\n}\n\n// Usage example\nconst apiKey = process.env.GROQ_API_KEY;\nif (!apiKey) {\n    throw new Error('GROQ_API_KEY environment variable is not set');\n}\n\nconst batchId = 'batch_01jh6xa7reempvjyh6n3yst2zw';\n\ngetBatchStatus(apiKey, batchId)\n    .then(result => console.log(result))\n    .catch(error => console.error('Error:', error));\n```\n\n---","keywords":["authentication","endpoints","error","tools"]},{"title":"pip install requests first!","content":"URL: https://console.groq.com/docs/batch/scripts/retrieve.py\n\nimport requests # pip install requests first! \n\ndef download_file_content(api_key, output_file_id, output_file):\n    url = f\"https://api.groq.com/openai/v1/files/{output_file_id}/content\"\n    \n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n    \n    response = requests.get(url, headers=headers)\n    \n    # Write the content to a file\n    with open(output_file, 'wb') as f:\n        f.write(response.content)\n    \n    return f\"File downloaded successfully to {output_file}\"\n\n# Usage example\napi_key = \"YOUR_GROQ_API_KEY\"\noutput_file_id = \"file_01jh6xa97be52b7pg88czwrrwb\" # replace with your own completed batch job's `output_file_id`\noutput_file = \"batch_output.jsonl\" # replace with your own file of choice to download batch job contents to\n\ntry:\n    result = download_file_content(api_key, output_file_id, output_file)\n    print(result)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n---","keywords":["authentication","endpoints","error"]},{"title":"Batch: Create Batch Job (js)","content":"URL: https://console.groq.com/docs/batch/scripts/create_batch_job\n\n```javascript\nrequire('dotenv').config();\nconst fetch = require('node-fetch'); // npm install node-fetch first!\n\nasync function createBatch(apiKey, inputFileId) {\n    const url = 'https://api.groq.com/openai/v1/batches';\n    \n    const headers = {\n        'Authorization': `Bearer ${apiKey}`,\n        'Content-Type': 'application/json'\n    };\n    \n    const data = {\n        input_file_id: inputFileId,\n        endpoint: '/v1/chat/completions',\n        completion_window: '24h'\n    };\n\n    try {\n        const response = await fetch(url, {\n            method: 'POST',\n            headers: headers,\n            body: JSON.stringify(data)\n        });\n\n        const result = await response.json();\n        return result;\n    } catch (error) {\n        throw new Error(`Batch creation failed: ${error.message}`);\n    }\n}\n\n// Usage example\nconst apiKey = process.env.GROQ_API_KEY;\nif (!apiKey) {\n    throw new Error('GROQ_API_KEY environment variable is not set');\n}\n\nconst fileId = 'file_01jh6x76wtemjr74t1fh0faj5t'; // replace with your `id` from file upload API response object\n\ncreateBatch(apiKey, fileId)\n    .then(result => console.log(result))\n    .catch(error => console.error('Error:', error));\n```\n\n---","keywords":["authentication","endpoints","error","tools"]},{"title":"pip install requests first!","content":"URL: https://console.groq.com/docs/batch/scripts/create_batch_job.py\n\nimport requests # pip install requests first! \n\ndef create_batch(api_key, input_file_id):\n    url = \"https://api.groq.com/openai/v1/batches\"\n    \n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\"\n    }\n    \n    data = {\n        \"input_file_id\": input_file_id,\n        \"endpoint\": \"/v1/chat/completions\",\n        \"completion_window\": \"24h\"\n    }\n    \n    response = requests.post(url, headers=headers, json=data)\n    return response.json()\n\n# Usage example\napi_key = \"YOUR_GROQ_API_KEY\"\nfile_id = \"file_01jh6x76wtemjr74t1fh0faj5t\" # replace with your `id` from file upload API response object\n\ntry:\n    result = create_batch(api_key, file_id)\n    print(result)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n---","keywords":["authentication","endpoints","error"]},{"title":"Batch: Retrieve (js)","content":"URL: https://console.groq.com/docs/batch/scripts/retrieve\n\n```javascript\nrequire('dotenv').config();\nconst fs = require('node:fs');\nconst fetch = require('node-fetch'); // npm install node-fetch first!\n\nasync function downloadFileContent(apiKey, outputFileId, outputFile) {\n    const url = `https://api.groq.com/openai/v1/files/${outputFileId}/content`;\n    \n    const headers = {\n        'Authorization': `Bearer ${apiKey}`\n    };\n\n    try {\n        const response = await fetch(url, {\n            method: 'GET',\n            headers: headers\n        });\n\n        if (!response.ok) {\n            throw new Error(`HTTP error! status: ${response.status}`);\n        }\n\n        // Get the response as a buffer/text\n        const content = await response.text();\n        \n        // Write to file\n        fs.writeFileSync(outputFile, content);\n        \n        return `File downloaded successfully to ${outputFile}`;\n    } catch (error) {\n        throw new Error(`Download failed: ${error.message}`);\n    }\n}\n\n// Usage example\nconst apiKey = process.env.GROQ_API_KEY;\nif (!apiKey) {\n    throw new Error('GROQ_API_KEY environment variable is not set');\n}\n\nconst outputFileId = 'file_01jh6xa97be52b7pg88czwrrwb'; // replace with your own completed batch job's `output_file_id`\nconst outputFile = 'batch_output.jsonl'; // replace with your own file of choice to download batch job contents to\n\ndownloadFileContent(apiKey, outputFileId, outputFile)\n    .then(result => console.log(result))\n    .catch(error => console.error('Error:', error));\n```\n\n---","keywords":["authentication","endpoints","error","tools"]},{"title":"pip install requests first!","content":"URL: https://console.groq.com/docs/batch/scripts/status.py\n\nimport requests # pip install requests first!\n\ndef get_batch_status(api_key, batch_id):\n    url = f\"https://api.groq.com/openai/v1/batches/{batch_id}\"\n    \n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\"\n    }\n    \n    response = requests.get(url, headers=headers)\n    return response.json()\n\n# Usage example\napi_key = \"YOUR_GROQ_API_KEY\"\nbatch_id = \"batch_01jh6xa7reempvjyh6n3yst2zw\"\n\ntry:\n    result = get_batch_status(api_key, batch_id)\n    print(result)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n---","keywords":["authentication","endpoints","error"]},{"title":"Batch: Upload File (js)","content":"URL: https://console.groq.com/docs/batch/scripts/upload_file\n\n```javascript\nrequire('dotenv').config();\nconst fs = require('node:fs');\nconst FormData = require('form-data'); // npm install form-data first!\nconst fetch = require('node-fetch'); // npm install node-fetch first!\n\nasync function uploadFileToGroq(apiKey, filePath) {\n   const url = 'https://api.groq.com/openai/v1/files';\n   \n   const formData = new FormData();\n   formData.append('purpose', 'batch');\n   formData.append('file', fs.createReadStream(filePath));\n\n   try {\n       const response = await fetch(url, {\n           method: 'POST',\n           headers: {\n               'Authorization': `Bearer ${apiKey}`,\n           },\n           body: formData\n       });\n\n       const result = await response.json();\n       return result;\n   } catch (error) {\n       throw new Error(`Upload failed: ${error.message}`);\n   }\n}\n\nconst apiKey = process.env.GROQ_API_KEY;\nif (!apiKey) {\n   throw new Error('GROQ_API_KEY environment variable is not set');\n}\n\nconst filePath = 'path-to-your-jsonl-batch-file-here';\n\nuploadFileToGroq(apiKey, filePath)\n   .then(result => console.log('Upload result:', result))\n   .catch(error => console.error('Error:', error));\n```\n\n---","keywords":["authentication","endpoints","error","tools"]},{"title":"Groq Batch API","content":"URL: https://console.groq.com/docs/batch","keywords":["endpoints"]},{"title":"Groq Batch API","content":"Process large-scale workloads asynchronously with our Batch API. \n\n### What is Batch Processing?\nBatch processing lets you run thousands of API requests at scale by submitting your workload as an asynchronous batch of requests to Groq with 25% lower cost (50% off from now until end of April 2025), no impact to your standard rate limits, and 24-hour\nprocessing window.\n\n### Overview\nWhile some of your use cases may require synchronous API requests, asynchronous batch processing is perfect for use cases that don't need immediate reponses or for processing a large number of queries that standard\nrate limits cannot handle, such as processing large datasets, generating content in bulk, and running evaluations. \nCompared to using our synchronous API endpoints, our Batch API has:\n- **Higher rate limits:** Process thousands of requests per batch with no impact on your standard API rate limits\n- **Cost efficiency:** 25% cost discount compared to synchronous APIs (50% off now until end of April 2025)","keywords":["endpoints","rate limits","pricing"]},{"title":"Model Availability and Pricing","content":"The Batch API can currently be used to execute queries for chat completion (both text and vision) and audio transcription inputs with the following models:\nThe following models are available:\n* llama-3.2-90b-vision-preview\n* mistral-saba-24b\n* deepseek-r1-distill-llama-70b\n* llama-3.1-8b-instant\n* llama-3.3-70b-versatile\n* distil-whisper-large-v3-en\n* whisper-large-v3\n* whisper-large-v3-turbo\nPricing is at a 25% cost discount compared to [synchronous API pricing (50% off now until end of April 2025).](https://groq.com/pricing)","keywords":["models","endpoints","pricing","speech","llama-3.2-","mistral-","deepseek-","llama-70b","llama-3.1-","llama-3.3-","whisper-"]},{"title":"Getting Started","content":"Our Batch API endpoints allow you to collect a group of requests into a single file, kick off a batch processing job to execute the requests within your file, query for the status of your batch, and eventually \nretrieve the results when your batch is complete.\nMultiple batch jobs can be submitted at once.\nEach batch has a 24-hour processing window, during which we'll process as many requests as our capacity allows while maintaining service quality for all users. We allow for setting \na batch window from 24 hours to 7 days and recommend setting a longer batch window allow us more time to complete your batch jobs instead of \nexpiiring them.\n\n### 1. Prepare Your Batch File\nA batch is composed of a list of API requests and every batch job starts with a JSON Lines (JSONL) file that contains the requests\nyou want processed. Each line in this file represents a single API call. \nThe Groq Batch API currently supports:\n- Chat completion requests through `/v1/chat/completions`\n- Audio transcription requests through `/v1/audio/transcriptions`\n\nThe structure for each line must include:\n- `custom_id`: Your unique identifier for tracking the batch request\n- `method`: The HTTP method (currently `POST` only)\n- `url`: The API endpoint to call (`/v1/chat/completions` or `/v1/audio/transcriptions`)\n- `body`: The parameters of your request matching our synchronous API format. See our API Reference [here.](https://console.groq.com/docs/api-reference#chat-create)\n\nThe following is an example of a JSONL batch file with different types of requests:\n```json\n{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is 2+2?\"}]}}\n{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is 2+3?\"}]}}\n{\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"count up to 1000000. starting with 1, 2, 3. print all the numbers, do not stop until you get to 1000000.\"}]}}\n```\n```json\n{\"custom_id\":\"job-cb6d01f6-1\",\"method\":\"POST\",\"url\":\"/v1/audio/transcriptions\",\"body\":{\"model\":\"whisper-large-v3\",\"language\":\"en\",\"url\":\"https://github.com/voxserv/audio_quality_testing_samples/raw/refs/heads/master/testaudio/8000/test01_20s.wav\",\"response_format\":\"verbose_json\",\"timestamp_granularities\":[\"segment\"]}}\n{\"custom_id\":\"job-cb6d01f6-2\",\"method\":\"POST\",\"url\":\"/v1/audio/transcriptions\",\"body\":{\"model\":\"whisper-large-v3\",\"language\":\"en\",\"url\":\"https://github.com/voxserv/audio_quality_testing_samples/raw/refs/heads/master/testaudio/8000/test01_20s.wav\",\"response_format\":\"verbose_json\",\"timestamp_granularities\":[\"segment\"]}}\n{\"custom_id\":\"job-cb6d01f6-3\",\"method\":\"POST\",\"url\":\"/v1/audio/transcriptions\",\"body\":{\"model\":\"distil-whisper-large-v3-en\",\"language\":\"en\",\"url\":\"https://github.com/voxserv/audio_quality_testing_samples/raw/refs/heads/master/testaudio/8000/test01_20s.wav\",\"response_format\":\"verbose_json\",\"timestamp_granularities\":[\"segment\"]}}\n```\n```json\n{\"custom_id\": \"chat-request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is quantum computing?\"}]}}\n{\"custom_id\": \"audio-request-1\", \"method\": \"POST\", \"url\": \"/v1/audio/transcriptions\", \"body\": {\"model\": \"whisper-large-v3\", \"language\": \"en\", \"url\": \"https://github.com/voxserv/audio_quality_testing_samples/raw/refs/heads/master/testaudio/8000/test01_20s.wav\", \"response_format\": \"verbose_json\", \"timestamp_granularities\": [\"segment\"]}}\n{\"custom_id\": \"chat-request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.3-70b-versatile\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}]}}\n```\n#### Converting Sync Calls to Batch Format \nIf you're familiar with making synchronous API calls, converting them to batch format is straightforward. Here's how a regular API call transforms\ninto a batch request:\n```json\n# Your typical synchronous API call:\nresponse = client.chat.completions.create(\n    model=\"llama-3.1-8b-instant\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is quantum computing?\"}\n    ]\n)\n\n# The same call in batch format (must be on a single line as JSONL):\n{\"custom_id\": \"quantum-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"user\", \"content\": \"What is quantum computing?\"}]}}\n```\n```json\n# Your typical synchronous API call:\nresponse = client.audio.transcriptions.create(\n    model=\"whisper-large-v3\",\n    language=\"en\",\n    url=\"https://example.com/audio-file.wav\",\n    response_format=\"verbose_json\",\n    timestamp_granularities=[\"segment\"]\n)\n\n# The same call in batch format (must be on a single line as JSONL):\n{\"custom_id\": \"audio-1\", \"method\": \"POST\", \"url\": \"/v1/audio/transcriptions\", \"body\": {\"model\": \"whisper-large-v3\", \"language\": \"en\", \"url\": \"https://example.com/audio-file.wav\", \"response_format\": \"verbose_json\", \"timestamp_granularities\": [\"segment\"]}}\n```\n\n### 2. Upload Your Batch File\nUpload your `.jsonl` batch file using the Files API endpoint for when kicking off your batch job:\n**Note:** The Files API currently only supports `.jsonl` files 50,000 lines or less and up to maximum of 200MB in size. There is no limit for the \nnumber of batch jobs you can submit. We recommend submitting multiple shorter batch files for a better chance of completion.\n\nYou will receive a JSON response that contains the ID (`id`) for your file object that you will then use to create your batch job:\n```json\n{\n    \"id\":\"file_01jh6x76wtemjr74t1fh0faj5t\",\n    \"object\":\"file\",\n    \"bytes\":966,\n    \"created_at\":1736472501,\n    \"filename\":\"input_file.jsonl\",\n    \"purpose\":\"batch\"\n}\n```\n\n### 3. Create Your Batch Job \nOnce you've uploaded your `.jsonl` file, you can use the file object ID (in this case, `file_01jh6x76wtemjr74t1fh0faj5t` as shown in Step 2) to create a batch: \n**Note:** The completion window for batch jobs can be set from to 24 hours (`24h`) to 7 days (`7d`). We recommend setting a longer batch window \nto have a better chance for completed batch jobs rather than expirations for when we are under heavy load.\n\nThis request will return a Batch object with metadata about your batch, including the batch `id` that you can use to check the status of your batch:\n```json\n{\n    \"id\":\"batch_01jh6xa7reempvjyh6n3yst2zw\",\n    \"object\":\"batch\",\n    \"endpoint\":\"/v1/chat/completions\",\n    \"errors\":null,\n    \"input_file_id\":\"file_01jh6x76wtemjr74t1fh0faj5t\",\n    \"completion_window\":\"24h\",\n    \"status\":\"validating\",\n    \"output_file_id\":null,\n    \"error_file_id\":null,\n    \"finalizing_at\":null,\n    \"failed_at\":null,\n    \"expired_at\":null,\n    \"cancelled_at\":null,\n    \"request_counts\":{\n        \"total\":0,\n        \"completed\":0,\n        \"failed\":0\n    },\n    \"metadata\":null,\n    \"created_at\":1736472600,\n    \"expires_at\":1736559000,\n    \"cancelling_at\":null,\n    \"completed_at\":null,\n    \"in_progress_at\":null\n}\n```\n\n### 4. Check Batch Status\nYou can check the status of a batch any time your heart desires with the batch `id` (in this case, `batch_01jh6xa7reempvjyh6n3yst2zw` from the above Batch response object), which will also return a Batch object:\n\nThe status of a given batch job can return any of the following status codes:\n\n| Status        | Description                                                                |\n|---------------|----------------------------------------------------------------------------|\n| `validating`  | batch file is being validated before the batch processing begins           |\n| `failed`      | batch file has failed the validation process                               |\n| `in_progress` | batch file was successfully validated and the batch is currently being run |\n| `finalizing`  | batch has completed and the results are being prepared                     |\n| `completed`   | batch has been completed and the results are ready                         |\n| `expired`     | batch was not able to be completed within the 24-hour time window          |\n| `cancelling`  | batch is being cancelled (may take up to 10 minutes)                       |\n| `cancelled`   | batch was cancelled                                                        |\n\nWhen your batch job is complete, the Batch object will return an `output_file_id` and/or an `error_file_id` that you can then use to retrieve\nyour results (as shown below in Step 5). Here's an example:\n```json\n{\n    \"id\":\"batch_01jh6xa7reempvjyh6n3yst2zw\",\n    \"object\":\"batch\",\n    \"endpoint\":\"/v1/chat/completions\",\n    \"errors\":[\n        {\n            \"code\":\"invalid_method\",\n            \"message\":\"Invalid value: 'GET'. Supported values are: 'POST'\",\"param\":\"method\",\n            \"line\":4\n        }\n    ],\n    \"input_file_id\":\"file_01jh6x76wtemjr74t1fh0faj5t\",\n    \"completion_window\":\"24h\",\n    \"status\":\"completed\",\n    \"output_file_id\":\"file_01jh6xa97be52b7pg88czwrrwb\",\n    \"error_file_id\":\"file_01jh6xa9cte52a5xjnmnt5y0je\",\n    \"finalizing_at\":null,\n    \"failed_at\":null,\n    \"expired_at\":null,\n    \"cancelled_at\":null,\n    \"request_counts\":\n    {\n        \"total\":3,\n        \"completed\":2,\n        \"failed\":1\n    },\n    \"metadata\":null,\n    \"created_at\":1736472600,\n    \"expires_at\":1736559000,\n    \"cancelling_at\":null,\n    \"completed_at\":1736472607,\n    \"in_progress_at\":1736472601\n}\n```\n\n### 5. Retrieve Batch Results \nNow for the fun. Once the batch is complete, you can retrieve the results using the `output_file_id` from your Batch object (in this case, `file_01jh6xa97be52b7pg88czwrrwb` from the above Batch response object) and write it to\na file on your machine (`batch_output.jsonl` in this case) to view them:\n\nThe output `.json\n\n---","keywords":["models","endpoints","parameters","rate limits","error","speech","llama-3.1-","whisper-","llama-3.3-"]},{"title":"Script: Openai Compat (py)","content":"URL: https://console.groq.com/docs/scripts/openai-compat.py\n\nimport os\nimport openai\n\nclient = openai.OpenAI(\n    base_url=\"https://api.groq.com/openai/v1\",\n    api_key=os.environ.get(\"GROQ_API_KEY\")\n)\n\n---","keywords":["endpoints"]},{"title":"Script: Openai Compat (js)","content":"URL: https://console.groq.com/docs/scripts/openai-compat\n\nimport OpenAI from \"openai\";\n\nconst client = new OpenAI({\n  apiKey: process.env.GROQ_API_KEY,\n  baseURL: \"https://api.groq.com/openai/v1\"\n});\n\n---","keywords":["endpoints"]},{"title":"Qwen Qwq 32b: Model (tsx)","content":"URL: https://console.groq.com/docs/model/qwen-qwq-32b\n\nQwen/Qwq-32B is a 32-billion parameter reasoning model delivering competitive performance against state-of-the-art models like DeepSeek-R1 and o1-mini on complex reasoning and coding tasks. Deployed on Groq's hardware, it provides the world's fastest reasoning, producing chains and results in seconds.\nQwen/Qwq-32B is a 32-billion parameter reasoning model delivering competitive performance against state-of-the-art models like DeepSeek-R1 and o1-mini on complex reasoning and coding tasks. Deployed on Groq's hardware, it provides the world's fastest reasoning, producing chains and results in seconds.\nQwen/Qwq-32B is a 32-billion parameter reasoning model delivering competitive performance against state-of-the-art models like DeepSeek-R1 and o1-mini on complex reasoning and coding tasks. Deployed on Groq's hardware, it provides the world's fastest reasoning, producing chains and results in seconds.\n\n---","keywords":["models","endpoints","parameters","inference","qwen ","qwen-","qwen","deepseek-"]},{"title":"Qwen 2.5 Coder 32b: Model (tsx)","content":"URL: https://console.groq.com/docs/model/qwen-2.5-coder-32b\n\nQwen-2.5-Coder-32B is a specialized version of Qwen-2.5-32B, fine-tuned specifically for code generation and development tasks. Built on 5.5 trillion tokens of code and technical content, it delivers instant, production-quality code generation that matches GPT-4's capabilities.\nQwen-2.5-Coder-32B is a specialized version of Qwen-2.5-32B, fine-tuned specifically for code generation and development tasks. Built on 5.5 trillion tokens of code and technical content, it delivers instant, production-quality code generation that matches GPT-4's capabilities.\nQwen-2.5-Coder-32B is a specialized version of Qwen-2.5-32B, fine-tuned specifically for code generation and development tasks. Built on 5.5 trillion tokens of code and technical content, it delivers instant, production-quality code generation that matches GPT-4's capabilities.\nhttps://chat.groq.com/?model=qwen-2.5-coder-32b\nGroq Hosted AI Models\nen_US\nwebsite\n\n---","keywords":["models","authentication","endpoints","qwen 2.5","qwen-2.5-"]},{"title":"Llama3 70b 8192: Model (tsx)","content":"URL: https://console.groq.com/docs/model/llama3-70b-8192\n\nLlama 3.0 70B on Groq offers a balance of performance and speed as a reliable foundation model that excels at dialogue and content-generation tasks. While newer models have since emerged, Llama 3.0 70B remains production-ready and cost-effective with fast, consistent outputs via Groq API.\nLlama 3.0 70B on Groq offers a balance of performance and speed as a reliable foundation model that excels at dialogue and content-generation tasks. While newer models have since emerged, Llama 3.0 70B remains production-ready and cost-effective with fast, consistent outputs via Groq API.\nLlama 3.0 70B on Groq offers a balance of performance and speed as a reliable foundation model that excels at dialogue and content-generation tasks. While newer models have since emerged, Llama 3.0 70B remains production-ready and cost-effective with fast, consistent outputs via Groq API. \nhttps://chat.groq.com/?model=llama3-70b-8192\nGroq Hosted AI Models\nen_US\nwebsite\n\n---","keywords":["models","endpoints","pricing","inference","llama3","llama3-","llama 3.0"]},{"title":"Llama 3.1 8b Instant: Model (tsx)","content":"URL: https://console.groq.com/docs/model/llama-3.1-8b-instant\n\nllama-3.1-8b-instant on Groq offers rapid response times with production-grade reliability, suitable for latency-sensitive applications. The model balances efficiency and performance, providing quick responses for chat interfaces, content filtering systems, and large-scale data processing workloads.\nllama-3.1-8b-instant on Groq offers rapid response times with production-grade reliability, suitable for latency-sensitive applications. The model balances efficiency and performance, providing quick responses for chat interfaces, content filtering systems, and large-scale data processing workloads.\nllama-3.1-8b-instant on Groq offers rapid response times with production-grade reliability, suitable for latency-sensitive applications. The model balances efficiency and performance, providing quick responses for chat interfaces, content filtering systems, and large-scale data processing workloads.\n\n---","keywords":["models","endpoints","inference","llama 3.1","llama-3.1-"]},{"title":"Llama 3.2 3b Preview: Model (tsx)","content":"URL: https://console.groq.com/docs/model/llama-3.2-3b-preview\n\nLLaMA-3.2-3B-Preview is one of the fastest models on Groq, offering a great balance of speed and generation quality. With 3.1 billion parameters and a 128K context window, it delivers rapid responses while providing improved accuracy compared to the 1B version. The model excels at tasks like content creation, summarization, and information retrieval, making it ideal for applications where quality matters without requiring a large model. Its efficient design translates to cost-effective performance for real-time applications such as chatbots, content generation, and summarization tasks that need reliable responses with good output quality.\n\nLLaMA-3.2-3B-Preview is one of the fastest models on Groq, offering a great balance of speed and generation quality. With 3.1 billion parameters and a 128K context window, it delivers rapid responses while providing improved accuracy compared to the 1B version. The model excels at tasks like content creation, summarization, and information retrieval, making it ideal for applications where quality matters without requiring a large model. Its efficient design translates to cost-effective performance for real-time applications such as chatbots, content generation, and summarization tasks that need reliable responses with good output quality.\n\nhttps://chat.groq.com/?model=llama-3.2-3b-preview\nGroq Hosted AI Models\nen_US\nwebsite\n\nLLaMA-3.2-3B-Preview is one of the fastest models on Groq, offering a great balance of speed and generation quality. With 3.1 billion parameters and a 128K context window, it delivers rapid responses while providing improved accuracy compared to the 1B version. The model excels at tasks like content creation, summarization, and information retrieval, making it ideal for applications where quality matters without requiring a large model. Its efficient design translates to cost-effective performance for real-time applications such as chatbots, content generation, and summarization tasks that need reliable responses with good output quality.\n\nsummary_large_image\nGroq Hosted Models: LLaMA-3.2-3B-Preview\nLLaMA-3.2-3B-Preview is one of the fastest models on Groq, offering a great balance of speed and generation quality. With 3.1 billion parameters and a 128K context window, it delivers rapid responses while providing improved accuracy compared to the 1B version. The model excels at tasks like content creation, summarization, and information retrieval, making it ideal for applications where quality matters without requiring a large model. Its efficient design translates to cost-effective performance for real-time applications such as chatbots, content generation, and summarization tasks that need reliable responses with good output quality.\n\nindex: true\nfollow: true\n\nhttps://chat.groq.com/?model=llama-3.2-3b-preview\n\n---","keywords":["models","endpoints","parameters","pricing","inference","llama 3.2","llama-3.2-"]},{"title":"Llama 3.3 70b Specdec: Model (tsx)","content":"URL: https://console.groq.com/docs/model/llama-3.3-70b-specdec\n\nLlama-3.3-70B-SpecDec is Groq's speculative decoding version of Meta's Llama 3.3 70B model, optimized for high-speed inference while maintaining high quality. This speculative decoding variant delivers exceptional performance with significantly reduced latency, making it ideal for real-time applications while maintaining the robust capabilities of the Llama 3.3 70B architecture.\n\n---","keywords":["models","endpoints","error","inference","llama 3.3","llama-3.3-"]},{"title":"Llama3 8b 8192: Model (tsx)","content":"URL: https://console.groq.com/docs/model/llama3-8b-8192\n\nLlama-3-8B-8192 delivers exceptional performance with industry-leading speed and cost-efficiency on Groq hardware. This model stands out as one of the most economical options while maintaining impressive throughput, making it perfect for high-volume applications where both speed and cost matter.\n\nLlama-3-8B-8192 delivers exceptional performance with industry-leading speed and cost-efficiency on Groq hardware. This model stands out as one of the most economical options while maintaining impressive throughput, making it perfect for high-volume applications where both speed and cost matter.\n\nLlama-3-8B-8192 delivers exceptional performance with industry-leading speed and cost-efficiency on Groq hardware. This model stands out as one of the most economical options while maintaining impressive throughput, making it perfect for high-volume applications where both speed and cost matter.\n\n---","keywords":["models","endpoints","error","pricing","inference","llama3","llama3-","llama-3-"]},{"title":"Mistral Saba 24b: Model (tsx)","content":"URL: https://console.groq.com/docs/model/mistral-saba-24b\n\nMistral Saba 24B is a specialized model trained to excel in Arabic, Farsi, Urdu, Hebrew, and Indic languages. With a 32K token context window and tool use capabilities, it delivers exceptional results across multilingual tasks while maintaining strong performance in English.\nMistral Saba 24B is a specialized model trained to excel in Arabic, Farsi, Urdu, Hebrew, and Indic languages. With a 32K token context window and tool use capabilities, it delivers exceptional results across multilingual tasks while maintaining strong performance in English.\nMistral Saba 24B is a specialized model trained to excel in Arabic, Farsi, Urdu, Hebrew, and Indic languages. With a 32K token context window and tool use capabilities, it delivers exceptional results across multilingual tasks while maintaining strong performance in English.\n\n---","keywords":["models","authentication","endpoints","error","tools","inference","mistral ","mistral-"]},{"title":"Qwen 2.5 32b: Model (tsx)","content":"URL: https://console.groq.com/docs/model/qwen-2.5-32b\n\nQwen-2.5-32B is Alibaba's flagship model, delivering near-instant responses with GPT-4 level capabilities across a wide range of tasks. Built on 5.5 trillion tokens of diverse training data, it excels at everything from creative writing to complex reasoning.\n\n---","keywords":["models","authentication","endpoints","qwen 2.5","qwen-2.5-"]},{"title":"Deepseek R1 Distill Llama 70b: Model (tsx)","content":"URL: https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b","keywords":["models","endpoints","deepseek ","llama 70b","deepseek-","llama-70b"]},{"title":"Groq Hosted Models: DeepSeek-R1-Distill-Llama-70B","content":"DeepSeek-R1-Distill-Llama-70B is a distilled version of DeepSeek's R1 model, fine-tuned from the Llama-3.3-70B-Instruct base model. This model leverages knowledge distillation to retain robust reasoning capabilities and deliver exceptional performance on mathematical and logical reasoning tasks with Groq's industry-leading speed.\n\n### Open Graph\n* Title: Groq Hosted Models: DeepSeek-R1-Distill-Llama-70B\n* Description: DeepSeek-R1-Distill-Llama-70B is a distilled version of DeepSeek's R1 model, fine-tuned from the Llama-3.3-70B-Instruct base model. This model leverages knowledge distillation to retain robust reasoning capabilities and deliver exceptional performance on mathematical and logical reasoning tasks with Groq's industry-leading speed.\n* URL: https://chat.groq.com/?model=deepseek-r1-distill-llama-70b\n* Site Name: Groq Hosted AI Models\n* Images:\n  + URL: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B/og-image.jpg\n  + Width: 1200\n  + Height: 630\n  + Alt: DeepSeek-R1-Distill-Llama-70B Model\n* Locale: en_US\n* Type: website\n\n### Twitter\n* Card: summary_large_image\n* Title: Groq Hosted Models: DeepSeek-R1-Distill-Llama-70B\n* Description: DeepSeek-R1-Distill-Llama-70B is a distilled version of DeepSeek's R1 model, fine-tuned from the Llama-3.3-70B-Instruct base model. This model leverages knowledge distillation to retain robust reasoning capabilities and deliver exceptional performance on mathematical and logical reasoning tasks with Groq's industry-leading speed.\n* Images: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B/twitter-image.jpg\n\n### Robots\n* Index: true\n* Follow: true\n\n### Alternates\n* Canonical: https://chat.groq.com/?model=deepseek-r1-distill-llama-70b\n\n---","keywords":["models","endpoints","error","inference","deepseek-","llama-70b","deepseek","llama-3.3-"]},{"title":"PlayAI Dialog v1.0 Model Card","content":"URL: https://console.groq.com/docs/model/playai-tts","keywords":["models","endpoints"]},{"title":"PlayAI Dialog v1.0 Model Card","content":"### Model Details\n\n| Property | Value |\n|----------|-------|\n| **Name** | PlayAI Dialog |\n| **Model IDs** | `playai-tts`, `playai-tts-arabic` |\n| **Version** | 1.0 |\n| **Developer** | Playht, Inc |\n\n### Model Overview\n\nPlayAI Dialog v1.0 is a generative AI model designed to assist with creative content generation, interactive storytelling, and narrative development. Built on a transformer-based architecture, the model generates human-like audio to support writers, game developers, and content creators in vocalizing text to speech, crafting voice agentic experiences, or exploring interactive dialogue options.\n\n### Key Features\n\n- **Creative Generation**: Produces imaginative and contextually coherent audio based on user prompts and text\n- **Interactivity**: Supports dynamic conversation flows suitable for interactive storytelling, agent-like interactions, and gaming scenarios\n- **Customizability**: Allows users to clone voices and adjust tone, style, or narrative focus through configurable parameters\n\n### Model Architecture and Training\n\n#### Architecture\n\n- Based on a transformer architecture similar to state-of-the-art large language models\n- Optimized for high-quality speech output in a large variety of accents and styles\n\n#### Training Data\n\n- **Sources**: A blend of publicly available video and audio works, and interactive dialogue datasets, supplemented with licensed creative content and recordings\n- **Volume**: Trained on millions of audio samples spanning diverse genres, narrative, and conversational styles\n- **Preprocessing**: Involves standard audio normalization, tokenization, and filtering to remove sensitive or low-quality content\n\n### Evaluation and Performance Metrics\n\n#### Evaluation Datasets\n\n- Internally curated audio and dialogue datasets\n- Human user feedback from beta testing in creative applications and testing\n\n#### Performance Metrics\n\n| Metric | Result | Notes |\n|--------|--------|-------|\n| **Word Error Rate** | 1.4% | Measured on public (e.g. [CMU Arctic](http://www.festvox.org/cmu_arctic/)) and private datasets to guarantee diversity in sentence length, conversational styles, and alphanumerical representation. |\n| **Character Error Rate** | 0.6% | Measured on public (e.g. [CMU Arctic](http://www.festvox.org/cmu_arctic/)) and private datasets to guarantee diversity in sentence length, conversational styles, and alphanumerical representation. |\n| **Speaker Similarity** | 82% | Shows how closely an AI-generated voice matches the original. PlayAI's Dialog model leads the field with 86% similarity. Tested on eight cloned voices across short and long-form datasets, with 100 generations per voice using [ReDimNet](https://github.com/IDRnD/ReDimNet) for speaker similarity calculations. |\n| **Human Preference** | 80% | 3rd Party Testing (via [podonos](https://www.podonos.com/)) on 60 samples of emotive conversations (100 ratings each, 6,000 total), where the user was shown multiple samples from PlayAI Dialog and other leading models in the market and asked to choose a preference. |\n\n### Limitations and Bias Considerations\n\n#### Known Limitations\n\n- **Cultural Bias**: The model's outputs can reflect biases present in its training data. It might underrepresent certain pronunciations and accents.\n- **Variability**: The inherently stochastic nature of creative generation means that outputs can be unpredictable and may require human curation.\n\n#### Bias and Fairness Mitigation\n\n- **Bias Audits**: Regular reviews and bias impact assessments are conducted to identify poor quality or unintended audio generations.\n- **User Controls**: Users are encouraged to provide feedback on problematic outputs, which informs iterative updates and bias mitigation strategies.\n\n### Ethical and Regulatory Considerations\n\n#### Data Privacy\n\n- All training data has been processed and anonymized in accordance with GDPR and other relevant data protection laws.\n- We do not train on any of our user data.\n\n#### Ethical Use Guidelines\n\n- Users should ensure that PlayAI is applied responsibly ‚Äî particularly in creative contexts where content sensitivity is important.\n- The model should not be used to generate harmful, misleading, or plagiarized content.\n- Responsible deployment practices are strongly encouraged.\n\n### Maintenance and Updates\n\n#### Versioning\n\n- PlayAI Dialog v1.0 is the inaugural release.\n- Future versions will integrate more languages, emotional controllability, and custom voices.\n\n#### Support and Feedback\n\n- Users are invited to submit feedback and report issues via \"Chat with us\" on [Groq Console](https://console.groq.com).\n- Regular updates and maintenance reviews are scheduled to ensure ongoing compliance with legal standards and to incorporate evolving best practices.\n\n### Licensing\n\n- **License**: PlayAI-Groq Commercial License\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","error","tools","speech","inference"]},{"title":"Llama Guard 3 8b: Model (tsx)","content":"URL: https://console.groq.com/docs/model/llama-guard-3-8b\n\nLlama-Guard-3-8B, a specialized content moderation model built on the Llama framework, excels at identifying and filtering potentially harmful content. Groq supports fast inference with industry-leading latency and performance for high-speed AI processing for your content moderation applications.\nLlama-Guard-3-8B, a specialized content moderation model built on the Llama framework, excels at identifying and filtering potentially harmful content. Groq supports fast inference with industry-leading latency and performance for high-speed AI processing for your content moderation applications.\nhttps://chat.groq.com/?model=llama-guard-3-8b\nGroq Hosted AI Models\nen_US\nwebsite\nsummary_large_image\nGroq Hosted Models: Llama-Guard-3-8B\nLlama-Guard-3-8B, a specialized content moderation model built on the Llama framework, excels at identifying and filtering potentially harmful content. Groq supports fast inference with industry-leading latency and performance for high-speed AI processing for your content moderation applications.\ntrue\ntrue\nhttps://chat.groq.com/?model=llama-guard-3-8b\n\n---","keywords":["models","endpoints","inference","llama ","llama-"]},{"title":"Deepseek R1 Distill Qwen 32b: Model (tsx)","content":"URL: https://console.groq.com/docs/model/deepseek-r1-distill-qwen-32b\n\nDeepSeek-R1-Distill-Qwen-32B is a distilled version of DeepSeek's R1 model, fine-tuned from the Qwen-2.5-32B base model. This model leverages knowledge distillation to retain robust reasoning capabilities while enhancing efficiency. Delivering exceptional performance on mathematical and logical reasoning tasks, it achieves near-o1 level capabilities with faster response times. With its massive 128K context window, native tool use, and JSON mode support, it excels at complex problem-solving while maintaining the reasoning depth of much larger models.\n\nDeepSeek-R1-Distill-Qwen-32B is a distilled version of DeepSeek's R1 model, fine-tuned from the Qwen-2.5-32B base model. This model leverages knowledge distillation to retain robust reasoning capabilities while enhancing efficiency. Delivering exceptional performance on mathematical and logical reasoning tasks, it achieves near-o1 level capabilities with faster response times. With its massive 128K context window, native tool use, and JSON mode support, it excels at complex problem-solving while maintaining the reasoning depth of much larger models.\n\nhttps://chat.groq.com/?model=deepseek-r1-distill-qwen-32b\nGroq Hosted AI Models\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B/og-image.jpg\n1200\n630\nDeepSeek-R1-Distill-Qwen-32B Model\nen_US\nwebsite\n\nsummary_large_image\nGroq Hosted Models: DeepSeek-R1-Distill-Qwen-32B\nDeepSeek-R1-Distill-Qwen-32B is a distilled version of DeepSeek's R1 model, fine-tuned from the Qwen-2.5-32B base model. This model leverages knowledge distillation to retain robust reasoning capabilities while enhancing efficiency. Delivering exceptional performance on mathematical and logical reasoning tasks, it achieves near-o1 level capabilities with faster response times. With its massive 128K context window, native tool use, and JSON mode support, it excels at complex problem-solving while maintaining the reasoning depth of much larger models.\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B/twitter-image.jpg\n\ntrue\ntrue\n\nhttps://chat.groq.com/?model=deepseek-r1-distill-qwen-32b\n\n---","keywords":["models","endpoints","error","tools","inference","deepseek ","qwen 32b","deepseek-","qwen-32b","deepseek","qwen-2.5-"]},{"title":"Deepseek R1 Distill Llama 70b Specdec: Model (tsx)","content":"URL: https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b-specdec","keywords":["models","endpoints","deepseek ","llama 70b","deepseek-","llama-70b"]},{"title":"Groq Hosted Models: DeepSeek-R1-Distill-Llama-70B-SpecDec","content":"DeepSeek-R1-Distill-Llama-70B-SpecDec is Groq's speculative decoding version of DeepSeek's R1 Distill Llama 70B model. With the reasoning patterns of DeepSeek's R1 distilled to the base Llama 3 model, you can achieve better performance on mathematical and logical reasoning tasks and with significantly faster response times as a result of speculative decoding on Groq.\n\n### Open Graph\n* title: Groq Hosted Models: DeepSeek-R1-Distill-Llama-70B-SpecDec\n* description: DeepSeek-R1-Distill-Llama-70B-SpecDec is a speculative decoding version of DeepSeek's R1 model, fine-tuned from the Llama-3.3-70B-SpecDec base model. This model leverages knowledge distillation to retain robust reasoning capabilities while enhancing efficiency, delivering exceptional performance on mathematical and logical reasoning tasks with significantly faster response times.\n* url: https://chat.groq.com/?model=deepseek-r1-distill-llama-70b-specdec\n* siteName: Groq Hosted AI Models\n* locale: en_US\n* type: website\n\n### Twitter\n* card: summary_large_image\n* title: Groq Hosted Models: DeepSeek-R1-Distill-Llama-70B-SpecDec\n* description: DeepSeek-R1-Distill-Llama-70B-SpecDec is a speculative decoding version of DeepSeek's R1 model, fine-tuned from the Llama-3.3-70B-SpecDec base model. This model leverages knowledge distillation to retain robust reasoning capabilities while enhancing efficiency, delivering exceptional performance on mathematical and logical reasoning tasks with significantly faster response times.\n\n### Robots\n* index: true\n* follow: true\n\n### Alternates\n* canonical: https://chat.groq.com/?model=deepseek-r1-distill-llama-70b-specdec\n\n---","keywords":["models","endpoints","error","inference","deepseek-","llama-70b","deepseek","llama 70b","llama 3","llama-3.3-"]},{"title":"Llama 3.2 1b Preview: Model (tsx)","content":"URL: https://console.groq.com/docs/model/llama-3.2-1b-preview\n\nLLaMA-3.2-1B-Preview is one of the fastest models on Groq, making it perfect for cost-sensitive, high-throughput applications. With just 1.23 billion parameters and a 128K context window, it delivers near-instant responses while maintaining impressive accuracy for its size. The model excels at essential tasks like text analysis, information retrieval, and content summarization, offering an optimal balance of speed, quality and cost. Its lightweight nature translates to significant cost savings compared to larger models, making it an excellent choice for rapid prototyping, content processing, and applications requiring quick, reliable responses without excessive computational overhead.\n\nLLaMA-3.2-1B-Preview is one of the fastest models on Groq, making it perfect for cost-sensitive, high-throughput applications. With just 1.23 billion parameters and a 128K context window, it delivers near-instant responses while maintaining impressive accuracy for its size. The model excels at essential tasks like text analysis, information retrieval, and content summarization, offering an optimal balance of speed, quality and cost. Its lightweight nature translates to significant cost savings compared to larger models, making it an excellent choice for rapid prototyping, content processing, and applications requiring quick, reliable responses without excessive computational overhead.\n\nhttps://chat.groq.com/?model=llama-3.2-1b-preview\nGroq Hosted AI Models\nen_US\nwebsite\n\nLLaMA-3.2-1B-Preview is one of the fastest models on Groq, making it perfect for cost-sensitive, high-throughput applications. With just 1.23 billion parameters and a 128K context window, it delivers near-instant responses while maintaining impressive accuracy for its size. The model excels at essential tasks like text analysis, information retrieval, and content summarization, offering an optimal balance of speed, quality and cost. Its lightweight nature translates to significant cost savings compared to larger models, making it an excellent choice for rapid prototyping, content processing, and applications requiring quick, reliable responses without excessive computational overhead.\n\nindex: true\nfollow: true\n\nhttps://chat.groq.com/?model=llama-3.2-1b-preview\n\n---","keywords":["models","endpoints","parameters","pricing","inference","llama 3.2","llama-3.2-"]},{"title":"LiveKit + Groq: Build End-to-End AI Voice Applications","content":"URL: https://console.groq.com/docs/livekit","keywords":["endpoints","speech"]},{"title":"LiveKit + Groq: Build End-to-End AI Voice Applications","content":"[LiveKit](https://livekit.io) complements Groq's high-performance speech recognition capabilities by providing text-to-speech and real-time communication features. This integration enables you to build \nend-to-end AI voice applications with:\n\n- **Complete Voice Pipeline:** Combine Groq's fast and accurate speech-to-text (STT) with LiveKit's text-to-speech (TTS) capabilities\n- **Real-time Communication:** Enable multi-user voice interactions with LiveKit's WebRTC infrastructure\n- **Flexible TTS Options:** Access multiple text-to-speech voices and languages through LiveKit's TTS integrations\n- **Scalable Architecture:** Handle thousands of concurrent users with LiveKit's distributed system\n\n### Quick Start (7 minutes to hello world)\n\n#### 1. Prerequisites\n- Grab your [Groq API Key](https://console.groq.com/keys)\n- Create a free [LiveKit Cloud account](https://cloud.livekit.io/login)\n- Install the [LiveKit CLI](https://docs.livekit.io/home/cli/cli-setup/) and authenticate in your Command Line Interface (CLI)\n- Create a free ElevenLabs account and [generate an API Key](https://elevenlabs.io/app/settings/api-keys)\n\n#### 1. Clone the starter template for our Python voice agent using your CLI:\nWhen prompted for your OpenAI and Deepgram API key, press **Enter** to skip as we'll be using custommized plugins for Groq and ElevenLabs for fast inference speed.\n```bash\nlk app create --template voice-pipeline-agent-python\n```\n\n#### 2. CD into your project directory and update the `.env.local` file to replace `OPENAI_API_KEY` and `DEEPGRAM_API_KEY` with the following:\n```bash\nGROQ_API_KEY=<your-groq-api-key>\nELEVEN_API_KEY=<your-elevenlabs-api-key>\n```\n\n#### 3. Update your `requirements.txt` file and add the following line:\n```bash\nlivekit-plugins-elevenlabs>=0.7.9\n```\n\n#### 4. Update your `agent.py` file with the following to configure Groq for STT with `whisper-large-v3`, Groq for LLM with `llama-3.3-70b-versatile`, and ElevenLabs for TTS:\n```python\nload_dotenv(dotenv_path=\".env.local\")\nlogger = logging.getLogger(\"voice-agent\")\n\n\ndef prewarm(proc: JobProcess):\n    proc.userdata[\"vad\"] = silero.VAD.load()\n\n\nasync def entrypoint(ctx: JobContext):\n    initial_ctx = llm.ChatContext().append(\n        role=\"system\",\n        text=(\n            \"You are a voice assistant created by LiveKit. Your interface with users will be voice. \"\n            \"You should use short and concise responses, and avoiding usage of unpronouncable punctuation. \"\n            \"You were created as a demo to showcase the capabilities of LiveKit's agents framework.\"\n        ),\n    )\n\n    logger.info(f\"connecting to room {ctx.room.name}\")\n    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)\n\n    # Wait for the first participant to connect\n    participant = await ctx.wait_for_participant()\n    logger.info(f\"starting voice assistant for participant {participant.identity}\")\n\n    agent = VoicePipelineAgent(\n        vad=ctx.proc.userdata[\"vad\"],\n        stt=openai.STT.with_groq(model=\"whisper-large-v3\"),\n        llm=openai.LLM.with_groq(model=\"llama-3.3-70b-versatile\"),\n        tts=elevenlabs.TTS(),\n        chat_ctx=initial_ctx,\n    )\n\n    agent.start(ctx.room, participant)\n\n    # The agent should be polite and greet the user when it joins :)\n    await agent.say(\"Hey, how can I help you today?\", allow_interruptions=True)\n\n\nif __name__ == \"__main__\":\n    cli.run_app(\n        WorkerOptions(\n            entrypoint_fnc=entrypoint,\n            prewarm_fnc=prewarm,\n        ),\n    )\n\n```\n\n#### 5. Make sure you're in your project directory to install the dependencies and start your agent:\n```bash\npython3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install -r requirements.txt\npython3 agent.py dev\n```\n\n#### 6. Within your project directory, clone the voice assistant frontend Next.js app starter template using your CLI:\n```bash\nlk app create --template voice-assistant-frontend\n```\n\n#### 7. CD into your frontend directory and launch your frontend application locally:\n```bash\npnpm install\npnpm dev\n```\n\n#### 8. Visit your application (http://localhost:3000/ by default), select **Connect** and talk to your agent! \n\n**Challenge:** Configure your voice assistant and the frontend to create a travel agent that will help plan trips! \n\nFor more detailed documentation and resources, see:\n- [Official Documentation: LiveKit](https://docs.livekit.io)\n\n---","keywords":["models","authentication","endpoints","rate limits","tools","speech","inference","whisper-","llama-3.3-"]},{"title":"Agno + Groq: Lightning Fast Agents","content":"URL: https://console.groq.com/docs/agno","keywords":["endpoints","tools","inference"]},{"title":"Agno + Groq: Lightning Fast Agents","content":"[Agno](https://github.com/agno-agi/agno) is a lightweight framework for building multi-modal Agents. Its easy to use, extremely fast and supports multi-modal inputs and outputs.\nWith Groq & Agno, you can build:\n\n- **Agentic RAG**: Agents that can search different knowledge stores for RAG or dynamic few-shot learning.\n- **Image Agents**: Agents that can understand images and make tool calls accordingly.\n- **Reasoning Agents**: Agents that can reason using a reasoning model, then generate a result using another model.\n- **Structured Outputs**: Agents that can generate pydantic objects adhering to a schema.\n\n### Python Quick Start (2 minutes to hello world)\n\nAgents are autonomous programs that use language models to achieve tasks. They solve problems by running tools, accessing knowledge and memory to improve responses.\nLet's build a simple web search agent, with a tool to search DuckDuckGo to get better results. \n\n#### 1. Create a file called `web_search_agent.py` and add the following code:\n```python web_search_agent.py\nfrom agno.agent import Agent\nfrom agno.models.groq import Groq\nfrom agno.tools.duckduckgo import DuckDuckGoTools\n\n# Initialize the agent with an LLM via Groq and DuckDuckGoTools\nagent = Agent(\n    model=Groq(id=\"llama-3.3-70b-versatile\"),\n    description=\"You are an enthusiastic news reporter with a flair for storytelling!\",\n    tools=[DuckDuckGoTools()],      # Add DuckDuckGo tool to search the web\n    show_tool_calls=True,           # Shows tool calls in the response, set to False to hide\n    markdown=True                   # Format responses in markdown\n)\n\n# Prompt the agent to fetch a breaking news story from New York\nagent.print_response(\"Tell me about a breaking news story from New York.\", stream=True)\n```\n\n#### 3. Set up and activate your virtual environment:\n```shell\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\n#### 4. Install the Groq, Agno, and DuckDuckGo dependencies:\n```shell\npip install -U groq agno duckduckgo-search\n```\n\n#### 5. Configure your Groq API Key:\n```bash\nGROQ_API_KEY=\"your-api-key\"\n```\n\n#### 6. Run your Agno agent that now extends your LLM's context to include web search for up-to-date information and send results in seconds:\n```shell\npython web_search_agent.py\n```\n\n### Multi-Agent Teams\nAgents work best when they have a singular purpose, a narrow scope, and a small number of tools. When the number of tools grows beyond what the language model can handle or the tools belong to different \ncategories, use a **team of agents** to spread the load.\nThe following code expands upon our quick start and creates a team of two agents to provide analysis on financial markets:\n```python agent_team.py\nfrom agno.agent import Agent\nfrom agno.models.groq import Groq\nfrom agno.tools.duckduckgo import DuckDuckGoTools\nfrom agno.tools.yfinance import YFinanceTools\n\nweb_agent = Agent(\n    name=\"Web Agent\",\n    role=\"Search the web for information\",\n    model=Groq(id=\"llama-3.3-70b-versatile\"),\n    tools=[DuckDuckGoTools()],\n    instructions=\"Always include sources\",\n    markdown=True,\n)\n\nfinance_agent = Agent(\n    name=\"Finance Agent\",\n    role=\"Get financial data\",\n    model=Groq(id=\"llama-3.3-70b-versatile\"),\n    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],\n    instructions=\"Use tables to display data\",\n    markdown=True,\n)\n\nagent_team = Agent(\n    team=[web_agent, finance_agent],\n    model=Groq(id=\"llama-3.3-70b-versatile\"),  # You can use a different model for the team leader agent\n    instructions=[\"Always include sources\", \"Use tables to display data\"],\n    # show_tool_calls=True,  # Uncomment to see tool calls in the response\n    markdown=True,\n)\n\n# Give the team a task\nagent_team.print_response(\"What's the market outlook and financial performance of AI semiconductor companies?\", stream=True)\n```\n\n### Additional Resources\nFor additional documentation and support, see the following:\n\n- [Agno Documentation](https://docs.agno.com)\n- [Groq via Agno Documentation](https://docs.agno.com/models/groq)\n- [Groq via Agno examples](https://docs.agno.com/examples/models/groq/basic)\n- [Various industry-ready examples](https://docs.agno.com/examples/introduction)\n\n---","keywords":["models","authentication","rate limits","error","pricing","tools","inference","llama-3.3-"]},{"title":"API Error Codes and Responses","content":"URL: https://console.groq.com/docs/errors","keywords":["endpoints","error"]},{"title":"API Error Codes and Responses","content":"Our API uses standard HTTP response status codes to indicate the success or failure of an API request. In cases of errors, the body of the response will contain a JSON object with details about the error. Below are the error codes you may encounter, along with their descriptions and example response bodies.\n\n\n### Error Codes Documentation\n\nOur API uses specific error codes to indicate the success or failure of an API request. Understanding these codes and their implications is essential for effective error handling and debugging.\n\n### Success Codes\n\n- **200 OK**: The request was successfully executed. No further action is needed.\n\n### Client Error Codes\n\n- **400 Bad Request**: The server could not understand the request due to invalid syntax. Review the request format and ensure it is correct.\n- **401 Unauthorized**: The request was not successful because it lacks valid authentication credentials for the requested resource. Ensure the request includes the necessary authentication credentials and the api key is valid.\n- **404 Not Found**: The requested resource could not be found. Check the request URL and the existence of the resource.\n- **422 Unprocessable Entity**: The request was well-formed but could not be followed due to semantic errors. Verify the data provided for correctness and completeness.\n- **429 Too Many Requests**: Too many requests were sent in a given timeframe. Implement request throttling and respect rate limits.\n- **498 Custom: Flex Tier Capacity Exceeded**: This is a custom status code we use and will return in the event that the flex tier is at capacity and the request won't be processed. You can try again later.\n- **499 Custom: Request Cancelled**: This is a custom status code we use in our logs page to signify when the request is cancelled by the caller.\n\n### Server Error Codes\n\n- **500 Internal Server Error**: A generic error occurred on the server. Try the request again later or contact support if the issue persists.\n- **502 Bad Gateway**: The server received an invalid response from an upstream server. This may be a temporary issue; retrying the request might resolve it.\n- **503 Service Unavailable**: The server is not ready to handle the request, often due to maintenance or overload. Wait before retrying the request.\n\n### Informational Codes\n\n- **206 Partial Content**: Only part of the resource is being delivered, usually in response to range headers sent by the client. Ensure this is expected for the request being made.\n\n### Error Object Explanation\n\nWhen an error occurs, our API returns a structured error object containing detailed information about the issue. This section explains the components of the error object to aid in troubleshooting and error handling.\n\n### Error Object Structure\n\nThe error object follows a specific structure, providing a clear and actionable message alongside an error type classification:\n\n```json\n{\n  \"error\": {\n    \"message\": \"String - description of the specific error\",\n    \"type\": \"invalid_request_error\"\n  }\n}\n```\n\n### Components\n\n- **`error` (object):** The primary container for error details.\n  - **`message` (string):** A descriptive message explaining the nature of the error, intended to aid developers in diagnosing the problem.\n  - **`type` (string):** A classification of the error type, such as `\"invalid_request_error\"`, indicating the general category of the problem encountered.\n\n---","keywords":["authentication","endpoints","rate limits","error"]},{"title":"Composio","content":"URL: https://console.groq.com/docs/composio","keywords":["endpoints"]},{"title":"Composio","content":"[Composio](https://composio.ai/) is a platform for managing and integrating tools with LLMs and AI agents. You can build fast, Groq-based assistants to seamlessly interact with external applications \nthrough features including:\n\n- **Tool Integration:** Connect AI agents to APIs, RPCs, shells, file systems, and web browsers with 90+ readily available tools\n- **Authentication Management:** Secure, user-level auth across multiple accounts and tools\n- **Optimized Execution:** Improve security and cost-efficiency with tailored execution environments\n- **Comprehensive Logging:** Track and analyze every function call made by your LLMs\n\n### Python Quick Start (5 minutes to hello world)\n#### 1. Install the required packages:\n```bash\npip install composio-langchain langchain-groq\n```\n\n#### 2. Configure your Groq and [Composio](https://app.composio.dev/) API keys:\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\nexport COMPOSIO_API_KEY=\"your-composio-api-key\"\n```\n\n#### 3. Connect your first Composio tool:\n```bash\n# Connect GitHub (you'll be guided through OAuth flow to get things going)\ncomposio add github\n\n# View all available tools\ncomposio apps\n```\n\n#### 4. Create your first Composio-enabled Groq agent:\nRunning this code will create an agent that can interact with GitHub through natural language in mere seconds! Your agent will be able to:\n- Perform GitHub operations like starring repos and creating issues for you\n- Securely manage your OAuth flows and API keys\n- Process natural language to convert your requests into specific tool actions \n- Provide feedback to let you know about the success or failure of operations\n\n```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_groq import ChatGroq\nfrom composio_langchain import ComposioToolSet, App\n\n# Initialize LLM\nllm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n\n# Get Composio tools (GitHub in this example)\ncomposio_toolset = ComposioToolSet()\ntools = composio_toolset.get_tools(apps=[App.GITHUB])\n\n# Create agent\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\n# Define task and run\ntask = \"Star groq/groq-api-cookbook repo on GitHub\"\nagent.run(task)\n\n**Challenge**: Create a Groq-powered agent that can summarize your GitHub issues and post updates to Slack through Composio tools! \n\nFor more detailed documentation and resources on building AI agents with Groq and Composio, see:\n- [Composio documentation](https://docs.composio.dev/framework/groq)\n- [Guide to Building Agents with Composio and Llama 3.1 models powered by Groq](https://composio.dev/blog/tool-calling-in-llama-3-a-guide-to-build-agents/)\n- [Groq API Cookbook tutorial](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/composio-newsletter-summarizer-agent)\n\n---","keywords":["models","authentication","endpoints","error","pricing","tools","inference","llama-3.3-","llama 3.1","llama-3-"]},{"title":"Overview: Chat (json)","content":"URL: https://console.groq.com/docs/overview/scripts/chat.json\n\n{\n  \"model\": \"llama-3.3-70b-versatile\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Explain the importance of fast language models\"\n    }\n  ]\n}\n\n---","keywords":["models","endpoints","inference","llama-3.3-"]},{"title":"Overview: Chat (py)","content":"URL: https://console.groq.com/docs/overview/scripts/chat.py\n\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of fast language models\",\n        }\n    ],\n    model=\"llama-3.3-70b-versatile\",\n    stream=False,\n)\n\nprint(chat_completion.choices[0].message.content)\n\n---","keywords":["models","endpoints","inference","llama-3.3-"]},{"title":"Overview: Chat (js)","content":"URL: https://console.groq.com/docs/overview/scripts/chat\n\n// Default\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq({ apiKey: process.env.GROQ_API_KEY });\n\nasync function main() {\n  const completion = await groq.chat.completions\n    .create({\n      messages: [\n        {\n          role: \"user\",\n          content: \"Explain the importance of fast language models\",\n        },\n      ],\n      model: \"llama-3.3-70b-versatile\",\n    })\n    .then((chatCompletion) => {\n      console.log(chatCompletion.choices[0]?.message?.content || \"\");\n    });\n}\n\nmain();\n\n---","keywords":["models","endpoints","tools","inference","llama-3.3-"]},{"title":"Overview","content":"URL: https://console.groq.com/docs/overview","keywords":["endpoints"]},{"title":"Overview","content":"Fast LLM inference, OpenAI-compatible. Simple to integrate, easy to scale. Start building in minutes.\n\n\n#### Start building apps on Groq\n\nGet up and running with the Groq API in a few minutes.\nCreate and setup your API Key\n\nExperiment with the Groq API\n\nCheck out cool Groq built apps\n\n\n#### Developer Resources\n\nEssential resources to accelerate your development and maximize productivity\n\nExplore all API parameters and response attributes\nCheck out sneak peeks, announcements & get support\nSee code examples and tutorials to jumpstart your app\nCompatible with OpenAI's client libraries\n\n\n#### The Models\n\nWe‚Äôre adding new models all the time and will let you know when a new one comes online.  See full details on our Models page.\n\nDeepseek R1 Distill Llama 70B\nLlama 3.3, 3.2, 3.1, and LlamaGuard\nWhisper Large v3, Turbo, and Distill\nMixtral 8x7b\nGemma 2\n\n---","keywords":["models","authentication","parameters","rate limits","speech","inference","deepseek ","llama 70b","llama 3.3","llama","whisper ","mixtral 8","gemma 2"]},{"title":"‚ú® Vercel AI SDK + Groq: Rapid App Development","content":"URL: https://console.groq.com/docs/ai-sdk","keywords":["endpoints"]},{"title":"‚ú® Vercel AI SDK + Groq: Rapid App Development","content":"Vercel's AI SDK enables seamless integration with Groq, providing developers with powerful tools to leverage language models hosted on Groq for a variety of applications. By combining Vercel's cutting-edge platform with Groq's advanced inference capabilities, developers can create scalable, high-speed applications with ease.\n\n### Why Choose the Vercel AI SDK?\n- A versatile toolkit for building applications powered by advanced language models like Llama 3.3 70B \n- Ideal for creating chat interfaces, document summarization, and natural language generation\n- Simple setup and flexible provider configurations for diverse use cases\n- Fully supports standalone usage and seamless deployment with Vercel\n- Scalable and efficient for handling complex tasks with minimal configuration\n\n### Quick Start Guide in JavaScript (5 minutes to deployment)\n\n#### 1. Create a new Next.js project with the AI SDK template:\n```bash\nnpx create-next-app@latest my-groq-app --typescript --tailwind --src-dir\ncd my-groq-app\n```\n#### 2. Install the required packages:\n```bash\nnpm install @ai-sdk/groq ai\nnpm install react-markdown\n```\n\n#### 3. Create a `.env.local` file in your project root and configure your Groq API Key:\n```bash\nGROQ_API_KEY=\"your-api-key\"\n```\n\n#### 4. Create a new directory structure for your Groq API endpoint:\n```bash\nmkdir -p src/app/api/chat\n```\n\n#### 5. Initialize the AI SDK by creating an API route file called `route.ts` in `app/api/chat`:\n```javascript\n// Allow streaming responses up to 30 seconds\nconst maxDuration = 30;\n\nasync function POST(req) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: groq('llama-3.3-70b-versatile'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n**Challenge**: Now that you have your basic chat interface working, try enhancing it to create a specialized code explanation assistant! \n\n#### 6. Create your front end interface by updating the `app/page.tsx` file:\n```javascript\n'use client';\n\nconst Chat = () => {\n  const messages = [];\n  const input = '';\n  const handleInputChange = () => {};\n  const handleSubmit = () => {};\n\n  return (\n    <div className=\"min-h-screen bg-white\">\n      <div className=\"mx-auto w-full max-w-2xl py-8 px-4\">\n        <div className=\"space-y-4 mb-4\">\n          {messages.map(m => (\n            <div \n              key={m.id} \n              className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}\n            >\n              <div \n                className={`\n                  max-w-[80%] rounded-lg px-4 py-2\n                  ${m.role === 'user' \n                    ? 'bg-blue-100 text-black' \n                    : 'bg-gray-100 text-black'}\n                `}\n              >\n                <div className=\"text-xs text-gray-500 mb-1\">\n                  {m.role === 'user' ? 'You' : 'Llama 3.3 70B powered by Groq'}\n                </div>\n                <div className=\"text-sm whitespace-pre-wrap\">\n                  {m.content}\n                </div>\n              </div>\n            </div>\n          ))}\n        </div>\n\n        <form onSubmit={handleSubmit} className=\"flex gap-4\">\n          <input\n            value={input}\n            onChange={handleInputChange}\n            placeholder=\"Type your message...\"\n            className=\"flex-1 rounded-lg border border-gray-300 px-4 py-2 text-black focus:outline-none focus:ring-2 focus:ring-[#f55036]\"\n          />\n          <button \n            type=\"submit\"\n            className=\"rounded-lg bg-[#f55036] px-4 py-2 text-white hover:bg-[#d94530] focus:outline-none focus:ring-2 focus:ring-[#f55036]\"\n          >\n            Send\n          </button>\n        </form>\n      </div>\n    </div>\n  );\n}\n\n#### 7. Run your development enviornment to test our application locally:\n```bash\nnpm run dev\n```\n\n#### 8. Easily deploy your application using Vercel CLI by installing `vercel` and then running the `vercel` command:\n\nThe CLI will guide you through a few simple prompts:\n- If this is your first time using Vercel CLI, you'll be asked to create an account or log in\n- Choose to link to an existing Vercel project or create a new one\n- Confirm your deployment settings \n\nOnce you've gone through the prompts, your app will be deployed instantly and you'll receive a production URL! \n```bash\nnpm install -g vercel\nvercel\n```\n\n### Additional Resources\n\nFor more details on integrating Groq with the Vercel AI SDK, see the following:\n- [Official Documentation: Vercel](https://sdk.vercel.ai/providers/ai-sdk-providers/groq)\n- [Vercel Templates for Groq](https://sdk.vercel.ai/providers/ai-sdk-providers/groq)\n\n---","keywords":["models","authentication","endpoints","tools","inference","llama 3.3","llama-3.3-"]},{"title":"E2B + Groq: Open-Source Code Interpreter","content":"URL: https://console.groq.com/docs/e2b","keywords":["endpoints"]},{"title":"E2B + Groq: Open-Source Code Interpreter","content":"[E2B](https://e2b.dev/) Code Interpreter is an open-source SDK that provides secure, sandboxed environments for executing code generated by LLMs via Groq API. Built specifically for AI data analysts, \ncoding applications, and reasoning-heavy agents, E2B enables you to both generate and execute code in a secure sandbox environment in real-time.\n\n### Python Quick Start (3 minutes to hello world)\n\n#### 1. Install the required packages:\n```bash\npip install groq e2b-code-interpreter python-dotenv\n```\n\n#### 2. Configure your Groq and [E2B](https://e2b.dev/docs) API keys:\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\nexport E2B_API_KEY=\"your-e2b-api-key\"\n```\n\n#### 3. Create your first simple and fast Code Interpreter application that generates and executes code to analyze data:\n\nRunning the below code will create a secure sandbox environment, generate Python code using `llama-3.3-70b-versatile` powered by Groq, execute the code, and display the results. When you go to your \n[E2B Dashboard](https://e2b.dev/dashboard), you'll see your sandbox's data. \n\n```python\nfrom e2b_code_interpreter import Sandbox\nfrom groq import Groq\nimport os\n\ne2b_api_key = os.environ.get('E2B_API_KEY')\ngroq_api_key = os.environ.get('GROQ_API_KEY')\n\n# Initialize Groq client\nclient = Groq(api_key=groq_api_key)\n\nSYSTEM_PROMPT = \"\"\"You are a Python data scientist. Generate simple code that:\n1. Uses numpy to generate 5 random numbers\n2. Prints only the mean and standard deviation in a clean format\nExample output format:\nMean: 5.2\nStd Dev: 1.8\"\"\"\n\ndef main():\n    # Create sandbox instance (by default, sandbox instances stay alive for 5 mins)\n    sbx = Sandbox()\n    \n    # Get code from Groq\n    response = client.chat.completions.create(\n        model=\"llama-3.1-70b-versatile\",\n        messages=[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": \"Generate random numbers and show their mean and standard deviation\"}\n        ]\n    )\n    \n    # Extract and run the code\n    code = response.choices[0].message.content\n    if \"```python\" in code:\n        code = code.split(\"```python\")[1].split(\"```\")[0]\n    \n    print(\"\\nGenerated Python code:\")\n    print(code)\n    \n    print(\"\\nExecuting code in sandbox...\")\n    execution = sbx.run_code(code)\n    print(execution.logs.stdout[0])\n    \nif __name__ == \"__main__\":\n    main()\n\n**Challenge**: Try modifying the example to analyze your own dataset or solve a different data science problem!\n\nFor more detailed documentation and resources on building with E2B and Groq, see:\n- [Tutorial: Code Interpreting with Groq (Python)](https://e2b.dev/blog/guide-code-interpreting-with-groq-and-e2b)\n- [Tutorial: Code Interpreting with Groq (JavaScript)](https://e2b.dev/blog/guide-groq-js)\n\n---","keywords":["models","authentication","endpoints","rate limits","error","tools","inference","llama-3.3-","llama-3.1-"]},{"title":"üöÖ LiteLLM + Groq for Production Deployments","content":"URL: https://console.groq.com/docs/litellm","keywords":["models","endpoints"]},{"title":"üöÖ LiteLLM + Groq for Production Deployments","content":"[LiteLLM](https://docs.litellm.ai/docs/) provides a simple framework with features to help productionize your application infrastructure, including:\n\n- **Cost Management:** Track spending, set budgets, and implement rate limiting for optimal resource utilization\n- **Smart Caching:** Cache frequent responses to reduce API calls while maintaining Groq's speed advantage\n- **Spend Tracking:** Track spend for individual API keys, users, and teams\n\n### Quick Start (2 minutes to hello world)\n\n#### 1. Install the package:\n```bash\npip install litellm\n```\n\n#### 2. Set up your API key:\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\n```\n\n#### 3. Send your first request:\n```python\nimport os\nimport litellm\n\napi_key = os.environ.get('GROQ_API_KEY')\n\n\nresponse = litellm.completion(\n    model=\"groq/llama-3.3-70b-versatile\", \n    messages=[\n       {\"role\": \"user\", \"content\": \"hello from litellm\"}\n   ],\n)\nprint(response)\n```\n\n### Next Steps\nFor detailed setup of advanced features:\n- [Configuration of Spend Tracking for Keys, Users, and Teams](https://docs.litellm.ai/docs/proxy/cost_tracking)\n- [Configuration for Budgets and Rate Limits](https://docs.litellm.ai/docs/proxy/users)\n\nFor more information on building production-ready applications with LiteLLM and Groq, see:\n- [Official Documentation: LiteLLM](https://docs.litellm.ai/docs/providers/groq)\n- [Tutorial: Groq API Cookbook](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/litellm-proxy-groq)\n\n---","keywords":["models","authentication","endpoints","rate limits","pricing","inference","llama-3.3-"]},{"title":"AutoGen + Groq: Building Multi-Agent AI Applications","content":"URL: https://console.groq.com/docs/autogen","keywords":["endpoints","tools"]},{"title":"AutoGen + Groq: Building Multi-Agent AI Applications","content":"[AutoGen](https://microsoft.github.io/autogen/) developed by [Microsoft Research](https://www.microsoft.com/research/) is an open-source framework for building multi-agent AI applications. By powering the\nAutoGen agentic framework with Groq's fast inference speed, you can create sophisticated AI agents that work together to solve complex tasks fast with features including:\n\n- **Multi-Agent Orchestration:** Create and manage multiple agents that can collaborate in realtime\n- **Tool Integration:** Easily connect agents with external tools and APIs\n- **Flexible Workflows:** Support both autonomous and human-in-the-loop conversation patterns\n- **Code Generation & Execution:** Enable agents to write, review, and execute code safely\n\n### Python Quick Start (3 minutes to hello world)\n#### 1. Install the required packages:\n```bash\npip install autogen-agentchat~=0.2 groq\n```\n\n#### 2. Configure your Groq API key:\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\n```\n\n#### 3. Create your first multi-agent application with Groq:\nIn AutoGen, **agents** are autonomous entities that can engage in conversations and perform tasks. The example below shows how to create a simple two-agent system with `llama-3.3-70b-versatile` where\n`UserProxyAgent` initiates the conversation with a question and `AssistantAgent` responds:\n\n```python\nconfig_list = [{\n    \"model\": \"llama-3.3-70b-versatile\",\n    \"api_key\": os.environ.get(\"GROQ_API_KEY\"),\n    \"api_type\": \"groq\"\n}]\n\nassistant = AssistantAgent(\n    name=\"groq_assistant\",\n    system_message=\"You are a helpful AI assistant.\",\n    llm_config={\"config_list\": config_list}\n)\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    code_execution_config=False\n)\n\nuser_proxy.initiate_chat(\n    assistant,\n    message=\"What are the key benefits of using Groq for AI apps?\"\n)\n```\n\n### Advanced Features\n\n#### Code Generation and Execution\nYou can enable secure code execution by configuring the `UserProxyAgent` that allows your agents to write and execute Python code in a controlled environment:\n```python\nwork_dir = Path(\"coding\")\nwork_dir.mkdir(exist_ok=True)\ncode_executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    code_execution_config={\"executor\": code_executor}\n)\n```\n\n#### Tool Integration\nYou can add tools for your agents to use by creating a function and registering it with the assistant. Here's an example of a weather forecast tool:\n```python\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the weather for some location\"\"\"\n    weather_data = {\n        \"berlin\": {\"temperature\": \"13\"},\n        \"istanbul\": {\"temperature\": \"40\"},\n        \"san francisco\": {\"temperature\": \"55\"}\n    }\n    \n    location_lower = location.lower()\n    if location_lower in weather_data:\n        return json.dumps({\n            \"location\": location.title(),\n            \"temperature\": weather_data[location_lower][\"temperature\"],\n            \"unit\": unit\n        })\n    return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\ndef weather_forecast(\n    location: Annotated[str, \"City name\"],\n    unit: Annotated[str, \"Temperature unit (fahrenheit/celsius)\"] = \"fahrenheit\"\n) -> str:\n    weather_details = get_current_weather(location=location, unit=unit)\n    weather = json.loads(weather_details)\n    return f\"{weather['location']} will be {weather['temperature']} degrees {weather['unit']}\"\n```\n\n#### Complete Code Example\nHere is our quick start agent code example combined with code execution and tool use that you can play with:\n```python\nconfig_list = [{\n    \"model\": \"llama-3.3-70b-versatile\",\n    \"api_key\": os.environ.get(\"GROQ_API_KEY\"),\n    \"api_type\": \"groq\"\n}]\n\nwork_dir = Path(\"coding\")\nwork_dir.mkdir(exist_ok=True)\ncode_executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the weather for some location\"\"\"\n    weather_data = {\n        \"berlin\": {\"temperature\": \"13\"},\n        \"istanbul\": {\"temperature\": \"40\"},\n        \"san francisco\": {\"temperature\": \"55\"}\n    }\n    \n    location_lower = location.lower()\n    if location_lower in weather_data:\n        return json.dumps({\n            \"location\": location.title(),\n            \"temperature\": weather_data[location_lower][\"temperature\"],\n            \"unit\": unit\n        })\n    return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\nassistant = AssistantAgent(\n    name=\"groq_assistant\",\n    system_message=\"\"\"You are a helpful AI assistant who can:\n    - Use weather information tools\n    - Write Python code for data visualization\n    - Analyze and explain results\"\"\",\n    llm_config={\"config_list\": config_list}\n)\n\ndef weather_forecast(\n    location: Annotated[str, \"City name\"],\n    unit: Annotated[str, \"Temperature unit (fahrenheit/celsius)\"] = \"fahrenheit\"\n) -> str:\n    weather_details = get_current_weather(location=location, unit=unit)\n    weather = json.loads(weather_details)\n    return f\"{weather['location']} will be {weather['temperature']} degrees {weather['unit']}\"\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    code_execution_config={\"executor\": code_executor}\n)\n\nuser_proxy.initiate_chat(\n    assistant,\n    message=\"\"\"Let's do two things:\n    1. Get the weather for Berlin, Istanbul, and San Francisco\n    2. Write a Python script to create a bar chart comparing their temperatures\"\"\"\n)\n\n**Challenge:** Add to the above example and create a multi-agent [`GroupChat`](https://microsoft.github.io/autogen/0.2/docs/topics/groupchat/customized_speaker_selection) workflow!\n\nFor more detailed documentation and resources on building agentic applications with Groq and AutoGen, see:\n- [AutoGen Documentation](https://microsoft.github.io/autogen/0.2/docs/topics/non-openai-models/cloud-groq/)\n- [AutoGroq](https://github.com/jgravelle/AutoGroq)\n\n---","keywords":["models","authentication","endpoints","parameters","rate limits","tools","speech","inference","llama-3.3-"]},{"title":"OpenAI Compatibility","content":"URL: https://console.groq.com/docs/openai","keywords":["endpoints"]},{"title":"OpenAI Compatibility","content":"<br>\n</br>\nWe designed Groq API to be mostly compatible with OpenAI's client libraries, making it easy to \nconfigure your existing applications to run on Groq and try our inference speed.\n\nWe also have our own [Groq Python and Groq TypeScript libraries](/docs/libraries) that we encourage you to use.\n\n### Configuring OpenAI to Use Groq API\nTo start using Groq with OpenAI's client libraries, pass your Groq API key to the `api_key` parameter\nand change the `base_url` to `https://api.groq.com/openai/v1`:\n\n\nYou can find your API key [here](/keys). \n\n### Currently Unsupported OpenAI Features\n\nNote that although Groq API is mostly OpenAI compatible, there are a few features we don't support just yet: \n\n#### Text Completions\nThe following fields are currently not supported and will result in a 400 error (yikes) if they are supplied:\n- `logprobs`\n\n- `logit_bias`\n\n- `top_logprobs`\n\n- `messages[].name`\n\n- If `N` is supplied, it must be equal to 1.\n\n#### Temperature\nIf you set a `temperature` value of 0, it will be converted to `1e-8`. If you run into any issues, please try setting the value to a float32 `> 0` and `<= 2`.\n\n#### Audio Transcription and Translation\nThe following values are not supported:\n- `vtt`\n- `srt`\n\n### Feedback\nIf you'd like to see support for such features as the above on Groq API, please reach out to us and let us know by submitting a \"Feature Request\" via \"Chat with us\" located on the left. We really value your feedback and would love to hear from you!\n\n---","keywords":["authentication","endpoints","parameters","error","speech","inference"]},{"title":"JigsawStack üß©","content":"URL: https://console.groq.com/docs/jigsawstack","keywords":["endpoints"]},{"title":"JigsawStack","content":"[JigsawStack](https://jigsawstack.com/) is a powerful AI SDK designed to integrate into any backend, automating tasks such as web scraping, Optical Character Recognition (OCR), translation, and more, using \nLarge Language Models (LLMs). By plugging JigsawStack into your existing application infrastructure, you can offload the heavy lifting and focus on building.\n\nThe JigsawStack Prompt Engine is a feature that allows you to not only leverage LLMs but automatically choose the best LLM for every one of your prompts, delivering lightning-fast inference speed and performance\npowered by Groq with features including:\n\n- **Mixture-of-Agents (MoA) Approach:** Automatically selects the best LLMs for your task, combining outputs for higher quality and faster results.\n- **Prompt Caching:** Optimizes performance for repeated prompt runs.\n- **Automatic Prompt Optimization:** Improves performance without manual intervention.\n- **Response Schema Validation:** Ensures accuracy and consistency in outputs.\n\nThe Propt Engine also comes with a built-in prompt guard feature via Llama Guard 3 powered by Groq, which helps prevent prompt injection and a wide range of unsafe categories when activated, such as:\n- Privacy Protection\n- Hate Speech Filtering\n- Sexual Content Blocking\n- Election Misinformation Prevention\n- Code Interpreter Abuse Protection\n- Unauthorized Professional Advice Prevention\n\nTo get started, refer to the JigsawStack documentation [here](https://docs.jigsawstack.com/integration/groq) and learn how to set up your Prompt \nEngine [here](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/jigsawstack-prompt-engine).\n\n---","keywords":["models","authentication","endpoints","rate limits","tools","speech","inference","llama "]},{"title":"Api Reference: Batches Read Request (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/batches-read-request\n\nimport Groq from 'groq-sdk';\n\nconst client = new Groq({\n  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted\n});\n\nasync function main() {\n  const batch = await client.batches.retrieve(\"batch_01jh6xa7reempvjyh6n3yst2zw\");\n  console.log(batch.id);\n}\n\nmain();\n\n---","keywords":["endpoints","tools"]},{"title":"Api Reference: Library Usage Response (json)","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-usage-response.json\n\n{\n  \"id\": \"chatcmpl-f51b2cd2-bef7-417e-964e-a08f0b513c22\",\n  \"object\": \"chat.completion\",\n  \"created\": 1730241104,\n  \"model\": \"llama3-8b-8192\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Fast language models have gained significant attention in recent years due to their ability to process and generate human-like text quickly and efficiently. The importance of fast language models can be understood from their potential applications and benefits:\\n\\n1. **Real-time Chatbots and Conversational Interfaces**: Fast language models enable the development of chatbots and conversational interfaces that can respond promptly to user queries, making them more engaging and useful.\\n2. **Sentiment Analysis and Opinion Mining**: Fast language models can quickly analyze text data to identify sentiments, opinions, and emotions, allowing for improved customer service, market research, and opinion mining.\\n3. **Language Translation and Localization**: Fast language models can quickly translate text between languages, facilitating global communication and enabling businesses to reach a broader audience.\\n4. **Text Summarization and Generation**: Fast language models can summarize long documents or even generate new text on a given topic, improving information retrieval and processing efficiency.\\n5. **Named Entity Recognition and Information Extraction**: Fast language models can rapidly recognize and extract specific entities, such as names, locations, and organizations, from unstructured text data.\\n6. **Recommendation Systems**: Fast language models can analyze large amounts of text data to personalize product recommendations, improve customer experience, and increase sales.\\n7. **Content Generation for Social Media**: Fast language models can quickly generate engaging content for social media platforms, helping businesses maintain a consistent online presence and increasing their online visibility.\\n8. **Sentiment Analysis for Stock Market Analysis**: Fast language models can quickly analyze social media posts, news articles, and other text data to identify sentiment trends, enabling financial analysts to make more informed investment decisions.\\n9. **Language Learning and Education**: Fast language models can provide instant feedback and adaptive language learning, making language education more effective and engaging.\\n10. **Domain-Specific Knowledge Extraction**: Fast language models can quickly extract relevant information from vast amounts of text data, enabling domain experts to focus on high-level decision-making rather than manual information gathering.\\n\\nThe benefits of fast language models include:\\n\\n* **Increased Efficiency**: Fast language models can process large amounts of text data quickly, reducing the time and effort required for tasks such as sentiment analysis, entity recognition, and text summarization.\\n* **Improved Accuracy**: Fast language models can analyze and learn from large datasets, leading to more accurate results and more informed decision-making.\\n* **Enhanced User Experience**: Fast language models can enable real-time interactions, personalized recommendations, and timely responses, improving the overall user experience.\\n* **Cost Savings**: Fast language models can automate many tasks, reducing the need for manual labor and minimizing costs associated with data processing and analysis.\\n\\nIn summary, fast language models have the potential to transform various industries and applications by providing fast, accurate, and efficient language processing capabilities.\"\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"queue_time\": 0.037493756,\n    \"prompt_tokens\": 18,\n    \"prompt_time\": 0.000680594,\n    \"completion_tokens\": 556,\n    \"completion_time\": 0.463333333,\n    \"total_tokens\": 574,\n    \"total_time\": 0.464013927\n  },\n  \"system_fingerprint\": \"fp_179b0f92c9\",\n  \"x_groq\": { \"id\": \"req_01jbd6g2qdfw2adyrt2az8hz4w\" }\n}\n\n---","keywords":["models","authentication","endpoints","rate limits","pricing","inference","llama3-"]},{"title":"Api Reference: Files Delete Request (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-delete-request\n\nimport Groq from 'groq-sdk';\n\nconst client = new Groq({\n  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted\n});\n\nasync function main() {\n  const fileDelete = await client.files.delete(\"file_01jh6x76wtemjr74t1fh0faj5t\");\n  console.log(fileDelete.id);\n}\n\nmain();\n\n---","keywords":["endpoints","tools"]},{"title":"Api Reference: Models List Response (json)","content":"URL: https://console.groq.com/docs/api-reference/scripts/models-list-response.json\n\n```\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"llama3-groq-70b-8192-tool-use-preview\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"Groq\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"gemma2-9b-it\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"Google\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama3-8b-8192\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"Meta\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama-3.2-90b-vision-preview\",\n      \"object\": \"model\",\n      \"created\": 1727226914,\n      \"owned_by\": \"Meta\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama3-70b-8192\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"Meta\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama-3.2-11b-vision-preview\",\n      \"object\": \"model\",\n      \"created\": 1727226869,\n      \"owned_by\": \"Meta\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama-3.2-11b-text-preview\",\n      \"object\": \"model\",\n      \"created\": 1727283005,\n      \"owned_by\": \"Meta\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"whisper-large-v3-turbo\",\n      \"object\": \"model\",\n      \"created\": 1728413088,\n      \"owned_by\": \"OpenAI\",\n      \"active\": true,\n      \"context_window\": 448,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llava-v1.5-7b-4096-preview\",\n      \"object\": \"model\",\n      \"created\": 1725402373,\n      \"owned_by\": \"Other\",\n      \"active\": true,\n      \"context_window\": 4096,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama-3.1-70b-versatile\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"Meta\",\n      \"active\": true,\n      \"context_window\": 32768,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama-3.2-3b-preview\",\n      \"object\": \"model\",\n      \"created\": 1727224290,\n      \"owned_by\": \"Meta\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"whisper-large-v3\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"OpenAI\",\n      \"active\": true,\n      \"context_window\": 448,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama-guard-3-8b\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"Meta\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"mixtral-8x7b-32768\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"Mistral AI\",\n      \"active\": true,\n      \"context_window\": 32768,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"gemma-7b-it\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"Google\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"distil-whisper-large-v3-en\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"Hugging Face\",\n      \"active\": true,\n      \"context_window\": 448,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama-3.2-1b-preview\",\n      \"object\": \"model\",\n      \"created\": 1727224268,\n      \"owned_by\": \"Meta\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama-3.2-90b-text-preview\",\n      \"object\": \"model\",\n      \"created\": 1727285716,\n      \"owned_by\": \"Meta\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama3-groq-8b-8192-tool-use-preview\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"Groq\",\n      \"active\": true,\n      \"context_window\": 8192,\n      \"public_apps\": null\n    },\n    {\n      \"id\": \"llama-3.1-8b-instant\",\n      \"object\": \"model\",\n      \"created\": 1693721698,\n      \"owned_by\": \"Meta\",\n      \"active\": true,\n      \"context_window\": 131072,\n      \"public_apps\": null\n    }\n  ]\n}\n```\n\n---","keywords":["models","endpoints","rate limits","tools","speech","llama3-","gemma2-","llama-3.2-","whisper-","llama-3.1-","llama-","mixtral-8","mistral ","gemma-7b"]},{"title":"Api Reference: Library Usage Image Input (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-usage-image-input\n\n// Image input\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq({ apiKey: process.env.GROQ_API_KEY });\n\nasync function main() {\n  const completion = await groq.chat.completions\n    .create({\n      messages: [\n        {\n          role: \"user\",\n          content: \"Explain the importance of fast language models\",\n        },\n      ],\n      model: \"mixtral-8x7b-32768\",\n    })\n    .then((chatCompletion) => {\n      console.log(chatCompletion.choices[0]?.message?.content || \"\");\n    });\n}\n\nmain();\n\n---","keywords":["models","endpoints","tools","inference","mixtral-8"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-list-request.py\n\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),  # This is the default and can be omitted\n)\nfile_list = client.files.list()\nprint(file_list.data)\n\n---","keywords":["endpoints"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/api-reference/scripts/batches-create-request.py\n\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),  # This is the default and can be omitted\n)\nbatch = client.batches.create(\n    completion_window=\"24h\",\n    endpoint=\"/v1/chat/completions\",\n    input_file_id=\"file_01jh6x76wtemjr74t1fh0faj5t\",\n)\nprint(batch.id)\n\n---","keywords":["endpoints"]},{"title":"Api Reference: Library Transcription Default (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-transcription-default\n\nimport fs from \"fs\";\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq();\nasync function main() {\n  const transcription = await groq.audio.transcriptions.create({\n    file: fs.createReadStream(\"sample_audio.m4a\"),\n    model: \"whisper-large-v3\",\n    prompt: \"Specify context or spelling\", // Optional\n    response_format: \"json\", // Optional\n    language: \"en\", // Optional\n    temperature: 0.0, // Optional\n  });\n  console.log(transcription.text);\n}\nmain();\n\n---","keywords":["models","endpoints","parameters","tools","speech","whisper-"]},{"title":"Api Reference: Files Download Request (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-download-request\n\nimport Groq from 'groq-sdk';\n\nconst client = new Groq({\n  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted\n});\n\nasync function main() {\n    const response = await client.files.content('file_01jh6x76wtemjr74t1fh0faj5t');\n    console.log(response);\n}\n\nmain();\n\n---","keywords":["endpoints","tools"]},{"title":"Api Reference: Files Upload Request (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-upload-request.curl\n\ncurl https://api.groq.com/openai/v1/files \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -F purpose=\"batch\" \\\n  -F \"file=@batch_file.jsonl\"\n\n---","keywords":["authentication","endpoints"]},{"title":"Api Reference: Model Request (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/model-request.curl\n\ncurl https://api.groq.com/openai/v1/models/llama-3.3-70b-versatile \\\n-H \"Authorization: Bearer $GROQ_API_KEY\"\n\n---","keywords":["models","authentication","endpoints","llama-3.3-"]},{"title":"Api Reference: Files List Request (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-list-request\n\nimport Groq from 'groq-sdk';\n\nconst client = new Groq({\n  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted\n});\n\nasync function main() {\n  const fileList = await client.files.list();\n  console.log(fileList.data);\n}\n\nmain();\n\n---","keywords":["endpoints","tools"]},{"title":"Api Reference: Files List Response (json)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-list-response.json\n\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n      \"object\": \"file\",\n      \"bytes\": 966,\n      \"created_at\": 1736472501,\n      \"filename\": \"batch_file.jsonl\",\n      \"purpose\": \"batch\"\n    }\n  ]\n}\n\n---","keywords":["endpoints"]},{"title":"Image input","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-usage-image-input.py\n\n# Image input\nimport os\n\nfrom groq import Groq\n\nclient = Groq(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"you are a helpful assistant.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of fast language models\",\n        }\n    ],\n    model=\"mixtral-8x7b-32768\",\n)\n\nprint(chat_completion.choices[0].message.content)\n\n---","keywords":["models","endpoints","inference","mixtral-8"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-delete-request.py\n\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),  # This is the default and can be omitted\n)\nfile_delete = client.files.delete(\n    \"file_01jh6x76wtemjr74t1fh0faj5t\",\n)\nprint(file_delete.id)\n\n---","keywords":["endpoints"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/api-reference/scripts/batches-read-request.py\n\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),  # This is the default and can be omitted\n)\nbatch = client.batches.retrieve(\n    \"batch_01jh6xa7reempvjyh6n3yst2zw\",\n)\nprint(batch.id)\n\n---","keywords":["endpoints"]},{"title":"Api Reference: Library Transcription Default (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-transcription-default.curl\n\ncurl https://api.groq.com/openai/v1/audio/transcriptions \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F file=\"@./sample_audio.m4a\" \\\n  -F model=\"whisper-large-v3\"\n\n---","keywords":["models","authentication","endpoints","speech","whisper-"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-download-request.py\n\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),  # This is the default and can be omitted\n)\nresponse = client.files.content(\n    \"file_01jh6x76wtemjr74t1fh0faj5t\",\n)\nprint(response)\n\n---","keywords":["endpoints"]},{"title":"Api Reference: Files Download Request (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-download-request.curl\n\ncurl https://api.groq.com/openai/v1/files/file_01jh6x76wtemjr74t1fh0faj5t/content \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n\n---","keywords":["authentication","endpoints"]},{"title":"Optional","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-transcription-default.py\n\nimport os\nfrom groq import Groq\n\nclient = Groq()\nfilename = os.path.dirname(__file__) + \"/sample_audio.m4a\"\n\nwith open(filename, \"rb\") as file:\n    transcription = client.audio.transcriptions.create(\n      file=(filename, file.read()),\n      model=\"whisper-large-v3\",\n      prompt=\"Specify context or spelling\",  # Optional\n      response_format=\"json\",  # Optional\n      language=\"en\",  # Optional\n      temperature=0.0  # Optional\n    )\n    print(transcription.text)\n\n---","keywords":["models","endpoints","parameters","speech","whisper-"]},{"title":"Api Reference: Batches Create Request (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/batches-create-request.curl\n\ncurl https://api.groq.com/openai/v1/batches \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input_file_id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n    \"endpoint\": \"/v1/chat/completions\",\n    \"completion_window\": \"24h\"\n  }'\n\n---","keywords":["authentication","endpoints"]},{"title":"Api Reference: Batches Create Request (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/batches-create-request\n\nimport Groq from 'groq-sdk';\n\nconst client = new Groq({\n  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted\n});\n\nasync function main() {\n  const batch = await client.batches.create({\n    completion_window: \"24h\",\n    endpoint: \"/v1/chat/completions\",\n    input_file_id: \"file_01jh6x76wtemjr74t1fh0faj5t\",\n  });\n  console.log(batch.id);\n}\n\nmain();\n\n---","keywords":["endpoints","tools"]},{"title":"Api Reference: Models List Request (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/models-list-request.curl\n\ncurl https://api.groq.com/openai/v1/models \\\n-H \"Authorization: Bearer $GROQ_API_KEY\"\n\n---","keywords":["models","authentication","endpoints"]},{"title":"Api Reference: Library Usage Default (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-usage-default\n\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq({ apiKey: process.env.GROQ_API_KEY });\n\nasync function main() {\n  const completion = await groq.chat.completions\n    .create({\n      messages: [\n        {\n          role: \"user\",\n          content: \"Explain the importance of fast language models\",\n        },\n      ],\n      model: \"llama-3.3-70b-versatile\",\n    })\n  console.log(completion.choices[0].message.content);\n}\n\nmain();\n\n---","keywords":["models","endpoints","tools","inference","llama-3.3-"]},{"title":"Api Reference: Library Translation Default (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-translation-default.curl\n\ncurl https://api.groq.com/openai/v1/audio/translations \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F file=\"@./sample_audio.m4a\" \\\n  -F model=\"whisper-large-v3\"\n\n---","keywords":["models","authentication","endpoints","speech","whisper-"]},{"title":"Default","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-translation-default.py\n\n# Default\nimport os\nfrom groq import Groq\n\nclient = Groq()\nfilename = os.path.dirname(__file__) + \"/sample_audio.m4a\"\n\nwith open(filename, \"rb\") as file:\n    translation = client.audio.translations.create(\n      file=(filename, file.read()),\n      model=\"whisper-large-v3\",\n      prompt=\"Specify context or spelling\",  # Optional\n      response_format=\"json\",  # Optional\n      temperature=0.0  # Optional\n    )\n    print(translation.text)\n\n---","keywords":["models","endpoints","parameters","speech","whisper-"]},{"title":"Api Reference: Library Transcription Response (json)","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-transcription-response.json\n\n{\n  \"text\": \"Your transcribed text appears here...\",\n  \"x_groq\": {\n    \"id\": \"req_unique_id\"\n  }\n}\n\n---","keywords":["endpoints","speech"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-read-request.py\n\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),  # This is the default and can be omitted\n)\nfile = client.files.info(\n    \"file_01jh6x76wtemjr74t1fh0faj5t\",\n)\nprint(file.id)\n\n---","keywords":["endpoints"]},{"title":"Api Reference: Model Request (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/model-request\n\nconst groq = new Groq({ apiKey: process.env.GROQ_API_KEY });\n\nasync function main() {\n  const model = await groq.models.retrieve(\"llama-3.3-70b-versatile\");\n  console.log(model);\n}\n\nmain();\n\n---","keywords":["models","endpoints","tools","llama-3.3-"]},{"title":"Api Reference: Files Read Request (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-read-request.curl\n\ncurl https://api.groq.com/openai/v1/files/file_01jh6x76wtemjr74t1fh0faj5t \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n\n---","keywords":["authentication","endpoints"]},{"title":"Api Reference: Model Response (json)","content":"URL: https://console.groq.com/docs/api-reference/scripts/model-response.json\n\n{\n  \"id\": \"llama3-8b-8192\",\n  \"object\": \"model\",\n  \"created\": 1693721698,\n  \"owned_by\": \"Meta\",\n  \"active\": true,\n  \"context_window\": 8192,\n  \"public_apps\": null\n}\n\n---","keywords":["models","endpoints","llama3-"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-upload-request.py\n\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),  # This is the default and can be omitted\n)\n\ncontent = b'{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"user\", \"content\": \"Explain the importance of fast language models\"}]}}'\n\nfile = client.files.create(\n    file=b'raw file contents',\n    purpose=content,\n)\n\nprint(file.id)\n\n---","keywords":["models","endpoints","inference","llama-3.1-"]},{"title":"Api Reference: Batches List Request (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/batches-list-request.curl\n\ncurl https://api.groq.com/openai/v1/batches \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n\n---","keywords":["authentication","endpoints"]},{"title":"Api Reference: Models List Request (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/models-list-request\n\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq({ apiKey: process.env.GROQ_API_KEY });\n\nasync function main() {\n  const models = await groq.models.list();\n  console.log(models);\n}\n\nmain();\n\n---","keywords":["models","endpoints","tools"]},{"title":"Api Reference: Batches List Response (json)","content":"URL: https://console.groq.com/docs/api-reference/scripts/batches-list-response.json\n\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"batch_01jh6xa7reempvjyh6n3yst2zw\",\n      \"object\": \"batch\",\n      \"endpoint\": \"/v1/chat/completions\",\n      \"errors\": null,\n      \"input_file_id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n      \"completion_window\": \"24h\",\n      \"status\": \"validating\",\n      \"output_file_id\": null,\n      \"error_file_id\": null,\n      \"finalizing_at\": null,\n      \"failed_at\": null,\n      \"expired_at\": null,\n      \"cancelled_at\": null,\n      \"request_counts\": {\n        \"total\": 0,\n        \"completed\": 0,\n        \"failed\": 0\n      },\n      \"metadata\": null,\n      \"created_at\": 1736472600,\n      \"expires_at\": 1736559000,\n      \"cancelling_at\": null,\n      \"completed_at\": null,\n      \"in_progress_at\": null\n    }\n  ]\n}\n\n---","keywords":["endpoints","error"]},{"title":"Api Reference: Batches List Request (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/batches-list-request\n\nimport Groq from 'groq-sdk';\n\nconst client = new Groq({\n  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted\n});\n\nasync function main() {\n  const batchList = await client.batches.list();\n  console.log(batchList.data);\n}\n\nmain();\n\n---","keywords":["endpoints","tools"]},{"title":"Api Reference: Batches Create Response (json)","content":"URL: https://console.groq.com/docs/api-reference/scripts/batches-create-response.json\n\n{\n  \"id\": \"batch_01jh6xa7reempvjyh6n3yst2zw\",\n  \"object\": \"batch\",\n  \"endpoint\": \"/v1/chat/completions\",\n  \"errors\": null,\n  \"input_file_id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n  \"completion_window\": \"24h\",\n  \"status\": \"validating\",\n  \"output_file_id\": null,\n  \"error_file_id\": null,\n  \"finalizing_at\": null,\n  \"failed_at\": null,\n  \"expired_at\": null,\n  \"cancelled_at\": null,\n  \"request_counts\": {\n    \"total\": 0,\n    \"completed\": 0,\n    \"failed\": 0\n  },\n  \"metadata\": null,\n  \"created_at\": 1736472600,\n  \"expires_at\": 1736559000,\n  \"cancelling_at\": null,\n  \"completed_at\": null,\n  \"in_progress_at\": null\n}\n\n---","keywords":["endpoints","error"]},{"title":"Api Reference: Library Translation Default (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-translation-default\n\n// Default\nimport fs from \"fs\";\nimport Groq from \"groq-sdk\";\n\nconst groq = new Groq();\nasync function main() {\n  const translation = await groq.audio.translations.create({\n    file: fs.createReadStream(\"sample_audio.m4a\"),\n    model: \"whisper-large-v3\",\n    prompt: \"Specify context or spelling\", // Optional\n    response_format: \"json\", // Optional\n    temperature: 0.0, // Optional\n  });\n  console.log(translation.text);\n}\nmain();\n\n---","keywords":["models","endpoints","parameters","tools","speech","whisper-"]},{"title":"Api Reference: Batches Read Request (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/batches-read-request.curl\n\ncurl https://api.groq.com/openai/v1/batches/batch_01jh6xa7reempvjyh6n3yst2zw \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n\n---","keywords":["authentication","endpoints"]},{"title":"Api Reference: Files Delete Request (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-delete-request.curl\n\ncurl -X DELETE https://api.groq.com/openai/v1/files/file_01jh6x76wtemjr74t1fh0faj5t \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n\n---","keywords":["authentication","endpoints"]},{"title":"Api Reference: Files List Request (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-list-request.curl\n\ncurl https://api.groq.com/openai/v1/files \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n\n---","keywords":["authentication","endpoints"]},{"title":"Api Reference: Library Usage Default (curl)","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-usage-default.curl\n\ncurl https://api.groq.com/openai/v1/chat/completions -s \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GROQ_API_KEY\" \\\n-d '{\n\"model\": \"llama-3.3-70b-versatile\",\n\"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Explain the importance of fast language models\"\n}]\n}'\n\n---","keywords":["models","authentication","endpoints","inference","llama-3.3-"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-usage-default.py\n\nimport os\n\nfrom groq import Groq\n\nclient = Groq(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"you are a helpful assistant.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of fast language models\",\n        }\n    ],\n    model=\"llama-3.3-70b-versatile\",\n)\n\nprint(chat_completion.choices[0].message.content)\n\n---","keywords":["models","endpoints","inference","llama-3.3-"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/api-reference/scripts/models-list-request.py\n\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\nmodels = client.models.list()\n\nprint(models)\n\n---","keywords":["models","endpoints"]},{"title":"Api Reference: Files Upload Request (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-upload-request\n\nimport Groq from 'groq-sdk';\n\nconst client = new Groq({\n  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted\n});\n\nconst content = Buffer.from('{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"user\", \"content\": \"Explain the importance of fast language models\"}]}}\\n');\n\nasync function main() {\n  const file = await client.files.create({ file: content, purpose: 'batch' });\n  console.log(file.id);\n}\n\nmain();\n\n---","keywords":["models","endpoints","tools","inference","llama-3.1-"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/api-reference/scripts/batches-list-request.py\n\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),  # This is the default and can be omitted\n)\nbatch_list = client.batches.list()\nprint(batch_list.data)\n\n---","keywords":["endpoints"]},{"title":"Api Reference: Library Translation Response (json)","content":"URL: https://console.groq.com/docs/api-reference/scripts/library-translation-response.json\n\n{\n  \"text\": \"Your translated text appears here...\",\n  \"x_groq\": {\n    \"id\": \"req_unique_id\"\n  }\n}\n\n---","keywords":["endpoints"]},{"title":"Api Reference: Files Upload Response (json)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-upload-response.json\n\n{\n  \"id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n  \"object\": \"file\",\n  \"bytes\": 966,\n  \"created_at\": 1736472501,\n  \"filename\": \"batch_file.jsonl\",\n  \"purpose\": \"batch\"\n}\n\n---","keywords":["endpoints"]},{"title":"Api Reference: Files Delete Response (json)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-delete-response.json\n\n{\n  \"id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n  \"object\": \"file\",\n  \"deleted\": true\n}\n\n---","keywords":["endpoints"]},{"title":"This is the default and can be omitted","content":"URL: https://console.groq.com/docs/api-reference/scripts/model-request.py\n\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\nmodel = client.models.retrieve(\"llama-3.3-70b-versatile\")\n\nprint(model)\n\n---","keywords":["models","endpoints","llama-3.3-"]},{"title":"Api Reference: Files Read Request (js)","content":"URL: https://console.groq.com/docs/api-reference/scripts/files-read-request\n\nimport Groq from 'groq-sdk';\n\nconst client = new Groq({\n  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted\n});\n\nasync function main() {\n    const file = await client.files.info('file_01jh6x76wtemjr74t1fh0faj5t');\n    console.log(file.id);\n}\n\nmain();\n\n---","keywords":["endpoints","tools"]},{"title":"Groq API Reference","content":"URL: https://console.groq.com/docs/api-reference","keywords":["endpoints"]},{"title":"Groq API Reference","content":"---","keywords":[]},{"title":"Script: Types.d (ts)","content":"URL: https://console.groq.com/docs/scripts/types.d\n\ndeclare module \"*.sh\" {\n  const content: string;\n  export default content;\n}\n\n---","keywords":["endpoints"]},{"title":"Script: Code Examples (ts)","content":"URL: https://console.groq.com/docs/scripts/code-examples\n\n```javascript\nexport const getExampleCode = (modelId: string) => ({\n  shell: `curl https://api.groq.com/v1/chat/completions \\\\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -d '{\n    \"model\": \"${modelId}\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Explain why fast inference is critical for reasoning models\"\n      }\n    ]\n  }'`,\n\n  javascript: `import Groq from \"groq-sdk\";\nconst groq = new Groq();\nasync function main() {\n  const completion = await groq.chat.completions.create({\n    model: \"${modelId}\",\n    messages: [\n      {\n        role: \"user\",\n        content: \"Explain why fast inference is critical for reasoning models\",\n      },\n    ],\n  });\n  console.log(completion.choices[0]?.message?.content);\n}\nmain().catch(console.error);`,\n\n  python: `from groq import Groq\nclient = Groq()\ncompletion = client.chat.completions.create(\n    model=\"${modelId}\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain why fast inference is critical for reasoning models\"\n        }\n    ]\n)\nprint(completion.choices[0].message.content)`,\n\n  json: `{\n  \"model\": \"${modelId}\",\n  \"messages\": [\n    {\n      \"role\": \"user\", \n      \"content\": \"Explain why fast inference is critical for reasoning models\"\n    }\n  ]\n}`,\n});\n```\n\n---","keywords":["models","authentication","endpoints","error","tools","inference"]}],"lastUpdated":"2025-03-30T18:53:14.912Z"}